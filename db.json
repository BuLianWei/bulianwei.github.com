{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/about/index.md.b","path":"about/index.md.b","modified":1,"renderable":0},{"_id":"source/about/index.md.bak","path":"about/index.md.bak","modified":1,"renderable":0},{"_id":"source/about/index的副本.md.bak的副本","path":"about/index的副本.md.bak的副本","modified":1,"renderable":0},{"_id":"themes/matery/source/favicon.png","path":"favicon.png","modified":1,"renderable":1},{"_id":"themes/matery/source/css/gitment.css","path":"css/gitment.css","modified":1,"renderable":1},{"_id":"themes/matery/source/css/my-gitalk.css","path":"css/my-gitalk.css","modified":1,"renderable":1},{"_id":"themes/matery/source/css/matery.css","path":"css/matery.css","modified":1,"renderable":1},{"_id":"themes/matery/source/css/my.css","path":"css/my.css","modified":1,"renderable":1},{"_id":"themes/matery/source/js/matery.js","path":"js/matery.js","modified":1,"renderable":1},{"_id":"themes/matery/source/js/search.js","path":"js/search.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/avatar.jpg","path":"medias/avatar.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/logo.png","path":"medias/logo.png","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/animate/animate.min.css","path":"libs/animate/animate.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.css","path":"libs/aos/aos.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.js","path":"libs/aos/aos.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","path":"libs/aplayer/APlayer.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/background/canvas-nest.js","path":"libs/background/canvas-nest.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon-dynamic.js","path":"libs/background/ribbon-dynamic.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon-refresh.min.js","path":"libs/background/ribbon-refresh.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon.min.js","path":"libs/background/ribbon.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","path":"libs/dplayer/DPlayer.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","path":"libs/cryptojs/crypto-js.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","path":"libs/gitalk/gitalk.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","path":"libs/jqcloud/jqcloud-1.0.4.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","path":"libs/jqcloud/jqcloud.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","path":"libs/gitment/gitment-default.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","path":"libs/masonry/masonry.pkgd.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","path":"libs/others/busuanzi.pure.mini.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/others/clicklove.js","path":"libs/others/clicklove.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","path":"libs/scrollprogress/scrollProgress.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","path":"libs/tocbot/tocbot.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","path":"libs/tocbot/tocbot.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/cover.jpg","path":"medias/cover.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/2.jpg","path":"medias/featureimages/2.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/21.jpg","path":"medias/featureimages/21.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/22.jpg","path":"medias/featureimages/22.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/23.jpg","path":"medias/featureimages/23.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/5.jpg","path":"medias/featureimages/5.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/7.jpg","path":"medias/featureimages/7.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/8.jpg","path":"medias/featureimages/8.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/reward/alipay.jpg","path":"medias/reward/alipay.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/reward/wechat.png","path":"medias/reward/wechat.png","modified":1,"renderable":1},{"_id":"source/notes/img/零拷贝.png","path":"notes/img/零拷贝.png","modified":1,"renderable":0},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","path":"libs/aplayer/APlayer.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","path":"libs/dplayer/DPlayer.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment.js","path":"libs/gitment/gitment.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/jquery/jquery-2.2.0.min.js","path":"libs/jquery/jquery-2.2.0.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/valine/Valine.min.js","path":"libs/valine/Valine.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/0.jpg","path":"medias/banner/0.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/0.jpg","path":"medias/featureimages/0.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/1.jpg","path":"medias/featureimages/1.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/10.jpg","path":"medias/featureimages/10.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/14.jpg","path":"medias/featureimages/14.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/15.jpg","path":"medias/featureimages/15.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/16.jpg","path":"medias/featureimages/16.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/11.jpg","path":"medias/featureimages/11.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/12.jpg","path":"medias/featureimages/12.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/13.jpg","path":"medias/featureimages/13.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/17.jpg","path":"medias/featureimages/17.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/18.jpg","path":"medias/featureimages/18.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/19.jpg","path":"medias/featureimages/19.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/20.jpg","path":"medias/featureimages/20.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/3.jpg","path":"medias/featureimages/3.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/4.jpg","path":"medias/featureimages/4.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/6.jpg","path":"medias/featureimages/6.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/9.jpg","path":"medias/featureimages/9.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/css/font-awesome.min.css","path":"libs/awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","path":"libs/gitalk/gitalk.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","path":"libs/lightGallery/css/lightgallery.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","path":"libs/lightGallery/img/loading.gif","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","path":"libs/lightGallery/img/video-play.png","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","path":"libs/lightGallery/img/vimeo-play.png","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","path":"libs/lightGallery/img/youtube-play.png","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","path":"libs/lightGallery/fonts/lg.eot","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","path":"libs/lightGallery/fonts/lg.svg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","path":"libs/lightGallery/fonts/lg.ttf","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","path":"libs/lightGallery/fonts/lg.woff","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","path":"libs/lightGallery/js/lightgallery-all.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","path":"libs/materialize/materialize.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","path":"libs/materialize/materialize.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/css/share.min.css","path":"libs/share/css/share.min.css","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","path":"libs/share/fonts/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","path":"libs/share/fonts/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","path":"libs/share/fonts/iconfont.ttf","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","path":"libs/share/fonts/iconfont.woff","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","path":"libs/share/js/jquery.share.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","path":"libs/share/js/social-share.min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/valine/av-min.js","path":"libs/valine/av-min.js","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/1.jpg","path":"medias/banner/1.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/3.jpg","path":"medias/banner/3.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/5.jpg","path":"medias/banner/5.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/2.jpg","path":"medias/banner/2.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/6.jpg","path":"medias/banner/6.jpg","modified":1,"renderable":1},{"_id":"source/notes/img/kafka架构.png","path":"notes/img/kafka架构.png","modified":1,"renderable":0},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.woff2","path":"libs/awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.woff","path":"libs/awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"source/notes/img/普通拷贝.png","path":"notes/img/普通拷贝.png","modified":1,"renderable":0},{"_id":"themes/matery/source/libs/awesome/fonts/FontAwesome.otf","path":"libs/awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.eot","path":"libs/awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.ttf","path":"libs/awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/matery/source/medias/banner/4.jpg","path":"medias/banner/4.jpg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.svg","path":"libs/awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","path":"libs/echarts/echarts.min.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"03fe0a623b67b9a8c471eca6e6e3dd62c3317c03","modified":1584595857000},{"_id":"source/404.md","hash":"6e12eb0f1ff042b3de34c89bd87739dd0e521695","modified":1584532336000},{"_id":"themes/matery/.DS_Store","hash":"0bbbd6db6699b53d330d7ba4afddb1f1b76ed211","modified":1584596755000},{"_id":"themes/matery/.gitignore","hash":"eaa3d84cb77d92a21b111fd1e37f53edc1ff9de0","modified":1584593486000},{"_id":"themes/matery/LICENSE","hash":"7df059597099bb7dcf25d2a9aedfaf4465f72d8d","modified":1584593486000},{"_id":"themes/matery/README.md","hash":"d07e122e05c7c78991132b1e6ad39638adf00749","modified":1584593486000},{"_id":"themes/matery/README_CN.md","hash":"74be780ca50b0e31abf95786cd54b57ff4679b82","modified":1584593486000},{"_id":"themes/matery/_config.yml","hash":"cb5cb1e6531cf95109ddaf74a72c110472c2f916","modified":1584597396000},{"_id":"source/_data/friends.json","hash":"2b4d18d300da957dd5750ac6567d86139ff8fae4","modified":1584593486000},{"_id":"source/_data/musics.json","hash":"46588a16b735cfb95b8083e39e2b9f6dec517dcb","modified":1584593486000},{"_id":"source/_posts/.DS_Store","hash":"6ca8e5b8e1eda621bfefa4259f4637168d8641e0","modified":1584595781000},{"_id":"source/_posts/hello-world.md","hash":"63a86441b2092fb816b8cedd39516d50dcd03d6f","modified":1584593486000},{"_id":"source/_posts/大数据面试题.md","hash":"7a18bf89e075a192e7b8934133fc59c347b5c6c1","modified":1584532336000},{"_id":"source/_posts/手撕Spark之WordCount RDD执行流程.md","hash":"82b95d51e11911c056f77a9d4c42152f0b7f9cec","modified":1584532336000},{"_id":"source/about/index.md","hash":"8c810b0130e86a9052564b986e568a57bbebc416","modified":1584593486000},{"_id":"source/about/index.md.b","hash":"8c810b0130e86a9052564b986e568a57bbebc416","modified":1584532336000},{"_id":"source/about/index.md.bak","hash":"96c7cd8d539be6e58828ddf53a636aa21f4cef87","modified":1584532336000},{"_id":"source/about/index的副本.md.bak的副本","hash":"d57d7e7e3bdb23c6d4a294fbd2dd0492c5ade71c","modified":1584532336000},{"_id":"source/about/resume的副本.md","hash":"edadbd8fb750e08dc2cbaef2262cfd35d8984a9b","modified":1584532336000},{"_id":"source/categories/index.md","hash":"9758f1d8d7131ee10e4b1e3004ce298760c4be5c","modified":1584593486000},{"_id":"source/friends/index.md","hash":"f447cb37394634a9eb3fd0e57a81242c7b3529ff","modified":1584593486000},{"_id":"source/notes/Docker.md","hash":"5b76d0cb91f5564b4b8db2405a47c39ab6544e5f","modified":1584532336000},{"_id":"source/notes/Bigdata.md","hash":"62c97741df9dccef5e45d0d40e69aae811d417d2","modified":1584532336000},{"_id":"source/notes/Flink-Trouble.md","hash":"f181c4efe5d722f88a7697fc5a0523da23b44d91","modified":1584532336000},{"_id":"source/notes/Flink.md","hash":"da1753acb3692cd69c06eb63aee20d1414463626","modified":1584532336000},{"_id":"source/notes/Github.md","hash":"943b907a6e4098ed45d0b6fa27fdf0d4ddc1d9c6","modified":1584532336000},{"_id":"source/notes/Golang.md","hash":"4867a07d820380eec261a6acb99dbc7c9edc0b26","modified":1584532996000},{"_id":"source/notes/Hive.md","hash":"f138b5ced327157702a11685c49e896bc369a8a3","modified":1584532336000},{"_id":"source/notes/Kafka.md","hash":"edeb9a607de62b7659665945f9b5aa98852c7d65","modified":1584532336000},{"_id":"source/notes/Linux.md","hash":"28d3cda8809bcdf22e9cfd1991c1b898935c0d1e","modified":1584532336000},{"_id":"source/notes/Redis.md","hash":"61d1a50da3da768c488ee42e7383ef63906f4749","modified":1584532336000},{"_id":"source/notes/Scala.md","hash":"7846d29be76f40f7c820d5bb27c6722202db6f72","modified":1584532336000},{"_id":"source/notes/Spark.md","hash":"83c02049edad7c794fbb7d0ecb07dea360e1b76e","modified":1584532336000},{"_id":"source/notes/index.md","hash":"07bb7cf122ebedb788483c4f57e9ab4e3414de56","modified":1584533025000},{"_id":"source/tags/index.md","hash":"5a056cf0f7a89b20cfffae3546e9b80e89095056","modified":1584593486000},{"_id":"themes/matery/languages/default.yml","hash":"2c4d4e72f0fb3431260643dcb85af11655fabe13","modified":1584593486000},{"_id":"themes/matery/languages/zh-CN.yml","hash":"db4b71662d408255eebd633fa3ded8d039122df3","modified":1584593486000},{"_id":"themes/matery/layout/about.ejs","hash":"ee639d0310867976b3e5fb9f92c215a17a433703","modified":1584593486000},{"_id":"themes/matery/layout/archive.ejs","hash":"c8459660e87a0a07a6e007a99a2303b142bb7744","modified":1584593486000},{"_id":"themes/matery/layout/categories.ejs","hash":"8e54665cc25d7c333da7d9f312987190be6215da","modified":1584593486000},{"_id":"themes/matery/layout/category.ejs","hash":"95176564ea32e65ddc9fdbe9074332239d6a1305","modified":1584593486000},{"_id":"themes/matery/layout/friends.ejs","hash":"89c47cf1eb2cf1feb8a8d06f4eb7c76b713f79c0","modified":1584593486000},{"_id":"themes/matery/layout/index.ejs","hash":"489f2fe7e4171613504f58e7e0462d81d2f7f546","modified":1584593486000},{"_id":"themes/matery/layout/layout.ejs","hash":"2adaf28874ae6f94c126cba9661e30f0c61c374b","modified":1584597254000},{"_id":"themes/matery/layout/post.ejs","hash":"f9662a96d0f497a3b2731472b8ad871c7cbdf13a","modified":1584593486000},{"_id":"themes/matery/layout/tag.ejs","hash":"4305eeeb3434c24ba2493fa08d1f1bd9f2efa9aa","modified":1584593486000},{"_id":"themes/matery/layout/tags.ejs","hash":"cf9517aa6a0111355121f44615d6923e312283c7","modified":1584593486000},{"_id":"themes/matery/source/.DS_Store","hash":"23e4dd52f79fa8deb9d6e07fb3a38ec93716d5dd","modified":1584596766000},{"_id":"themes/matery/source/favicon.png","hash":"20674c497b75fc889194b47fd18ecea12303d8ec","modified":1584593486000},{"_id":"themes/matery/layout/_partial/back-top.ejs","hash":"8c91d2088c9bb323246b054d4940bde6cead6828","modified":1584593486000},{"_id":"themes/matery/layout/_partial/bg-cover-content.ejs","hash":"2d9a44f6fbed4d117bbc403095d9a810cb05303c","modified":1584593486000},{"_id":"themes/matery/layout/_partial/disqus.ejs","hash":"a0f53d1a9b579d52e52ccad8c6e330bf3b89547e","modified":1584593486000},{"_id":"themes/matery/layout/_partial/bg-cover.ejs","hash":"02191109712f61c0e487b8f0b8466597181a9004","modified":1584593486000},{"_id":"themes/matery/layout/_partial/footer.ejs","hash":"e8fa64db1c0210e3fac372637691f940d63a1ff4","modified":1584593486000},{"_id":"themes/matery/layout/_partial/gitalk.ejs","hash":"e4c5bf28ddc29519eee8debe79cce45bf279adeb","modified":1584593486000},{"_id":"themes/matery/layout/_partial/github-link.ejs","hash":"3aeb581bd78ab8e15b858e4c44c03bcf92f20b9e","modified":1584593486000},{"_id":"themes/matery/layout/_partial/gitment.ejs","hash":"0abfb51dc80ad063fb2118bee28de6bb8d99ed4e","modified":1584593486000},{"_id":"themes/matery/layout/_partial/google-analytics.ejs","hash":"5f4992205617da5f8cc5863c62b5ec46e414e2fb","modified":1584593486000},{"_id":"themes/matery/layout/_partial/header.ejs","hash":"e253c813b3ee5ed924700a95133741802e58adc5","modified":1584593486000},{"_id":"themes/matery/layout/_partial/head.ejs","hash":"1f337fe1343f87fc958eded799a9ac93fc194e87","modified":1584593486000},{"_id":"themes/matery/layout/_partial/index-cover.ejs","hash":"6583c00323d891a03343b6a621a0484a68d74f8a","modified":1584593486000},{"_id":"themes/matery/layout/_partial/livere.ejs","hash":"9c3401b42ea7f26410a5593bae93ada7e57b43be","modified":1584593486000},{"_id":"themes/matery/layout/_partial/mobile-nav.ejs","hash":"b70a2d40677d64d6b56fc51ac1331ad3a50e777c","modified":1584593486000},{"_id":"themes/matery/layout/_partial/navigation.ejs","hash":"20216e7ad6b48d4a4f8d11d6881e667e5186820f","modified":1584593486000},{"_id":"themes/matery/layout/_partial/paging.ejs","hash":"68a24cad2b2049c4dc3a250aa30bf4256f9e50cb","modified":1584593486000},{"_id":"themes/matery/layout/_partial/post-cover.ejs","hash":"5b423384b9c0fe77acc4247a8a85304022e5bd2a","modified":1584593486000},{"_id":"themes/matery/layout/_partial/post-detail-toc.ejs","hash":"b7320ccb822f9969c58401a0bc946a8a7a1fea9c","modified":1584593486000},{"_id":"themes/matery/layout/_partial/post-detail.ejs","hash":"13cc4efb0cf29dc9879041da74d6bc8bf2695010","modified":1584593486000},{"_id":"themes/matery/layout/_partial/post-statis.ejs","hash":"2b2fe8e8e94e65c52a4dbd454168e9b9df6baf10","modified":1584593486000},{"_id":"themes/matery/layout/_partial/prev-next.ejs","hash":"3f73f077b05b6bf41150b00f43870a41d3fa390f","modified":1584593486000},{"_id":"themes/matery/layout/_partial/reward.ejs","hash":"3dff4f6a73973b0b32f40604244255f3c2a5bb78","modified":1584593486000},{"_id":"themes/matery/layout/_partial/search.ejs","hash":"942609b9240d5c8c09b24562fc8fb31eabe1cae4","modified":1584593486000},{"_id":"themes/matery/layout/_partial/share.ejs","hash":"34f8e4250bb66012026aa50686a7c89a0414ca1b","modified":1584593486000},{"_id":"themes/matery/layout/_partial/social-link.ejs","hash":"62e10bf4577946190e9c31dcdc2799a4ad1d00dd","modified":1584593486000},{"_id":"themes/matery/layout/_partial/valine.ejs","hash":"90527186fc8ed906eb1f20b59bc7f86caab9087b","modified":1584593486000},{"_id":"themes/matery/layout/_widget/category-cloud.ejs","hash":"a5a10d6fa66a389d0253d7a52e0a646af6e8e9be","modified":1584593486000},{"_id":"themes/matery/layout/_widget/category-radar.ejs","hash":"f5561dd7d53d68897a33090bf677719213459b19","modified":1584593486000},{"_id":"themes/matery/layout/_widget/dream.ejs","hash":"684450f0b42f89ab70370c5248b34e55b7adf6fc","modified":1584593486000},{"_id":"themes/matery/layout/_widget/music.ejs","hash":"8eafddbd73fed80e85c66d49837c1a241b087258","modified":1584593486000},{"_id":"themes/matery/layout/_widget/my-gallery.ejs","hash":"f81eb2891bea326908057029e2a063001371ba9b","modified":1584593486000},{"_id":"themes/matery/layout/_widget/my-projects.ejs","hash":"b9bf70ec5d97b0e14bb1b4f60f92db7680be5949","modified":1584593486000},{"_id":"themes/matery/layout/_widget/my-skills.ejs","hash":"bd0edf8dad95b2255890d59fb6d6ed6f2eab9c2f","modified":1584593486000},{"_id":"themes/matery/layout/_widget/post-calendar.ejs","hash":"0b0a3eb6af29bf0d55d535958c44b01c0f18d10d","modified":1584593486000},{"_id":"themes/matery/layout/_widget/recommend.ejs","hash":"babaa0cb32146870785449c70748721235e4eff0","modified":1584593486000},{"_id":"themes/matery/layout/_widget/post-charts.ejs","hash":"af0604623db37ef800bb7ad48028d18d99efbbc3","modified":1584593486000},{"_id":"themes/matery/layout/_widget/tag-cloud.ejs","hash":"a3725f0e3a405acb595b04630a27765b537fb580","modified":1584593486000},{"_id":"themes/matery/layout/_widget/tag-wordcloud.ejs","hash":"cb7a0151cd20e90351e151c22bca9d4c3112f234","modified":1584593486000},{"_id":"themes/matery/layout/_widget/video.ejs","hash":"bda810cc135b52f834f1c1ccf52defccacace714","modified":1584593486000},{"_id":"themes/matery/source/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1584593486000},{"_id":"themes/matery/source/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1584593486000},{"_id":"themes/matery/source/css/matery.css","hash":"d136162e907de9ff3ba170dfbea220b1e688e593","modified":1584593486000},{"_id":"themes/matery/source/css/my.css","hash":"3d3ce7c84fce447b3531432537b79117b08b8818","modified":1584593486000},{"_id":"themes/matery/source/js/matery.js","hash":"92f07106944f5ef7cd72e84bb3534513d00eebe1","modified":1584593486000},{"_id":"themes/matery/source/js/search.js","hash":"499e11786efbb04815b54a1de317cc8606a37555","modified":1584593486000},{"_id":"themes/matery/source/libs/.DS_Store","hash":"0a1db350271bd19f56cc0623c3c27990368a2296","modified":1584596815000},{"_id":"themes/matery/source/medias/avatar.jpg","hash":"b7f8ca0c682f95d93f002c845aafbcb508ec2b0f","modified":1584593486000},{"_id":"themes/matery/source/medias/logo.png","hash":"4050259723bd418648ec40028a8020364e57a6a3","modified":1584593486000},{"_id":"themes/matery/source/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1584593486000},{"_id":"themes/matery/source/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1584593486000},{"_id":"themes/matery/source/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1584593486000},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1584593486000},{"_id":"themes/matery/source/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1581995895000},{"_id":"themes/matery/source/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1581995895000},{"_id":"themes/matery/source/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1581995895000},{"_id":"themes/matery/source/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1581995895000},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1584593486000},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1584593486000},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","hash":"3aac1db83b0135c521187254ff302d125cc30706","modified":1584593486000},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1584593486000},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1584593486000},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1584593486000},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1584593486000},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1584593486000},{"_id":"themes/matery/source/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1584593486000},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1584593486000},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","hash":"15601837bf8557c2fd111e4450ed4c8495fd11a0","modified":1584593486000},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1584593486000},{"_id":"themes/matery/source/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/2.jpg","hash":"16f1d89cdba4dce935ac0f12599e0fcfda543a93","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/21.jpg","hash":"d70b088850c3565e5b5bb9eb8fe4abe688c964cf","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/22.jpg","hash":"bf5b59d193e5ca089a7fff034c222bfa2c4dc41f","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/23.jpg","hash":"ed5ac9f616d3b99af5188a10b1761884c37e93e5","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/5.jpg","hash":"c3c1f36a1b1886037db604f151f335cd4599e970","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/7.jpg","hash":"a0246a4a560438938489cdd154e35f172b3f31b0","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/8.jpg","hash":"5a46ca4ab4c4ab2101a2af77a31a8878bccc483c","modified":1584593486000},{"_id":"themes/matery/source/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1584593486000},{"_id":"themes/matery/source/medias/reward/wechat.png","hash":"fe93385aa92fe328e01c8221a80b039be9e4e140","modified":1584593486000},{"_id":"source/notes/img/零拷贝.png","hash":"b2e42de0f335105eb56aecc3fb6e653d71029ea6","modified":1584532336000},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1584593486000},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1584593486000},{"_id":"themes/matery/source/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1584593486000},{"_id":"themes/matery/source/libs/jquery/jquery-2.2.0.min.js","hash":"5d7e5bbfa540f0e53bd599e4305e1a4e815b5dd1","modified":1584593486000},{"_id":"themes/matery/source/libs/valine/Valine.min.js","hash":"031c1a5640d64ab3b829395ad5a7596b9fb122e6","modified":1584593486000},{"_id":"themes/matery/source/medias/banner/0.jpg","hash":"1f2ec55fe7825475fde2601573bb622f0bf2acba","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/0.jpg","hash":"2066cdda98ad0035071cd4aa7bd696eb078c0b6d","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/1.jpg","hash":"d16e28bd23ea3a63643826dde5eea6b7a9bdda5d","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/10.jpg","hash":"838e704942de076c60894d14e5f280e2724b6f68","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/14.jpg","hash":"8aeb816faca2d5eaea4cce9e881d6ff87b8c7cf1","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/15.jpg","hash":"5cf9fc64d5d74ab6ba69bb8bff580fdc22ba32d0","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/16.jpg","hash":"9cac6b80b0cc8959fc8aabfbd1adcab79ebebfc9","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/11.jpg","hash":"9ed45f95b83626e3d91d6c405eb8bfe6fcb9736a","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/12.jpg","hash":"047be4239dd7e0be83243ee6b49a392a61f16b9a","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/13.jpg","hash":"66706dfde7d910182c2f1dbadd0e9e917630b8dd","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/17.jpg","hash":"f168ca5b046d10a878a7b0bcfab540e2c4428887","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/18.jpg","hash":"ae23fdfaa59bc57b7ed49e90c5d59e4b68e1eea5","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/19.jpg","hash":"57bc7c804b78b5cceb4eb1f9e51b734b75151b71","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/20.jpg","hash":"8271c4a327632b566ea62f546c083d08a0528e72","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/3.jpg","hash":"5e879652e032f02961a331b598a50b60ebe80a39","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/4.jpg","hash":"4eea5bdb5724ef1ed65790e481eda0d2fb176bf0","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/6.jpg","hash":"c63ff64bdd5f6c82da8804c7248fc519d23eaf0b","modified":1584593486000},{"_id":"themes/matery/source/medias/featureimages/9.jpg","hash":"815c84778b721e3606c2bd7c099c7de7c53251ba","modified":1584593486000},{"_id":"themes/matery/source/libs/awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1584593486000},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","hash":"734f56442e62fe55f677e8ccae7f175445667767","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","hash":"fbfdbe06aebf7d0c00da175a4810cf888d128f11","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","hash":"1142b47de219dddfba2e712cd3189dec0c8b7bee","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","hash":"39150b45ec5fc03155b7ebeaa44f1829281788e2","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","hash":"9a732790adc004b22022cc60fd5f77ec4c8e3e5a","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1584593486000},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1584593486000},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","hash":"80ae4aa0dba3634dd9bf59586d541d2dd8d8191c","modified":1584593486000},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1584593486000},{"_id":"themes/matery/source/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1584593486000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1584593486000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","hash":"f0a1b849868a6bf351ff98dc3924a4e7254eb88b","modified":1584593486000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1584593486000},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1584593486000},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","hash":"16ce82901ca0e302cf47a35fb10f59009a5e7eb9","modified":1584593486000},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","hash":"4df722bafde2c5d8faaace0d1f894798385a8793","modified":1584593486000},{"_id":"themes/matery/source/libs/valine/av-min.js","hash":"2577e72b52b736d99649f9e95be8976d58563333","modified":1584593486000},{"_id":"themes/matery/source/medias/banner/1.jpg","hash":"c3d5ab183b39a7140941b8375e29498f9d24f343","modified":1584593486000},{"_id":"themes/matery/source/medias/banner/3.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1584593486000},{"_id":"themes/matery/source/medias/banner/5.jpg","hash":"4a08deec1dd5b4f1490e8fc23adfb75a0f88b0c4","modified":1584593486000},{"_id":"themes/matery/source/medias/banner/2.jpg","hash":"8d3c8391ff161eec70f66d69e5545a9468cc52ef","modified":1584593486000},{"_id":"themes/matery/source/medias/banner/6.jpg","hash":"62e9586a8cec91a160f147c424a3d1d1aea360f9","modified":1584593486000},{"_id":"source/notes/img/kafka架构.png","hash":"f0e0406e5310d389605609ce3c3225fa245280a1","modified":1584532336000},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1584593486000},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1584593486000},{"_id":"source/notes/img/普通拷贝.png","hash":"07b5d706b386ce490c66160d9cf9eb6d6d651b62","modified":1584532336000},{"_id":"themes/matery/source/libs/awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1584593486000},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1584593486000},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1584593486000},{"_id":"themes/matery/source/medias/banner/4.jpg","hash":"56850c3139cbd72a0eff0c35d8fac32c9c66dd6a","modified":1584593486000},{"_id":"themes/matery/source/libs/awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1584593486000},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1584593486000},{"_id":"public/atom.xml","hash":"6b54af90c9075ad7c092d1dea3aacf3ef383b3e9","modified":1584597442939},{"_id":"public/search.xml","hash":"ca99a2341ae3a929eb13ea1f9c778049ccae4702","modified":1584597442940},{"_id":"public/404.html","hash":"736e165794256558ec58fcd87b0282a1e5689f96","modified":1584597443405},{"_id":"public/categories/index.html","hash":"6d0905b72c86cc9fc4e794337c22c6dd4260a4f3","modified":1584597443405},{"_id":"public/notes/Github.html","hash":"6de458e1dab30d151542b6d746ac6edad7305519","modified":1584597443405},{"_id":"public/notes/Golang.html","hash":"238c6fe04bf5a746836c4cad9e35e4c4bcc24d10","modified":1584597443405},{"_id":"public/notes/index.html","hash":"40c73d86ec84586fb321aae8913c567851035004","modified":1584597443405},{"_id":"public/tags/index.html","hash":"a22027b2aac8a818845a1cc46a5ccf7e553b394a","modified":1584597443405},{"_id":"public/archives/2018/index.html","hash":"c00ea8c6601a9b761e5039125737fafa4ddb8f08","modified":1584597443406},{"_id":"public/archives/2018/09/index.html","hash":"b16ed252250561295ce90544d4d3859e7346282a","modified":1584597443406},{"_id":"public/archives/2019/index.html","hash":"99488db5c218d14f322da01bae4c6fec650d4a12","modified":1584597443406},{"_id":"public/archives/2019/12/index.html","hash":"75cf06875e2adad1e33988f7ea651be04b817958","modified":1584597443406},{"_id":"public/archives/2020/index.html","hash":"f70163eef5a60d2f3d3cc98f5531977f33d05a02","modified":1584597443406},{"_id":"public/archives/2020/01/index.html","hash":"a54b201783200d93040606f3f3359783f3541d29","modified":1584597443406},{"_id":"public/categories/前端/index.html","hash":"20e7783e2d5f86b9bbae4a06fa2c841e04ec7340","modified":1584597443406},{"_id":"public/categories/大数据面试题/index.html","hash":"db7122bc34b12a57c57b2df30ee7458703ad496f","modified":1584597443406},{"_id":"public/categories/Spark/index.html","hash":"08aa5b32c10eb95d652c6ed9e3394976ff67494d","modified":1584597443406},{"_id":"public/tags/Hexo/index.html","hash":"cf3960558ce96252b3cc44839f1082a3857a50c9","modified":1584597443407},{"_id":"public/tags/大数据面试题/index.html","hash":"1183d45bded05808e4a051cb7526e3a534cf91e0","modified":1584597443407},{"_id":"public/tags/Spark/index.html","hash":"33e292d86d8399bffb6bde7f9c548417aa43fe9f","modified":1584597443407},{"_id":"public/about/index.html","hash":"79676a1a2a181e83c0ba3c5e6e965683d4d76aa6","modified":1584597443407},{"_id":"public/about/resume的副本.html","hash":"e8a09d6ad5f3644c53d046bfab7fe03d2d0cc768","modified":1584597443407},{"_id":"public/friends/index.html","hash":"63ef519ef7ce95de9c0e3dd38bf11974d44f1f08","modified":1584597443407},{"_id":"public/notes/Docker.html","hash":"95e7bf185a80d5d77386d65944f8977a071e16c3","modified":1584597443407},{"_id":"public/notes/Bigdata.html","hash":"3543d6f615b17c337e01cf4c1ab8cd133ba542e3","modified":1584597443407},{"_id":"public/notes/Flink-Trouble.html","hash":"48c5409fcacf51de5e0f69d172cb063a2809f130","modified":1584597443408},{"_id":"public/notes/Hive.html","hash":"4fa9873b4d76337855c7f569b9d4b4f92059f1d8","modified":1584597443408},{"_id":"public/notes/Kafka.html","hash":"1a881669ca374119e8154caf3523a0d64ffa70f4","modified":1584597443408},{"_id":"public/notes/Linux.html","hash":"aedd926c74053efaea85daaa5a7a94c40637682d","modified":1584597443408},{"_id":"public/notes/Scala.html","hash":"ef97b1c5f4ce34a96d8fef5dc7f131e8482783d2","modified":1584597443408},{"_id":"public/notes/Spark.html","hash":"c57b83875c0f06c0d74b5e5ef0606cd6e3511a5c","modified":1584597443408},{"_id":"public/notes/Flink.html","hash":"3e34fd72fe6ea7fab4f91087e4338059771a60cf","modified":1584597443408},{"_id":"public/notes/Redis.html","hash":"856dc28a004c385413318e1d5621cb11c8d27739","modified":1584597443408},{"_id":"public/2020/01/15/da-shu-ju-mian-shi-ti/index.html","hash":"ff43606a470e733243b277fd56b15a4a3243e3ea","modified":1584597443408},{"_id":"public/2019/12/18/shou-si-spark-zhi-wordcount-rdd-zhi-xing-liu-cheng/index.html","hash":"ae2130d3f0a8a9309d75a1998aa172cae77c67d8","modified":1584597443409},{"_id":"public/2018/09/12/hello-world/index.html","hash":"4e74583b941e43d244192f6cf61dac515c01deb3","modified":1584597443409},{"_id":"public/archives/index.html","hash":"5e2753d59efa70fc29d79699ea1ff4a4ab5b7048","modified":1584597443409},{"_id":"public/index.html","hash":"15737ffbe14a9f52756fd972f0382b43fad4df08","modified":1584597443409},{"_id":"public/about/index.md.b","hash":"8c810b0130e86a9052564b986e568a57bbebc416","modified":1584597443424},{"_id":"public/about/index.md.bak","hash":"96c7cd8d539be6e58828ddf53a636aa21f4cef87","modified":1584597443424},{"_id":"public/about/index的副本.md.bak的副本","hash":"d57d7e7e3bdb23c6d4a294fbd2dd0492c5ade71c","modified":1584597443424},{"_id":"public/medias/avatar.jpg","hash":"b7f8ca0c682f95d93f002c845aafbcb508ec2b0f","modified":1584597443424},{"_id":"public/medias/logo.png","hash":"4050259723bd418648ec40028a8020364e57a6a3","modified":1584597443424},{"_id":"public/medias/featureimages/23.jpg","hash":"ed5ac9f616d3b99af5188a10b1761884c37e93e5","modified":1584597443424},{"_id":"public/medias/featureimages/8.jpg","hash":"5a46ca4ab4c4ab2101a2af77a31a8878bccc483c","modified":1584597443424},{"_id":"public/medias/featureimages/15.jpg","hash":"5cf9fc64d5d74ab6ba69bb8bff580fdc22ba32d0","modified":1584597443424},{"_id":"public/medias/featureimages/16.jpg","hash":"9cac6b80b0cc8959fc8aabfbd1adcab79ebebfc9","modified":1584597443425},{"_id":"public/medias/featureimages/13.jpg","hash":"66706dfde7d910182c2f1dbadd0e9e917630b8dd","modified":1584597443425},{"_id":"public/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1584597443425},{"_id":"public/libs/lightGallery/img/video-play.png","hash":"fbfdbe06aebf7d0c00da175a4810cf888d128f11","modified":1584597443425},{"_id":"public/libs/lightGallery/img/vimeo-play.png","hash":"1142b47de219dddfba2e712cd3189dec0c8b7bee","modified":1584597443425},{"_id":"public/libs/lightGallery/img/youtube-play.png","hash":"39150b45ec5fc03155b7ebeaa44f1829281788e2","modified":1584597443425},{"_id":"public/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1584597443425},{"_id":"public/libs/lightGallery/fonts/lg.svg","hash":"9a732790adc004b22022cc60fd5f77ec4c8e3e5a","modified":1584597443425},{"_id":"public/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1584597443425},{"_id":"public/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1584597443425},{"_id":"public/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1584597443425},{"_id":"public/libs/share/fonts/iconfont.svg","hash":"f0a1b849868a6bf351ff98dc3924a4e7254eb88b","modified":1584597443425},{"_id":"public/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1584597443425},{"_id":"public/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1584597443425},{"_id":"public/css/prism-tomorrow.css","hash":"3b99487dfc9b4e51e9105a93743b92a761840e34","modified":1584597443426},{"_id":"public/favicon.png","hash":"20674c497b75fc889194b47fd18ecea12303d8ec","modified":1584597443436},{"_id":"public/medias/featureimages/2.jpg","hash":"16f1d89cdba4dce935ac0f12599e0fcfda543a93","modified":1584597443436},{"_id":"public/medias/featureimages/22.jpg","hash":"bf5b59d193e5ca089a7fff034c222bfa2c4dc41f","modified":1584597443437},{"_id":"public/medias/featureimages/21.jpg","hash":"d70b088850c3565e5b5bb9eb8fe4abe688c964cf","modified":1584597443437},{"_id":"public/medias/featureimages/7.jpg","hash":"a0246a4a560438938489cdd154e35f172b3f31b0","modified":1584597443437},{"_id":"public/medias/reward/wechat.png","hash":"fe93385aa92fe328e01c8221a80b039be9e4e140","modified":1584597443437},{"_id":"public/medias/featureimages/5.jpg","hash":"c3c1f36a1b1886037db604f151f335cd4599e970","modified":1584597443437},{"_id":"public/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1584597443437},{"_id":"public/medias/banner/0.jpg","hash":"1f2ec55fe7825475fde2601573bb622f0bf2acba","modified":1584597443437},{"_id":"public/medias/featureimages/0.jpg","hash":"2066cdda98ad0035071cd4aa7bd696eb078c0b6d","modified":1584597443437},{"_id":"public/medias/featureimages/1.jpg","hash":"d16e28bd23ea3a63643826dde5eea6b7a9bdda5d","modified":1584597443437},{"_id":"public/medias/featureimages/10.jpg","hash":"838e704942de076c60894d14e5f280e2724b6f68","modified":1584597443437},{"_id":"public/medias/featureimages/14.jpg","hash":"8aeb816faca2d5eaea4cce9e881d6ff87b8c7cf1","modified":1584597443438},{"_id":"public/medias/featureimages/11.jpg","hash":"9ed45f95b83626e3d91d6c405eb8bfe6fcb9736a","modified":1584597443438},{"_id":"public/medias/featureimages/12.jpg","hash":"047be4239dd7e0be83243ee6b49a392a61f16b9a","modified":1584597443438},{"_id":"public/medias/featureimages/17.jpg","hash":"f168ca5b046d10a878a7b0bcfab540e2c4428887","modified":1584597443438},{"_id":"public/medias/featureimages/18.jpg","hash":"ae23fdfaa59bc57b7ed49e90c5d59e4b68e1eea5","modified":1584597443438},{"_id":"public/medias/featureimages/19.jpg","hash":"57bc7c804b78b5cceb4eb1f9e51b734b75151b71","modified":1584597443438},{"_id":"public/medias/featureimages/20.jpg","hash":"8271c4a327632b566ea62f546c083d08a0528e72","modified":1584597443438},{"_id":"public/medias/featureimages/3.jpg","hash":"5e879652e032f02961a331b598a50b60ebe80a39","modified":1584597443439},{"_id":"public/medias/featureimages/6.jpg","hash":"c63ff64bdd5f6c82da8804c7248fc519d23eaf0b","modified":1584597443439},{"_id":"public/medias/featureimages/4.jpg","hash":"4eea5bdb5724ef1ed65790e481eda0d2fb176bf0","modified":1584597443439},{"_id":"public/medias/featureimages/9.jpg","hash":"815c84778b721e3606c2bd7c099c7de7c53251ba","modified":1584597443439},{"_id":"public/medias/banner/2.jpg","hash":"8d3c8391ff161eec70f66d69e5545a9468cc52ef","modified":1584597443439},{"_id":"public/libs/awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1584597443439},{"_id":"public/libs/awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1584597443439},{"_id":"public/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1584597443450},{"_id":"public/css/my-gitalk.css","hash":"eeda46a83d0db1cc239a9cd27d544faf663f9883","modified":1584597443450},{"_id":"public/css/my.css","hash":"3d3ce7c84fce447b3531432537b79117b08b8818","modified":1584597443451},{"_id":"public/js/search.js","hash":"499e11786efbb04815b54a1de317cc8606a37555","modified":1584597443451},{"_id":"public/js/matery.js","hash":"92f07106944f5ef7cd72e84bb3534513d00eebe1","modified":1584597443451},{"_id":"public/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1584597443451},{"_id":"public/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1584597443451},{"_id":"public/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1584597443451},{"_id":"public/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1584597443451},{"_id":"public/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1584597443451},{"_id":"public/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1584597443451},{"_id":"public/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1584597443451},{"_id":"public/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1584597443451},{"_id":"public/libs/tocbot/tocbot.css","hash":"15601837bf8557c2fd111e4450ed4c8495fd11a0","modified":1584597443452},{"_id":"public/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1584597443452},{"_id":"public/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1584597443452},{"_id":"public/medias/banner/1.jpg","hash":"c3d5ab183b39a7140941b8375e29498f9d24f343","modified":1584597443452},{"_id":"public/medias/banner/3.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1584597443452},{"_id":"public/medias/banner/5.jpg","hash":"4a08deec1dd5b4f1490e8fc23adfb75a0f88b0c4","modified":1584597443453},{"_id":"public/medias/banner/6.jpg","hash":"62e9586a8cec91a160f147c424a3d1d1aea360f9","modified":1584597443453},{"_id":"public/libs/awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1584597443453},{"_id":"public/libs/awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1584597443453},{"_id":"public/libs/awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1584597443453},{"_id":"public/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1584597443461},{"_id":"public/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1584597443461},{"_id":"public/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1584597443463},{"_id":"public/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1584597443463},{"_id":"public/libs/gitalk/gitalk.css","hash":"3aac1db83b0135c521187254ff302d125cc30706","modified":1584597443474},{"_id":"public/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1584597443474},{"_id":"public/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1584597443475},{"_id":"public/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1584597443475},{"_id":"public/libs/share/js/social-share.min.js","hash":"4df722bafde2c5d8faaace0d1f894798385a8793","modified":1584597443475},{"_id":"public/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1584597443492},{"_id":"public/libs/awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1584597443492},{"_id":"public/libs/share/js/jquery.share.min.js","hash":"16ce82901ca0e302cf47a35fb10f59009a5e7eb9","modified":1584597443493},{"_id":"public/medias/banner/4.jpg","hash":"56850c3139cbd72a0eff0c35d8fac32c9c66dd6a","modified":1584597443493},{"_id":"public/css/matery.css","hash":"d136162e907de9ff3ba170dfbea220b1e688e593","modified":1584597443556},{"_id":"public/notes/img/零拷贝.png","hash":"b2e42de0f335105eb56aecc3fb6e653d71029ea6","modified":1584597443556},{"_id":"public/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1584597443640},{"_id":"public/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1584597443640},{"_id":"public/notes/img/kafka架构.png","hash":"f0e0406e5310d389605609ce3c3225fa245280a1","modified":1584597443642},{"_id":"public/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1584597443655},{"_id":"public/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1584597443655},{"_id":"public/notes/img/普通拷贝.png","hash":"07b5d706b386ce490c66160d9cf9eb6d6d651b62","modified":1584597443656},{"_id":"public/libs/awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1584597443656},{"_id":"public/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1584597443669},{"_id":"public/libs/valine/Valine.min.js","hash":"031c1a5640d64ab3b829395ad5a7596b9fb122e6","modified":1584597443744},{"_id":"public/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1584597443748},{"_id":"public/libs/jquery/jquery-2.2.0.min.js","hash":"5d7e5bbfa540f0e53bd599e4305e1a4e815b5dd1","modified":1584597443751},{"_id":"public/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1584597443758},{"_id":"public/libs/materialize/materialize.min.css","hash":"580459a012f556fba86438953062013a94b201af","modified":1584597443768},{"_id":"public/libs/valine/av-min.js","hash":"2577e72b52b736d99649f9e95be8976d58563333","modified":1584597443769},{"_id":"public/libs/gitalk/gitalk.min.js","hash":"734f56442e62fe55f677e8ccae7f175445667767","modified":1584597443772},{"_id":"public/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1584597443778},{"_id":"public/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1584597443810}],"Category":[{"name":"前端","_id":"ck7ycejg40004jr64xlx3uf76"},{"name":"大数据面试题","_id":"ck7ycejhg000qjr64u0sekcjf"},{"name":"Spark","_id":"ck7ycejhh000sjr64zovfsnnp"}],"Data":[{"_id":"friends","data":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}]},{"_id":"musics","data":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}],"Page":[{"title":"404","date":"2020-03-19T05:09:33.000Z","type":"404","layout":"404","description":"Oops～，你来到了没有知识的荒原！ :(","_content":"\n","source":"404.md","raw":"---\ntitle: 404\ndate: \ntype: \"404\"\nlayout: \"404\"\ndescription: \"Oops～，你来到了没有知识的荒原！ :(\"\n---\n\n","updated":"2020-03-18T11:52:16.000Z","path":"404.html","comments":1,"_id":"ck7ycejdk0000jr64ng6in9w2","content":"","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":""},{"title":"about","date":"2018-10-05T08:33:28.000Z","type":"about","layout":"about","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2018-10-05 16:33:28\ntype: \"about\"\nlayout: \"about\"\n---\n","updated":"2020-03-19T04:51:26.000Z","path":"about/index.html","comments":1,"_id":"ck7ycejfy0002jr64fvgs7rh5","content":"","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":""},{"title":"个人简历","date":"2019-12-25T03:50:00.000Z","tags":null,"_content":"\n\n#  个人信息\n\n手机|微信：18851659629\t邮箱：bulianwei@gmail.com\t期望职位：大数据开发\t期望薪资：22k～25k\t期望城市：北京\t博客：https://blog.csdn.net/branwel | http://bulianwei.github.io \n\n# 专业技能\n\n- 掌握离线数仓（Hive|MapReduce|Spark）\n- 掌握实时数仓（Flink|Spark）\n- 掌握Kafka消息队列\n- 掌握Redis分布式缓存\n- 掌握Spark性能调优\n- 熟悉使用Docker容器技术\n- 熟悉JVM虚拟机，配置生产环境中JVM参数，可以快速定位CPU占用过高等问题\n- 熟悉JUC多线程高并发\n- 熟悉GC垃圾回收\n- 熟练使用Git代码管理工具\n- 熟悉数仓分层建设+权限控制\n\n# 工作经历\n\n ## 轻图信息技术(北京)有限公司 \t\t\n\n大数据开发工程师 | 数据治理部门\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（ 2017年12月 ~ 2019年6月）\n\n  ### 位置融合集市\n\n位置融合是数仓的基础集市层，从Kafka推送过来的DDR，OIDD，DPI，WCDR数据直接存在HDFS上，位置融合就是对这些数据进行位置相关的处理操作，为数仓提供高质量的基础数据。\n\n   #### 开发环境：\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)     Java+MR\n\n   #### 职能描述：\n\n- 前期实现与客户对接，进行需求的前期澄清\n- 设计需求所需的算法模型，与客户沟通算法模型，对算法模型进行沟通后的优化\n- 核心算法模型的代码实现\n- 辅助开发组小伙伴针对其他模块代码进行开发\n- 和测试组小伙伴针将开发的项目测试上线\n- 维护项目，bug查找与修复\n\n\n  ### 旅游集市\n\n旅游集市是针对新疆大数据旅游项目做的专项研发，后期将算应用于全国。为出游规划和游客位置信息进行的详细展示。\n\n   #### 开发环境\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Scala(2.11)+SparkStreaming(2.1.0)+Kafka(2.1.0)  Scala+SparkStreaming+Kafka\n\n   #### 职能描述\n\n- 前期与客户进行需求澄清\n- 针对沟通过的需求进行算法设计并与客户沟通算法模型\n- 协同开发小伙伴和测试小伙伴对项目进行开发测试上线\n- 维护和优化项目\n\n\n  ### 智慧城市集市\n\n智慧城市集市是给予数仓的基础数据，汇总统计单位用户在省、市、区县的停留记录。智慧城市是针对用户在范围上一次停留记录的粗粒的汇总。将一个省、市、区县内的用户数据做的合并，为专项集市提供过滤数据。\n\n   #### 开发环境\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)     Java+MR\n\n   #### 职能描述\n\n- 与客户进行前期需求澄清\n- 针对前期澄清过的需求进行算法模型实现\n- 开发项目代码，测试代码\n- 维护项目\n\n\n  ### 商业地产集市 \n\n商业地产集市是基于运营商信息数据在数仓的一个子系统。数据来源于运营商的各类数据，基于这些数据进行加工汇总，从而生成一系列数据指标，根据这些数据指标来来解决传统商业模式下的招商难、选址难、运营难的“三难”困境。商业地产集市基于数仓，分为专题层、宽表层、DI层、指标层四层业务结构。\n\n   ####  开发环境\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Hive(1.1.0)    HQL+Shell\n\n   #### 职能描述\n\n- 负责商场/商圈指标日/月表算法逻辑设计、脚本开发\n- 负责后期线上问题跟踪和Bug解决\n\n\n  ### 掌合天下数仓\n\n掌合天下在品牌推广、市场竞争等多方面背景下，为充分体现加强信息挖掘、加强信息化管理、加强科学定量决策的管理理念，初步决定建立掌合天下数仓建设。数仓初步建设包括驾驶舱、订单、用户、商品、财务、风险在内的6个主要集市。\n\n   #### 开发环境\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Hive(1.1.0)     HQL+Shell+Azkaban+Tableau\n\n   #### 职能描述\n\n- 负责订单、用户集市的设计和开发\n- 负责Azkaban维护\n- 负责后期集市维护，Bug解决\n\n\n\n ## 北京中科弘睿科技有限公司\t \n大数据开发工程师 | 大数据部门\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（ 2016年3月 ~ 2017年10月 ）\n\n\n  ### 电子商城离线分析系统\n\n用户每天在电子商城平台上的点击、浏览、下单支付行为都会产生大量的浏览日志， 这些日志数据被汇总后，经过后期的过滤、分析、挖掘与学习后，为电子商城的各种推 荐、搜索系统甚至对公司以后的战略发展 供数据支持。\n电子商城离线分析系统总共分为用户数据基本分析模块、浏览器分析模块、地域分 析模块、外链分析模块、用户浏览深度分析模块、事件分析模块和订单分析模块等七大 主要模块。本系统利用这七大模块将有效数据以数据或图表的方式展现出来，供各业务部门参考。\n\n   #### 职能描述\n\n- 分析商城网页得出PV、UV、二跳率等关键性指标\n- 开发Spark代码对数据进行初级清洗、脱敏操作，并将处理之后的数据放入MySQL数据库\n- 开发Spark SQL代码针对用户基本数据分析模块编写单维度分析\n\n\n  ### 驾车习性系统\n\n驾车习性统分系统，是一个统计、分析用户驾车习惯的平台。通过对用户驾驶习性 的统计、分析，可得出用户的使用密度和一些经常出现的地方(家或者公司)。公司通 过分析出来的资料与相应的保险公司进行合作，建立大规模的用户数据库。此外，平台 还可以统计出某品牌车的销售Top区域， 高了该品牌对市场的掌握程度，从而决策下 一步的宣称、推广计划。\n\n   #### 职能描述\n\n- 使用Flume将日志数据文件传输、储存在HDFS中\n- 开发MapReduce代码对HDFS中数据进行清洗、脱敏操作\n\n\n\n ## 联想（北京）有限公司\t\t\t\t \n移动互联产品开发助理 | E&T\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（ 2015年3月 ~ 2016年1月 ）\n\n\n   #### 职能描述\n\n- 开发比赛软件项目代码（Android），并为比赛提供软件技术支持\n- 撰写Android开发教材数据存储模块\n\n\n# 教育背景\n2012.9 ～ 2015.7\t\t山东劳动职业技术学院\t大专\t软件工程\t\n\n# 其他\n\n- 荣誉/奖项：国家励志奖，省政府励志奖，学院奖，移动互联网全国软件开发二等奖\n\n\n\n# 致谢\n\n感谢您花时间阅读我的简历，期待能有机会和您共事。\n\n","source":"about/resume的副本.md","raw":"---\ntitle:  个人简历\ndate: 2019-12-25 11:50:00\ntags:\n---\n\n\n#  个人信息\n\n手机|微信：18851659629\t邮箱：bulianwei@gmail.com\t期望职位：大数据开发\t期望薪资：22k～25k\t期望城市：北京\t博客：https://blog.csdn.net/branwel | http://bulianwei.github.io \n\n# 专业技能\n\n- 掌握离线数仓（Hive|MapReduce|Spark）\n- 掌握实时数仓（Flink|Spark）\n- 掌握Kafka消息队列\n- 掌握Redis分布式缓存\n- 掌握Spark性能调优\n- 熟悉使用Docker容器技术\n- 熟悉JVM虚拟机，配置生产环境中JVM参数，可以快速定位CPU占用过高等问题\n- 熟悉JUC多线程高并发\n- 熟悉GC垃圾回收\n- 熟练使用Git代码管理工具\n- 熟悉数仓分层建设+权限控制\n\n# 工作经历\n\n ## 轻图信息技术(北京)有限公司 \t\t\n\n大数据开发工程师 | 数据治理部门\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（ 2017年12月 ~ 2019年6月）\n\n  ### 位置融合集市\n\n位置融合是数仓的基础集市层，从Kafka推送过来的DDR，OIDD，DPI，WCDR数据直接存在HDFS上，位置融合就是对这些数据进行位置相关的处理操作，为数仓提供高质量的基础数据。\n\n   #### 开发环境：\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)     Java+MR\n\n   #### 职能描述：\n\n- 前期实现与客户对接，进行需求的前期澄清\n- 设计需求所需的算法模型，与客户沟通算法模型，对算法模型进行沟通后的优化\n- 核心算法模型的代码实现\n- 辅助开发组小伙伴针对其他模块代码进行开发\n- 和测试组小伙伴针将开发的项目测试上线\n- 维护项目，bug查找与修复\n\n\n  ### 旅游集市\n\n旅游集市是针对新疆大数据旅游项目做的专项研发，后期将算应用于全国。为出游规划和游客位置信息进行的详细展示。\n\n   #### 开发环境\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Scala(2.11)+SparkStreaming(2.1.0)+Kafka(2.1.0)  Scala+SparkStreaming+Kafka\n\n   #### 职能描述\n\n- 前期与客户进行需求澄清\n- 针对沟通过的需求进行算法设计并与客户沟通算法模型\n- 协同开发小伙伴和测试小伙伴对项目进行开发测试上线\n- 维护和优化项目\n\n\n  ### 智慧城市集市\n\n智慧城市集市是给予数仓的基础数据，汇总统计单位用户在省、市、区县的停留记录。智慧城市是针对用户在范围上一次停留记录的粗粒的汇总。将一个省、市、区县内的用户数据做的合并，为专项集市提供过滤数据。\n\n   #### 开发环境\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)     Java+MR\n\n   #### 职能描述\n\n- 与客户进行前期需求澄清\n- 针对前期澄清过的需求进行算法模型实现\n- 开发项目代码，测试代码\n- 维护项目\n\n\n  ### 商业地产集市 \n\n商业地产集市是基于运营商信息数据在数仓的一个子系统。数据来源于运营商的各类数据，基于这些数据进行加工汇总，从而生成一系列数据指标，根据这些数据指标来来解决传统商业模式下的招商难、选址难、运营难的“三难”困境。商业地产集市基于数仓，分为专题层、宽表层、DI层、指标层四层业务结构。\n\n   ####  开发环境\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Hive(1.1.0)    HQL+Shell\n\n   #### 职能描述\n\n- 负责商场/商圈指标日/月表算法逻辑设计、脚本开发\n- 负责后期线上问题跟踪和Bug解决\n\n\n  ### 掌合天下数仓\n\n掌合天下在品牌推广、市场竞争等多方面背景下，为充分体现加强信息挖掘、加强信息化管理、加强科学定量决策的管理理念，初步决定建立掌合天下数仓建设。数仓初步建设包括驾驶舱、订单、用户、商品、财务、风险在内的6个主要集市。\n\n   #### 开发环境\n\nJDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Hive(1.1.0)     HQL+Shell+Azkaban+Tableau\n\n   #### 职能描述\n\n- 负责订单、用户集市的设计和开发\n- 负责Azkaban维护\n- 负责后期集市维护，Bug解决\n\n\n\n ## 北京中科弘睿科技有限公司\t \n大数据开发工程师 | 大数据部门\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（ 2016年3月 ~ 2017年10月 ）\n\n\n  ### 电子商城离线分析系统\n\n用户每天在电子商城平台上的点击、浏览、下单支付行为都会产生大量的浏览日志， 这些日志数据被汇总后，经过后期的过滤、分析、挖掘与学习后，为电子商城的各种推 荐、搜索系统甚至对公司以后的战略发展 供数据支持。\n电子商城离线分析系统总共分为用户数据基本分析模块、浏览器分析模块、地域分 析模块、外链分析模块、用户浏览深度分析模块、事件分析模块和订单分析模块等七大 主要模块。本系统利用这七大模块将有效数据以数据或图表的方式展现出来，供各业务部门参考。\n\n   #### 职能描述\n\n- 分析商城网页得出PV、UV、二跳率等关键性指标\n- 开发Spark代码对数据进行初级清洗、脱敏操作，并将处理之后的数据放入MySQL数据库\n- 开发Spark SQL代码针对用户基本数据分析模块编写单维度分析\n\n\n  ### 驾车习性系统\n\n驾车习性统分系统，是一个统计、分析用户驾车习惯的平台。通过对用户驾驶习性 的统计、分析，可得出用户的使用密度和一些经常出现的地方(家或者公司)。公司通 过分析出来的资料与相应的保险公司进行合作，建立大规模的用户数据库。此外，平台 还可以统计出某品牌车的销售Top区域， 高了该品牌对市场的掌握程度，从而决策下 一步的宣称、推广计划。\n\n   #### 职能描述\n\n- 使用Flume将日志数据文件传输、储存在HDFS中\n- 开发MapReduce代码对HDFS中数据进行清洗、脱敏操作\n\n\n\n ## 联想（北京）有限公司\t\t\t\t \n移动互联产品开发助理 | E&T\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t（ 2015年3月 ~ 2016年1月 ）\n\n\n   #### 职能描述\n\n- 开发比赛软件项目代码（Android），并为比赛提供软件技术支持\n- 撰写Android开发教材数据存储模块\n\n\n# 教育背景\n2012.9 ～ 2015.7\t\t山东劳动职业技术学院\t大专\t软件工程\t\n\n# 其他\n\n- 荣誉/奖项：国家励志奖，省政府励志奖，学院奖，移动互联网全国软件开发二等奖\n\n\n\n# 致谢\n\n感谢您花时间阅读我的简历，期待能有机会和您共事。\n\n","updated":"2020-03-18T11:52:16.000Z","path":"about/resume的副本.html","comments":1,"layout":"page","_id":"ck7ycejg20003jr64qzrr091b","content":"<h1 id=\"个人信息\"><a href=\"#个人信息\" class=\"headerlink\" title=\"个人信息\"></a>个人信息</h1><p>手机|微信：18851659629    邮箱：<a href=\"mailto:bulianwei@gmail.com\" target=\"_blank\" rel=\"noopener\">bulianwei@gmail.com</a>    期望职位：大数据开发    期望薪资：22k～25k    期望城市：北京    博客：<a href=\"https://blog.csdn.net/branwel\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/branwel</a> | <a href=\"http://bulianwei.github.io\">http://bulianwei.github.io</a> </p>\n<h1 id=\"专业技能\"><a href=\"#专业技能\" class=\"headerlink\" title=\"专业技能\"></a>专业技能</h1><ul>\n<li>掌握离线数仓（Hive|MapReduce|Spark）</li>\n<li>掌握实时数仓（Flink|Spark）</li>\n<li>掌握Kafka消息队列</li>\n<li>掌握Redis分布式缓存</li>\n<li>掌握Spark性能调优</li>\n<li>熟悉使用Docker容器技术</li>\n<li>熟悉JVM虚拟机，配置生产环境中JVM参数，可以快速定位CPU占用过高等问题</li>\n<li>熟悉JUC多线程高并发</li>\n<li>熟悉GC垃圾回收</li>\n<li>熟练使用Git代码管理工具</li>\n<li>熟悉数仓分层建设+权限控制</li>\n</ul>\n<h1 id=\"工作经历\"><a href=\"#工作经历\" class=\"headerlink\" title=\"工作经历\"></a>工作经历</h1><h2 id=\"轻图信息技术-北京-有限公司\"><a href=\"#轻图信息技术-北京-有限公司\" class=\"headerlink\" title=\"轻图信息技术(北京)有限公司\"></a>轻图信息技术(北京)有限公司</h2><p>大数据开发工程师 | 数据治理部门                                                                                （ 2017年12月 ~ 2019年6月）</p>\n<h3 id=\"位置融合集市\"><a href=\"#位置融合集市\" class=\"headerlink\" title=\"位置融合集市\"></a>位置融合集市</h3><p>位置融合是数仓的基础集市层，从Kafka推送过来的DDR，OIDD，DPI，WCDR数据直接存在HDFS上，位置融合就是对这些数据进行位置相关的处理操作，为数仓提供高质量的基础数据。</p>\n<h4 id=\"开发环境：\"><a href=\"#开发环境：\" class=\"headerlink\" title=\"开发环境：\"></a>开发环境：</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)     Java+MR</p>\n<h4 id=\"职能描述：\"><a href=\"#职能描述：\" class=\"headerlink\" title=\"职能描述：\"></a>职能描述：</h4><ul>\n<li>前期实现与客户对接，进行需求的前期澄清</li>\n<li>设计需求所需的算法模型，与客户沟通算法模型，对算法模型进行沟通后的优化</li>\n<li>核心算法模型的代码实现</li>\n<li>辅助开发组小伙伴针对其他模块代码进行开发</li>\n<li>和测试组小伙伴针将开发的项目测试上线</li>\n<li>维护项目，bug查找与修复</li>\n</ul>\n<h3 id=\"旅游集市\"><a href=\"#旅游集市\" class=\"headerlink\" title=\"旅游集市\"></a>旅游集市</h3><p>旅游集市是针对新疆大数据旅游项目做的专项研发，后期将算应用于全国。为出游规划和游客位置信息进行的详细展示。</p>\n<h4 id=\"开发环境\"><a href=\"#开发环境\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Scala(2.11)+SparkStreaming(2.1.0)+Kafka(2.1.0)  Scala+SparkStreaming+Kafka</p>\n<h4 id=\"职能描述\"><a href=\"#职能描述\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>前期与客户进行需求澄清</li>\n<li>针对沟通过的需求进行算法设计并与客户沟通算法模型</li>\n<li>协同开发小伙伴和测试小伙伴对项目进行开发测试上线</li>\n<li>维护和优化项目</li>\n</ul>\n<h3 id=\"智慧城市集市\"><a href=\"#智慧城市集市\" class=\"headerlink\" title=\"智慧城市集市\"></a>智慧城市集市</h3><p>智慧城市集市是给予数仓的基础数据，汇总统计单位用户在省、市、区县的停留记录。智慧城市是针对用户在范围上一次停留记录的粗粒的汇总。将一个省、市、区县内的用户数据做的合并，为专项集市提供过滤数据。</p>\n<h4 id=\"开发环境-1\"><a href=\"#开发环境-1\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)     Java+MR</p>\n<h4 id=\"职能描述-1\"><a href=\"#职能描述-1\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>与客户进行前期需求澄清</li>\n<li>针对前期澄清过的需求进行算法模型实现</li>\n<li>开发项目代码，测试代码</li>\n<li>维护项目</li>\n</ul>\n<h3 id=\"商业地产集市\"><a href=\"#商业地产集市\" class=\"headerlink\" title=\"商业地产集市\"></a>商业地产集市</h3><p>商业地产集市是基于运营商信息数据在数仓的一个子系统。数据来源于运营商的各类数据，基于这些数据进行加工汇总，从而生成一系列数据指标，根据这些数据指标来来解决传统商业模式下的招商难、选址难、运营难的“三难”困境。商业地产集市基于数仓，分为专题层、宽表层、DI层、指标层四层业务结构。</p>\n<h4 id=\"开发环境-2\"><a href=\"#开发环境-2\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Hive(1.1.0)    HQL+Shell</p>\n<h4 id=\"职能描述-2\"><a href=\"#职能描述-2\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>负责商场/商圈指标日/月表算法逻辑设计、脚本开发</li>\n<li>负责后期线上问题跟踪和Bug解决</li>\n</ul>\n<h3 id=\"掌合天下数仓\"><a href=\"#掌合天下数仓\" class=\"headerlink\" title=\"掌合天下数仓\"></a>掌合天下数仓</h3><p>掌合天下在品牌推广、市场竞争等多方面背景下，为充分体现加强信息挖掘、加强信息化管理、加强科学定量决策的管理理念，初步决定建立掌合天下数仓建设。数仓初步建设包括驾驶舱、订单、用户、商品、财务、风险在内的6个主要集市。</p>\n<h4 id=\"开发环境-3\"><a href=\"#开发环境-3\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Hive(1.1.0)     HQL+Shell+Azkaban+Tableau</p>\n<h4 id=\"职能描述-3\"><a href=\"#职能描述-3\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>负责订单、用户集市的设计和开发</li>\n<li>负责Azkaban维护</li>\n<li>负责后期集市维护，Bug解决</li>\n</ul>\n<h2 id=\"北京中科弘睿科技有限公司\"><a href=\"#北京中科弘睿科技有限公司\" class=\"headerlink\" title=\"北京中科弘睿科技有限公司\"></a>北京中科弘睿科技有限公司</h2><p>大数据开发工程师 | 大数据部门                                                                                    （ 2016年3月 ~ 2017年10月 ）</p>\n<h3 id=\"电子商城离线分析系统\"><a href=\"#电子商城离线分析系统\" class=\"headerlink\" title=\"电子商城离线分析系统\"></a>电子商城离线分析系统</h3><p>用户每天在电子商城平台上的点击、浏览、下单支付行为都会产生大量的浏览日志， 这些日志数据被汇总后，经过后期的过滤、分析、挖掘与学习后，为电子商城的各种推 荐、搜索系统甚至对公司以后的战略发展 供数据支持。<br>电子商城离线分析系统总共分为用户数据基本分析模块、浏览器分析模块、地域分 析模块、外链分析模块、用户浏览深度分析模块、事件分析模块和订单分析模块等七大 主要模块。本系统利用这七大模块将有效数据以数据或图表的方式展现出来，供各业务部门参考。</p>\n<h4 id=\"职能描述-4\"><a href=\"#职能描述-4\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>分析商城网页得出PV、UV、二跳率等关键性指标</li>\n<li>开发Spark代码对数据进行初级清洗、脱敏操作，并将处理之后的数据放入MySQL数据库</li>\n<li>开发Spark SQL代码针对用户基本数据分析模块编写单维度分析</li>\n</ul>\n<h3 id=\"驾车习性系统\"><a href=\"#驾车习性系统\" class=\"headerlink\" title=\"驾车习性系统\"></a>驾车习性系统</h3><p>驾车习性统分系统，是一个统计、分析用户驾车习惯的平台。通过对用户驾驶习性 的统计、分析，可得出用户的使用密度和一些经常出现的地方(家或者公司)。公司通 过分析出来的资料与相应的保险公司进行合作，建立大规模的用户数据库。此外，平台 还可以统计出某品牌车的销售Top区域， 高了该品牌对市场的掌握程度，从而决策下 一步的宣称、推广计划。</p>\n<h4 id=\"职能描述-5\"><a href=\"#职能描述-5\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>使用Flume将日志数据文件传输、储存在HDFS中</li>\n<li>开发MapReduce代码对HDFS中数据进行清洗、脱敏操作</li>\n</ul>\n<h2 id=\"联想（北京）有限公司\"><a href=\"#联想（北京）有限公司\" class=\"headerlink\" title=\"联想（北京）有限公司\"></a>联想（北京）有限公司</h2><p>移动互联产品开发助理 | E&amp;T                                                                                        （ 2015年3月 ~ 2016年1月 ）</p>\n<h4 id=\"职能描述-6\"><a href=\"#职能描述-6\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>开发比赛软件项目代码（Android），并为比赛提供软件技术支持</li>\n<li>撰写Android开发教材数据存储模块</li>\n</ul>\n<h1 id=\"教育背景\"><a href=\"#教育背景\" class=\"headerlink\" title=\"教育背景\"></a>教育背景</h1><p>2012.9 ～ 2015.7        山东劳动职业技术学院    大专    软件工程    </p>\n<h1 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h1><ul>\n<li>荣誉/奖项：国家励志奖，省政府励志奖，学院奖，移动互联网全国软件开发二等奖</li>\n</ul>\n<h1 id=\"致谢\"><a href=\"#致谢\" class=\"headerlink\" title=\"致谢\"></a>致谢</h1><p>感谢您花时间阅读我的简历，期待能有机会和您共事。</p>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"个人信息\"><a href=\"#个人信息\" class=\"headerlink\" title=\"个人信息\"></a>个人信息</h1><p>手机|微信：18851659629    邮箱：<a href=\"mailto:bulianwei@gmail.com\" target=\"_blank\" rel=\"noopener\">bulianwei@gmail.com</a>    期望职位：大数据开发    期望薪资：22k～25k    期望城市：北京    博客：<a href=\"https://blog.csdn.net/branwel\" target=\"_blank\" rel=\"noopener\">https://blog.csdn.net/branwel</a> | <a href=\"http://bulianwei.github.io\">http://bulianwei.github.io</a> </p>\n<h1 id=\"专业技能\"><a href=\"#专业技能\" class=\"headerlink\" title=\"专业技能\"></a>专业技能</h1><ul>\n<li>掌握离线数仓（Hive|MapReduce|Spark）</li>\n<li>掌握实时数仓（Flink|Spark）</li>\n<li>掌握Kafka消息队列</li>\n<li>掌握Redis分布式缓存</li>\n<li>掌握Spark性能调优</li>\n<li>熟悉使用Docker容器技术</li>\n<li>熟悉JVM虚拟机，配置生产环境中JVM参数，可以快速定位CPU占用过高等问题</li>\n<li>熟悉JUC多线程高并发</li>\n<li>熟悉GC垃圾回收</li>\n<li>熟练使用Git代码管理工具</li>\n<li>熟悉数仓分层建设+权限控制</li>\n</ul>\n<h1 id=\"工作经历\"><a href=\"#工作经历\" class=\"headerlink\" title=\"工作经历\"></a>工作经历</h1><h2 id=\"轻图信息技术-北京-有限公司\"><a href=\"#轻图信息技术-北京-有限公司\" class=\"headerlink\" title=\"轻图信息技术(北京)有限公司\"></a>轻图信息技术(北京)有限公司</h2><p>大数据开发工程师 | 数据治理部门                                                                                （ 2017年12月 ~ 2019年6月）</p>\n<h3 id=\"位置融合集市\"><a href=\"#位置融合集市\" class=\"headerlink\" title=\"位置融合集市\"></a>位置融合集市</h3><p>位置融合是数仓的基础集市层，从Kafka推送过来的DDR，OIDD，DPI，WCDR数据直接存在HDFS上，位置融合就是对这些数据进行位置相关的处理操作，为数仓提供高质量的基础数据。</p>\n<h4 id=\"开发环境：\"><a href=\"#开发环境：\" class=\"headerlink\" title=\"开发环境：\"></a>开发环境：</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)     Java+MR</p>\n<h4 id=\"职能描述：\"><a href=\"#职能描述：\" class=\"headerlink\" title=\"职能描述：\"></a>职能描述：</h4><ul>\n<li>前期实现与客户对接，进行需求的前期澄清</li>\n<li>设计需求所需的算法模型，与客户沟通算法模型，对算法模型进行沟通后的优化</li>\n<li>核心算法模型的代码实现</li>\n<li>辅助开发组小伙伴针对其他模块代码进行开发</li>\n<li>和测试组小伙伴针将开发的项目测试上线</li>\n<li>维护项目，bug查找与修复</li>\n</ul>\n<h3 id=\"旅游集市\"><a href=\"#旅游集市\" class=\"headerlink\" title=\"旅游集市\"></a>旅游集市</h3><p>旅游集市是针对新疆大数据旅游项目做的专项研发，后期将算应用于全国。为出游规划和游客位置信息进行的详细展示。</p>\n<h4 id=\"开发环境\"><a href=\"#开发环境\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Scala(2.11)+SparkStreaming(2.1.0)+Kafka(2.1.0)  Scala+SparkStreaming+Kafka</p>\n<h4 id=\"职能描述\"><a href=\"#职能描述\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>前期与客户进行需求澄清</li>\n<li>针对沟通过的需求进行算法设计并与客户沟通算法模型</li>\n<li>协同开发小伙伴和测试小伙伴对项目进行开发测试上线</li>\n<li>维护和优化项目</li>\n</ul>\n<h3 id=\"智慧城市集市\"><a href=\"#智慧城市集市\" class=\"headerlink\" title=\"智慧城市集市\"></a>智慧城市集市</h3><p>智慧城市集市是给予数仓的基础数据，汇总统计单位用户在省、市、区县的停留记录。智慧城市是针对用户在范围上一次停留记录的粗粒的汇总。将一个省、市、区县内的用户数据做的合并，为专项集市提供过滤数据。</p>\n<h4 id=\"开发环境-1\"><a href=\"#开发环境-1\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)     Java+MR</p>\n<h4 id=\"职能描述-1\"><a href=\"#职能描述-1\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>与客户进行前期需求澄清</li>\n<li>针对前期澄清过的需求进行算法模型实现</li>\n<li>开发项目代码，测试代码</li>\n<li>维护项目</li>\n</ul>\n<h3 id=\"商业地产集市\"><a href=\"#商业地产集市\" class=\"headerlink\" title=\"商业地产集市\"></a>商业地产集市</h3><p>商业地产集市是基于运营商信息数据在数仓的一个子系统。数据来源于运营商的各类数据，基于这些数据进行加工汇总，从而生成一系列数据指标，根据这些数据指标来来解决传统商业模式下的招商难、选址难、运营难的“三难”困境。商业地产集市基于数仓，分为专题层、宽表层、DI层、指标层四层业务结构。</p>\n<h4 id=\"开发环境-2\"><a href=\"#开发环境-2\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Hive(1.1.0)    HQL+Shell</p>\n<h4 id=\"职能描述-2\"><a href=\"#职能描述-2\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>负责商场/商圈指标日/月表算法逻辑设计、脚本开发</li>\n<li>负责后期线上问题跟踪和Bug解决</li>\n</ul>\n<h3 id=\"掌合天下数仓\"><a href=\"#掌合天下数仓\" class=\"headerlink\" title=\"掌合天下数仓\"></a>掌合天下数仓</h3><p>掌合天下在品牌推广、市场竞争等多方面背景下，为充分体现加强信息挖掘、加强信息化管理、加强科学定量决策的管理理念，初步决定建立掌合天下数仓建设。数仓初步建设包括驾驶舱、订单、用户、商品、财务、风险在内的6个主要集市。</p>\n<h4 id=\"开发环境-3\"><a href=\"#开发环境-3\" class=\"headerlink\" title=\"开发环境\"></a>开发环境</h4><p>JDK(1.7)+Hadoop(2.6.0)+CDH(5.8.3)+Hive(1.1.0)     HQL+Shell+Azkaban+Tableau</p>\n<h4 id=\"职能描述-3\"><a href=\"#职能描述-3\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>负责订单、用户集市的设计和开发</li>\n<li>负责Azkaban维护</li>\n<li>负责后期集市维护，Bug解决</li>\n</ul>\n<h2 id=\"北京中科弘睿科技有限公司\"><a href=\"#北京中科弘睿科技有限公司\" class=\"headerlink\" title=\"北京中科弘睿科技有限公司\"></a>北京中科弘睿科技有限公司</h2><p>大数据开发工程师 | 大数据部门                                                                                    （ 2016年3月 ~ 2017年10月 ）</p>\n<h3 id=\"电子商城离线分析系统\"><a href=\"#电子商城离线分析系统\" class=\"headerlink\" title=\"电子商城离线分析系统\"></a>电子商城离线分析系统</h3><p>用户每天在电子商城平台上的点击、浏览、下单支付行为都会产生大量的浏览日志， 这些日志数据被汇总后，经过后期的过滤、分析、挖掘与学习后，为电子商城的各种推 荐、搜索系统甚至对公司以后的战略发展 供数据支持。<br>电子商城离线分析系统总共分为用户数据基本分析模块、浏览器分析模块、地域分 析模块、外链分析模块、用户浏览深度分析模块、事件分析模块和订单分析模块等七大 主要模块。本系统利用这七大模块将有效数据以数据或图表的方式展现出来，供各业务部门参考。</p>\n<h4 id=\"职能描述-4\"><a href=\"#职能描述-4\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>分析商城网页得出PV、UV、二跳率等关键性指标</li>\n<li>开发Spark代码对数据进行初级清洗、脱敏操作，并将处理之后的数据放入MySQL数据库</li>\n<li>开发Spark SQL代码针对用户基本数据分析模块编写单维度分析</li>\n</ul>\n<h3 id=\"驾车习性系统\"><a href=\"#驾车习性系统\" class=\"headerlink\" title=\"驾车习性系统\"></a>驾车习性系统</h3><p>驾车习性统分系统，是一个统计、分析用户驾车习惯的平台。通过对用户驾驶习性 的统计、分析，可得出用户的使用密度和一些经常出现的地方(家或者公司)。公司通 过分析出来的资料与相应的保险公司进行合作，建立大规模的用户数据库。此外，平台 还可以统计出某品牌车的销售Top区域， 高了该品牌对市场的掌握程度，从而决策下 一步的宣称、推广计划。</p>\n<h4 id=\"职能描述-5\"><a href=\"#职能描述-5\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>使用Flume将日志数据文件传输、储存在HDFS中</li>\n<li>开发MapReduce代码对HDFS中数据进行清洗、脱敏操作</li>\n</ul>\n<h2 id=\"联想（北京）有限公司\"><a href=\"#联想（北京）有限公司\" class=\"headerlink\" title=\"联想（北京）有限公司\"></a>联想（北京）有限公司</h2><p>移动互联产品开发助理 | E&amp;T                                                                                        （ 2015年3月 ~ 2016年1月 ）</p>\n<h4 id=\"职能描述-6\"><a href=\"#职能描述-6\" class=\"headerlink\" title=\"职能描述\"></a>职能描述</h4><ul>\n<li>开发比赛软件项目代码（Android），并为比赛提供软件技术支持</li>\n<li>撰写Android开发教材数据存储模块</li>\n</ul>\n<h1 id=\"教育背景\"><a href=\"#教育背景\" class=\"headerlink\" title=\"教育背景\"></a>教育背景</h1><p>2012.9 ～ 2015.7        山东劳动职业技术学院    大专    软件工程    </p>\n<h1 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h1><ul>\n<li>荣誉/奖项：国家励志奖，省政府励志奖，学院奖，移动互联网全国软件开发二等奖</li>\n</ul>\n<h1 id=\"致谢\"><a href=\"#致谢\" class=\"headerlink\" title=\"致谢\"></a>致谢</h1><p>感谢您花时间阅读我的简历，期待能有机会和您共事。</p>\n"},{"title":"categories","date":"2018-10-01T16:47:23.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-10-02 00:47:23\ntype: \"categories\"\nlayout: \"categories\"\n---\n","updated":"2020-03-19T04:51:26.000Z","path":"categories/index.html","comments":1,"_id":"ck7ycejg70006jr649jyz06pi","content":"","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":""},{"title":"friends","date":"2018-12-13T15:43:47.000Z","type":"friends","layout":"friends","_content":"","source":"friends/index.md","raw":"---\ntitle: friends\ndate: 2018-12-13 23:43:47\ntype: \"friends\"\nlayout: \"friends\"\n---\n","updated":"2020-03-19T04:51:26.000Z","path":"friends/index.html","comments":1,"_id":"ck7ycejg80007jr641y7y0v46","content":"","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":""},{"_content":"# Docker\n\n## 命令\n\n### 镜像\n+ docker images\t//列出本地镜像\n  + -a\t//列出本地所有镜像\n  + -q\t//只显示镜像ID\n  + --digests\t//显示镜像的摘要信息\n  + --no-trunc\t//显示完整的镜像信息\n\n+ docker search\t//查找镜像\n\t+ --no-trunc\t//显示完整的镜像描述\n\t+ -s\t//列出收藏数不小于制定值的镜像\n\t+ --automated\t//只列出automated build 类型的镜像\n\n+ docker pull\t//下载镜像\n\n+ docker rmi\t//删除镜像\n\t+ -f\t//强制删除\n\n+ docker commit\t//提交容器副本使之成为一个新镜像\n\t+ -m\t//提交信息\n\t+ -a\t//作者\n\n### 容器\n+ docker run //使用镜像创建并启动容器\n\t+ -i\t//以交互模式运行容器\n\t+ -t\t//为容器重新分配一个熟人终端，通常与-i同用\n\t+ -d\t//后台运行容器\n\t+ --name\t//为容器指定名字\n\t+ -p\t//指定端口映射\n\t+ -P（大写）\t//随机端口映射\n\t+ -v\t//添加数据卷  ／宿主机绝对路径:/容器内目录\n+ docker ps\t//列出现在正在运行的容器\n\t+ -a\t//列出所有的容器\n\t+ -l\t//显示最近创建的容器\n\t+ -n\t//显示最近n个创建的容器\n\t+ -q\t//只显示容器编号\n\t+ --no-trunc\t//不截断输出\n+ docker exec\t//在运行的容器中执行命令\n+ docker attach\t//进入运行着的容器\n+ docker start\t//启动容器\n+ docker stop\t//关闭容器\n+ docker restart\t//重启容器\n+ docker top\t//查看容器内运行的进程\n+ docker inspect\t//查看容器内部细节\n+ docker cp\t//从容器内拷贝内容到主机上\n+ docker kill\t//杀掉容器\n+ docker rm\t//删除容器\n+ docker logs\t//查看日志\n\t+ -t\t//加入时间戳\n\t+ -f\t//跟进最新日志打印\n\t+ --tail\t//显示最新多少条\n\n\n## 容器数据卷\n容器的持久化，容器间继承+数据共享\n### 数据卷\ndocker run -v /宿主机绝对路径:/容器内目录 镜像名\n### 数据卷容器\ndocker run --volumes-from 父容器\n\n## DockerFile\n是一个由一系列命令和参数组成的构建Docker镜像的文件\n### 关键字\n+ FROM\t//父镜像，依赖镜像\n+ MAINTAINER\t//镜像维护姓名和邮箱\n+ RUN\t//容器构建时需要执行的命令\n+ EXPOSE\t//当前容器对外暴露出的端口\n+ WORKDIR\t//指定容器创建后，登陆进来的工作目录\n+ ENV\t//用来在构建镜像过程中设置的环境变量\n+ ADD\t//将宿主目录下的文件加载进容器里（自动处理url和解压）\n+ COPY\t//将宿主目录下的文件复制到容器相关目录\n+ VOLUME\t//容器数据卷，用于数据共享和持久化\n+ CMD\t//指定容器启动时要运行的命令（多个命令时只有最后一个生效，运行时指定的命令耶会覆盖cmd 命令）\n+ ENTRYPOINT\t//指定容器启动时要运行的命令，运行时指定的命令会被追加）\n+ ONBUILD\t//当构建一个被继承的DockerFile时的运行命令，父镜像的onbuild 在子镜像被build时触发\n\n\n\n\n\n\n\n\n\n\n","source":"notes/Docker.md","raw":"# Docker\n\n## 命令\n\n### 镜像\n+ docker images\t//列出本地镜像\n  + -a\t//列出本地所有镜像\n  + -q\t//只显示镜像ID\n  + --digests\t//显示镜像的摘要信息\n  + --no-trunc\t//显示完整的镜像信息\n\n+ docker search\t//查找镜像\n\t+ --no-trunc\t//显示完整的镜像描述\n\t+ -s\t//列出收藏数不小于制定值的镜像\n\t+ --automated\t//只列出automated build 类型的镜像\n\n+ docker pull\t//下载镜像\n\n+ docker rmi\t//删除镜像\n\t+ -f\t//强制删除\n\n+ docker commit\t//提交容器副本使之成为一个新镜像\n\t+ -m\t//提交信息\n\t+ -a\t//作者\n\n### 容器\n+ docker run //使用镜像创建并启动容器\n\t+ -i\t//以交互模式运行容器\n\t+ -t\t//为容器重新分配一个熟人终端，通常与-i同用\n\t+ -d\t//后台运行容器\n\t+ --name\t//为容器指定名字\n\t+ -p\t//指定端口映射\n\t+ -P（大写）\t//随机端口映射\n\t+ -v\t//添加数据卷  ／宿主机绝对路径:/容器内目录\n+ docker ps\t//列出现在正在运行的容器\n\t+ -a\t//列出所有的容器\n\t+ -l\t//显示最近创建的容器\n\t+ -n\t//显示最近n个创建的容器\n\t+ -q\t//只显示容器编号\n\t+ --no-trunc\t//不截断输出\n+ docker exec\t//在运行的容器中执行命令\n+ docker attach\t//进入运行着的容器\n+ docker start\t//启动容器\n+ docker stop\t//关闭容器\n+ docker restart\t//重启容器\n+ docker top\t//查看容器内运行的进程\n+ docker inspect\t//查看容器内部细节\n+ docker cp\t//从容器内拷贝内容到主机上\n+ docker kill\t//杀掉容器\n+ docker rm\t//删除容器\n+ docker logs\t//查看日志\n\t+ -t\t//加入时间戳\n\t+ -f\t//跟进最新日志打印\n\t+ --tail\t//显示最新多少条\n\n\n## 容器数据卷\n容器的持久化，容器间继承+数据共享\n### 数据卷\ndocker run -v /宿主机绝对路径:/容器内目录 镜像名\n### 数据卷容器\ndocker run --volumes-from 父容器\n\n## DockerFile\n是一个由一系列命令和参数组成的构建Docker镜像的文件\n### 关键字\n+ FROM\t//父镜像，依赖镜像\n+ MAINTAINER\t//镜像维护姓名和邮箱\n+ RUN\t//容器构建时需要执行的命令\n+ EXPOSE\t//当前容器对外暴露出的端口\n+ WORKDIR\t//指定容器创建后，登陆进来的工作目录\n+ ENV\t//用来在构建镜像过程中设置的环境变量\n+ ADD\t//将宿主目录下的文件加载进容器里（自动处理url和解压）\n+ COPY\t//将宿主目录下的文件复制到容器相关目录\n+ VOLUME\t//容器数据卷，用于数据共享和持久化\n+ CMD\t//指定容器启动时要运行的命令（多个命令时只有最后一个生效，运行时指定的命令耶会覆盖cmd 命令）\n+ ENTRYPOINT\t//指定容器启动时要运行的命令，运行时指定的命令会被追加）\n+ ONBUILD\t//当构建一个被继承的DockerFile时的运行命令，父镜像的onbuild 在子镜像被build时触发\n\n\n\n\n\n\n\n\n\n\n","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T11:52:16.000Z","path":"notes/Docker.html","title":"","comments":1,"layout":"page","_id":"ck7ycejg90008jr64dv9bv1hf","content":"<h1 id=\"Docker\"><a href=\"#Docker\" class=\"headerlink\" title=\"Docker\"></a>Docker</h1><h2 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h2><h3 id=\"镜像\"><a href=\"#镜像\" class=\"headerlink\" title=\"镜像\"></a>镜像</h3><ul>\n<li><p>docker images    //列出本地镜像</p>\n<ul>\n<li>-a    //列出本地所有镜像</li>\n<li>-q    //只显示镜像ID</li>\n<li>–digests    //显示镜像的摘要信息</li>\n<li>–no-trunc    //显示完整的镜像信息</li>\n</ul>\n</li>\n<li><p>docker search    //查找镜像</p>\n<ul>\n<li>–no-trunc    //显示完整的镜像描述</li>\n<li>-s    //列出收藏数不小于制定值的镜像</li>\n<li>–automated    //只列出automated build 类型的镜像</li>\n</ul>\n</li>\n<li><p>docker pull    //下载镜像</p>\n</li>\n<li><p>docker rmi    //删除镜像</p>\n<ul>\n<li>-f    //强制删除</li>\n</ul>\n</li>\n<li><p>docker commit    //提交容器副本使之成为一个新镜像</p>\n<ul>\n<li>-m    //提交信息</li>\n<li>-a    //作者</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"容器\"><a href=\"#容器\" class=\"headerlink\" title=\"容器\"></a>容器</h3><ul>\n<li>docker run //使用镜像创建并启动容器<ul>\n<li>-i    //以交互模式运行容器</li>\n<li>-t    //为容器重新分配一个熟人终端，通常与-i同用</li>\n<li>-d    //后台运行容器</li>\n<li>–name    //为容器指定名字</li>\n<li>-p    //指定端口映射</li>\n<li>-P（大写）    //随机端口映射</li>\n<li>-v    //添加数据卷  ／宿主机绝对路径:/容器内目录</li>\n</ul>\n</li>\n<li>docker ps    //列出现在正在运行的容器<ul>\n<li>-a    //列出所有的容器</li>\n<li>-l    //显示最近创建的容器</li>\n<li>-n    //显示最近n个创建的容器</li>\n<li>-q    //只显示容器编号</li>\n<li>–no-trunc    //不截断输出</li>\n</ul>\n</li>\n<li>docker exec    //在运行的容器中执行命令</li>\n<li>docker attach    //进入运行着的容器</li>\n<li>docker start    //启动容器</li>\n<li>docker stop    //关闭容器</li>\n<li>docker restart    //重启容器</li>\n<li>docker top    //查看容器内运行的进程</li>\n<li>docker inspect    //查看容器内部细节</li>\n<li>docker cp    //从容器内拷贝内容到主机上</li>\n<li>docker kill    //杀掉容器</li>\n<li>docker rm    //删除容器</li>\n<li>docker logs    //查看日志<ul>\n<li>-t    //加入时间戳</li>\n<li>-f    //跟进最新日志打印</li>\n<li>–tail    //显示最新多少条</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"容器数据卷\"><a href=\"#容器数据卷\" class=\"headerlink\" title=\"容器数据卷\"></a>容器数据卷</h2><p>容器的持久化，容器间继承+数据共享</p>\n<h3 id=\"数据卷\"><a href=\"#数据卷\" class=\"headerlink\" title=\"数据卷\"></a>数据卷</h3><p>docker run -v /宿主机绝对路径:/容器内目录 镜像名</p>\n<h3 id=\"数据卷容器\"><a href=\"#数据卷容器\" class=\"headerlink\" title=\"数据卷容器\"></a>数据卷容器</h3><p>docker run –volumes-from 父容器</p>\n<h2 id=\"DockerFile\"><a href=\"#DockerFile\" class=\"headerlink\" title=\"DockerFile\"></a>DockerFile</h2><p>是一个由一系列命令和参数组成的构建Docker镜像的文件</p>\n<h3 id=\"关键字\"><a href=\"#关键字\" class=\"headerlink\" title=\"关键字\"></a>关键字</h3><ul>\n<li>FROM    //父镜像，依赖镜像</li>\n<li>MAINTAINER    //镜像维护姓名和邮箱</li>\n<li>RUN    //容器构建时需要执行的命令</li>\n<li>EXPOSE    //当前容器对外暴露出的端口</li>\n<li>WORKDIR    //指定容器创建后，登陆进来的工作目录</li>\n<li>ENV    //用来在构建镜像过程中设置的环境变量</li>\n<li>ADD    //将宿主目录下的文件加载进容器里（自动处理url和解压）</li>\n<li>COPY    //将宿主目录下的文件复制到容器相关目录</li>\n<li>VOLUME    //容器数据卷，用于数据共享和持久化</li>\n<li>CMD    //指定容器启动时要运行的命令（多个命令时只有最后一个生效，运行时指定的命令耶会覆盖cmd 命令）</li>\n<li>ENTRYPOINT    //指定容器启动时要运行的命令，运行时指定的命令会被追加）</li>\n<li>ONBUILD    //当构建一个被继承的DockerFile时的运行命令，父镜像的onbuild 在子镜像被build时触发</li>\n</ul>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"Docker\"><a href=\"#Docker\" class=\"headerlink\" title=\"Docker\"></a>Docker</h1><h2 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h2><h3 id=\"镜像\"><a href=\"#镜像\" class=\"headerlink\" title=\"镜像\"></a>镜像</h3><ul>\n<li><p>docker images    //列出本地镜像</p>\n<ul>\n<li>-a    //列出本地所有镜像</li>\n<li>-q    //只显示镜像ID</li>\n<li>–digests    //显示镜像的摘要信息</li>\n<li>–no-trunc    //显示完整的镜像信息</li>\n</ul>\n</li>\n<li><p>docker search    //查找镜像</p>\n<ul>\n<li>–no-trunc    //显示完整的镜像描述</li>\n<li>-s    //列出收藏数不小于制定值的镜像</li>\n<li>–automated    //只列出automated build 类型的镜像</li>\n</ul>\n</li>\n<li><p>docker pull    //下载镜像</p>\n</li>\n<li><p>docker rmi    //删除镜像</p>\n<ul>\n<li>-f    //强制删除</li>\n</ul>\n</li>\n<li><p>docker commit    //提交容器副本使之成为一个新镜像</p>\n<ul>\n<li>-m    //提交信息</li>\n<li>-a    //作者</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"容器\"><a href=\"#容器\" class=\"headerlink\" title=\"容器\"></a>容器</h3><ul>\n<li>docker run //使用镜像创建并启动容器<ul>\n<li>-i    //以交互模式运行容器</li>\n<li>-t    //为容器重新分配一个熟人终端，通常与-i同用</li>\n<li>-d    //后台运行容器</li>\n<li>–name    //为容器指定名字</li>\n<li>-p    //指定端口映射</li>\n<li>-P（大写）    //随机端口映射</li>\n<li>-v    //添加数据卷  ／宿主机绝对路径:/容器内目录</li>\n</ul>\n</li>\n<li>docker ps    //列出现在正在运行的容器<ul>\n<li>-a    //列出所有的容器</li>\n<li>-l    //显示最近创建的容器</li>\n<li>-n    //显示最近n个创建的容器</li>\n<li>-q    //只显示容器编号</li>\n<li>–no-trunc    //不截断输出</li>\n</ul>\n</li>\n<li>docker exec    //在运行的容器中执行命令</li>\n<li>docker attach    //进入运行着的容器</li>\n<li>docker start    //启动容器</li>\n<li>docker stop    //关闭容器</li>\n<li>docker restart    //重启容器</li>\n<li>docker top    //查看容器内运行的进程</li>\n<li>docker inspect    //查看容器内部细节</li>\n<li>docker cp    //从容器内拷贝内容到主机上</li>\n<li>docker kill    //杀掉容器</li>\n<li>docker rm    //删除容器</li>\n<li>docker logs    //查看日志<ul>\n<li>-t    //加入时间戳</li>\n<li>-f    //跟进最新日志打印</li>\n<li>–tail    //显示最新多少条</li>\n</ul>\n</li>\n</ul>\n<h2 id=\"容器数据卷\"><a href=\"#容器数据卷\" class=\"headerlink\" title=\"容器数据卷\"></a>容器数据卷</h2><p>容器的持久化，容器间继承+数据共享</p>\n<h3 id=\"数据卷\"><a href=\"#数据卷\" class=\"headerlink\" title=\"数据卷\"></a>数据卷</h3><p>docker run -v /宿主机绝对路径:/容器内目录 镜像名</p>\n<h3 id=\"数据卷容器\"><a href=\"#数据卷容器\" class=\"headerlink\" title=\"数据卷容器\"></a>数据卷容器</h3><p>docker run –volumes-from 父容器</p>\n<h2 id=\"DockerFile\"><a href=\"#DockerFile\" class=\"headerlink\" title=\"DockerFile\"></a>DockerFile</h2><p>是一个由一系列命令和参数组成的构建Docker镜像的文件</p>\n<h3 id=\"关键字\"><a href=\"#关键字\" class=\"headerlink\" title=\"关键字\"></a>关键字</h3><ul>\n<li>FROM    //父镜像，依赖镜像</li>\n<li>MAINTAINER    //镜像维护姓名和邮箱</li>\n<li>RUN    //容器构建时需要执行的命令</li>\n<li>EXPOSE    //当前容器对外暴露出的端口</li>\n<li>WORKDIR    //指定容器创建后，登陆进来的工作目录</li>\n<li>ENV    //用来在构建镜像过程中设置的环境变量</li>\n<li>ADD    //将宿主目录下的文件加载进容器里（自动处理url和解压）</li>\n<li>COPY    //将宿主目录下的文件复制到容器相关目录</li>\n<li>VOLUME    //容器数据卷，用于数据共享和持久化</li>\n<li>CMD    //指定容器启动时要运行的命令（多个命令时只有最后一个生效，运行时指定的命令耶会覆盖cmd 命令）</li>\n<li>ENTRYPOINT    //指定容器启动时要运行的命令，运行时指定的命令会被追加）</li>\n<li>ONBUILD    //当构建一个被继承的DockerFile时的运行命令，父镜像的onbuild 在子镜像被build时触发</li>\n</ul>\n"},{"_content":"\n# Bigdata\n\n## 1. Hadoop\n~~~\n1.hdfs集群：负责文件读写\n2.yarn集群：负责为mapredece程序分配运算硬件资源\nHadoop配置：\n1.core-site.xml：fs.DefaultFS\thadoop.tmp.dir\n2.hadoop.env.sh:  JAVA_HOME\n3.mapred-site.xml:  mapreduce.framework.name\n4.hdfs-site.xml:  dfs.replication\n5.yarn-site.xml:   yarn.resourcemanager.hostname  yarn.nodemanager.aux-services    \nslaves\u0001\u001a\u001a：自动化脚本使用的配置文件\n修改Linux启动等待时间  vi /etc/fstab\nmkdir -p  /aa/bb/cc      可以直接建立aa下的bb下的cc  创建层级文件\n挂载光驱：mount -t iso9660 -o ro\t /dev/cdrom  /mnt/cdrom \n设置光驱开机挂载：vi /etc/fstab\t增加 /dev/cdrom\t/mnt/cdrom\tiso9660\tdefaults\t0 0\nrename\t文件重命名\t\n与mv的区别\nmv 支持单个文件重命名\nrename 支持多个文件重命名\ncp /aa/*  /bb\t复制aa下的所有文件到bb下面\nhadoop  fs -put  filename / \t\t上传文件\nhadoop  fs  -get   filename\t\t下载文件\nhadoop 副本数由客户端决定（conf.set 》自定义配置文件》jar包中的配置）\nnamenode和datanode两个的工作目录不能在同一个文件夹下，否则会启动不起来。\neclipse 本地运行代码时遇到调用远程路径权限不够时，在eclipse客户端运行配置时添加VM arguments参数使用hadoop用户运行，参数为“\t-DHADOOP_USER_NAME=hadoop\t”\nmr程序：\nconfiguration参数设置：\nconf.set（”mapreduce.framework.name\",\"local\"）#是否为本地运行模式，默认为local本地模式\nconf.set(\"fs.defaultFS\",\"file:///\")\t#输入输出数据的文件路径，默认为file\nconf.set(\"fs.defaultFS\",\"hdfs://hadoop01:9000/\") #输入输出文件路径\n本地运行模式文件可以用file也可以用hdfs\n设置在yarn上运行程序（要想设置在集群上运行程序 一下三个参数必须设置）\nconf.set（“mapreduce.framework.name”,\"yarn\"）#设置运行模式为在集群上运行\nconf.set(\"yarn.resourcename.hostname\",\"hadoop01\")\t#设置yarn的管理者resourcename在那一台机器上\nconf.set(\"fs.defaultFS\",\"hdfs://hadoop01:9000/\") #程序在设置在yarn上运行时必须设置文件存储为hdfs\n程序里面systemout的日子存在于hadoop里面的logs的userlogs里面对应job_id下的container里的syslog里面\nnamenode在刚启动时只包含block的数量和blockid不知道块所在datanode，要等待datanode向他汇报后才会在namenode元数据中补全文件块的位置信息，只有在namenode找到999.8%的块的位置信息才会退出安全模式正常对外提供服务。\n~~~\n","source":"notes/Bigdata.md","raw":"\n# Bigdata\n\n## 1. Hadoop\n~~~\n1.hdfs集群：负责文件读写\n2.yarn集群：负责为mapredece程序分配运算硬件资源\nHadoop配置：\n1.core-site.xml：fs.DefaultFS\thadoop.tmp.dir\n2.hadoop.env.sh:  JAVA_HOME\n3.mapred-site.xml:  mapreduce.framework.name\n4.hdfs-site.xml:  dfs.replication\n5.yarn-site.xml:   yarn.resourcemanager.hostname  yarn.nodemanager.aux-services    \nslaves\u0001\u001a\u001a：自动化脚本使用的配置文件\n修改Linux启动等待时间  vi /etc/fstab\nmkdir -p  /aa/bb/cc      可以直接建立aa下的bb下的cc  创建层级文件\n挂载光驱：mount -t iso9660 -o ro\t /dev/cdrom  /mnt/cdrom \n设置光驱开机挂载：vi /etc/fstab\t增加 /dev/cdrom\t/mnt/cdrom\tiso9660\tdefaults\t0 0\nrename\t文件重命名\t\n与mv的区别\nmv 支持单个文件重命名\nrename 支持多个文件重命名\ncp /aa/*  /bb\t复制aa下的所有文件到bb下面\nhadoop  fs -put  filename / \t\t上传文件\nhadoop  fs  -get   filename\t\t下载文件\nhadoop 副本数由客户端决定（conf.set 》自定义配置文件》jar包中的配置）\nnamenode和datanode两个的工作目录不能在同一个文件夹下，否则会启动不起来。\neclipse 本地运行代码时遇到调用远程路径权限不够时，在eclipse客户端运行配置时添加VM arguments参数使用hadoop用户运行，参数为“\t-DHADOOP_USER_NAME=hadoop\t”\nmr程序：\nconfiguration参数设置：\nconf.set（”mapreduce.framework.name\",\"local\"）#是否为本地运行模式，默认为local本地模式\nconf.set(\"fs.defaultFS\",\"file:///\")\t#输入输出数据的文件路径，默认为file\nconf.set(\"fs.defaultFS\",\"hdfs://hadoop01:9000/\") #输入输出文件路径\n本地运行模式文件可以用file也可以用hdfs\n设置在yarn上运行程序（要想设置在集群上运行程序 一下三个参数必须设置）\nconf.set（“mapreduce.framework.name”,\"yarn\"）#设置运行模式为在集群上运行\nconf.set(\"yarn.resourcename.hostname\",\"hadoop01\")\t#设置yarn的管理者resourcename在那一台机器上\nconf.set(\"fs.defaultFS\",\"hdfs://hadoop01:9000/\") #程序在设置在yarn上运行时必须设置文件存储为hdfs\n程序里面systemout的日子存在于hadoop里面的logs的userlogs里面对应job_id下的container里的syslog里面\nnamenode在刚启动时只包含block的数量和blockid不知道块所在datanode，要等待datanode向他汇报后才会在namenode元数据中补全文件块的位置信息，只有在namenode找到999.8%的块的位置信息才会退出安全模式正常对外提供服务。\n~~~\n","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T11:52:16.000Z","path":"notes/Bigdata.html","title":"","comments":1,"layout":"page","_id":"ck7ycejgb000ajr64dzmcs55z","content":"<h1 id=\"Bigdata\"><a href=\"#Bigdata\" class=\"headerlink\" title=\"Bigdata\"></a>Bigdata</h1><h2 id=\"1-Hadoop\"><a href=\"#1-Hadoop\" class=\"headerlink\" title=\"1. Hadoop\"></a>1. Hadoop</h2><pre><code>1.hdfs集群：负责文件读写\n2.yarn集群：负责为mapredece程序分配运算硬件资源\nHadoop配置：\n1.core-site.xml：fs.DefaultFS    hadoop.tmp.dir\n2.hadoop.env.sh:  JAVA_HOME\n3.mapred-site.xml:  mapreduce.framework.name\n4.hdfs-site.xml:  dfs.replication\n5.yarn-site.xml:   yarn.resourcemanager.hostname  yarn.nodemanager.aux-services    \nslaves\u0001\u001a\u001a：自动化脚本使用的配置文件\n修改Linux启动等待时间  vi /etc/fstab\nmkdir -p  /aa/bb/cc      可以直接建立aa下的bb下的cc  创建层级文件\n挂载光驱：mount -t iso9660 -o ro     /dev/cdrom  /mnt/cdrom \n设置光驱开机挂载：vi /etc/fstab    增加 /dev/cdrom    /mnt/cdrom    iso9660    defaults    0 0\nrename    文件重命名    \n与mv的区别\nmv 支持单个文件重命名\nrename 支持多个文件重命名\ncp /aa/*  /bb    复制aa下的所有文件到bb下面\nhadoop  fs -put  filename /         上传文件\nhadoop  fs  -get   filename        下载文件\nhadoop 副本数由客户端决定（conf.set 》自定义配置文件》jar包中的配置）\nnamenode和datanode两个的工作目录不能在同一个文件夹下，否则会启动不起来。\neclipse 本地运行代码时遇到调用远程路径权限不够时，在eclipse客户端运行配置时添加VM arguments参数使用hadoop用户运行，参数为“    -DHADOOP_USER_NAME=hadoop    ”\nmr程序：\nconfiguration参数设置：\nconf.set（”mapreduce.framework.name&quot;,&quot;local&quot;）#是否为本地运行模式，默认为local本地模式\nconf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;)    #输入输出数据的文件路径，默认为file\nconf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://hadoop01:9000/&quot;) #输入输出文件路径\n本地运行模式文件可以用file也可以用hdfs\n设置在yarn上运行程序（要想设置在集群上运行程序 一下三个参数必须设置）\nconf.set（“mapreduce.framework.name”,&quot;yarn&quot;）#设置运行模式为在集群上运行\nconf.set(&quot;yarn.resourcename.hostname&quot;,&quot;hadoop01&quot;)    #设置yarn的管理者resourcename在那一台机器上\nconf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://hadoop01:9000/&quot;) #程序在设置在yarn上运行时必须设置文件存储为hdfs\n程序里面systemout的日子存在于hadoop里面的logs的userlogs里面对应job_id下的container里的syslog里面\nnamenode在刚启动时只包含block的数量和blockid不知道块所在datanode，要等待datanode向他汇报后才会在namenode元数据中补全文件块的位置信息，只有在namenode找到999.8%的块的位置信息才会退出安全模式正常对外提供服务。</code></pre>","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"Bigdata\"><a href=\"#Bigdata\" class=\"headerlink\" title=\"Bigdata\"></a>Bigdata</h1><h2 id=\"1-Hadoop\"><a href=\"#1-Hadoop\" class=\"headerlink\" title=\"1. Hadoop\"></a>1. Hadoop</h2><pre><code>1.hdfs集群：负责文件读写\n2.yarn集群：负责为mapredece程序分配运算硬件资源\nHadoop配置：\n1.core-site.xml：fs.DefaultFS    hadoop.tmp.dir\n2.hadoop.env.sh:  JAVA_HOME\n3.mapred-site.xml:  mapreduce.framework.name\n4.hdfs-site.xml:  dfs.replication\n5.yarn-site.xml:   yarn.resourcemanager.hostname  yarn.nodemanager.aux-services    \nslaves\u0001\u001a\u001a：自动化脚本使用的配置文件\n修改Linux启动等待时间  vi /etc/fstab\nmkdir -p  /aa/bb/cc      可以直接建立aa下的bb下的cc  创建层级文件\n挂载光驱：mount -t iso9660 -o ro     /dev/cdrom  /mnt/cdrom \n设置光驱开机挂载：vi /etc/fstab    增加 /dev/cdrom    /mnt/cdrom    iso9660    defaults    0 0\nrename    文件重命名    \n与mv的区别\nmv 支持单个文件重命名\nrename 支持多个文件重命名\ncp /aa/*  /bb    复制aa下的所有文件到bb下面\nhadoop  fs -put  filename /         上传文件\nhadoop  fs  -get   filename        下载文件\nhadoop 副本数由客户端决定（conf.set 》自定义配置文件》jar包中的配置）\nnamenode和datanode两个的工作目录不能在同一个文件夹下，否则会启动不起来。\neclipse 本地运行代码时遇到调用远程路径权限不够时，在eclipse客户端运行配置时添加VM arguments参数使用hadoop用户运行，参数为“    -DHADOOP_USER_NAME=hadoop    ”\nmr程序：\nconfiguration参数设置：\nconf.set（”mapreduce.framework.name&quot;,&quot;local&quot;）#是否为本地运行模式，默认为local本地模式\nconf.set(&quot;fs.defaultFS&quot;,&quot;file:///&quot;)    #输入输出数据的文件路径，默认为file\nconf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://hadoop01:9000/&quot;) #输入输出文件路径\n本地运行模式文件可以用file也可以用hdfs\n设置在yarn上运行程序（要想设置在集群上运行程序 一下三个参数必须设置）\nconf.set（“mapreduce.framework.name”,&quot;yarn&quot;）#设置运行模式为在集群上运行\nconf.set(&quot;yarn.resourcename.hostname&quot;,&quot;hadoop01&quot;)    #设置yarn的管理者resourcename在那一台机器上\nconf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://hadoop01:9000/&quot;) #程序在设置在yarn上运行时必须设置文件存储为hdfs\n程序里面systemout的日子存在于hadoop里面的logs的userlogs里面对应job_id下的container里的syslog里面\nnamenode在刚启动时只包含block的数量和blockid不知道块所在datanode，要等待datanode向他汇报后才会在namenode元数据中补全文件块的位置信息，只有在namenode找到999.8%的块的位置信息才会退出安全模式正常对外提供服务。</code></pre>"},{"_content":"# Flink-Trouble\n\n## 1. 处理时间窗口数据时未添加窗口或者设置处理的时间类型\n### 错误现象\n```\nCaused by: java.lang.RuntimeException: Record has Long.MIN_VALUE timestamp (= no timestamp marker). Is the time characteristic set to 'ProcessingTime', or did you forget to call 'DataStream.assignTimestampsAndWatermarks(...)'?\n```\n### 解决方案\n+ assignTimestampsAndWatermarks\n+ env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\n## 2. 程序重写(删除算子)导致使用savepoint数据快照无法恢复启动\n### 错误现象\n```\nFailed to rollback to checkpoint/savepoint file:/Users/bulianwei/workspace/project/idea/flink_demo/data/test/savepoint-18c72f-803ce50127dc. Cannot map checkpoint/savepoint state for operator c27dcf7b54ef6bfd6cff02ca8870b681 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.\n```\n### 解决方案\n启动时使用 --allowNonRestoredState 或者 -n 跳过删除算子恢复\nflink run -s savepointdir -n -c mainclasspath jarpath\n\n## 3. 使用flink stop 停止job时出现无法停止现象\n### 错误现象\n```\n[org.apache.flink.runtime.rest.handler.RestHandlerException: Config key [state.savepoints.dir] is not set. Property [targetDirectory] must be provided.\n```\n### 解决方案\n使用flink stop jobid停止job时会保存数据快照，如果未设置savepointpath会导致无法保存快照，会导致无法停止job\nflink stop jobid -p savepointpath\n\n## 4.\n### 错误现象\n```\n\n```\n### 解决方案","source":"notes/Flink-Trouble.md","raw":"# Flink-Trouble\n\n## 1. 处理时间窗口数据时未添加窗口或者设置处理的时间类型\n### 错误现象\n```\nCaused by: java.lang.RuntimeException: Record has Long.MIN_VALUE timestamp (= no timestamp marker). Is the time characteristic set to 'ProcessingTime', or did you forget to call 'DataStream.assignTimestampsAndWatermarks(...)'?\n```\n### 解决方案\n+ assignTimestampsAndWatermarks\n+ env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\n## 2. 程序重写(删除算子)导致使用savepoint数据快照无法恢复启动\n### 错误现象\n```\nFailed to rollback to checkpoint/savepoint file:/Users/bulianwei/workspace/project/idea/flink_demo/data/test/savepoint-18c72f-803ce50127dc. Cannot map checkpoint/savepoint state for operator c27dcf7b54ef6bfd6cff02ca8870b681 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.\n```\n### 解决方案\n启动时使用 --allowNonRestoredState 或者 -n 跳过删除算子恢复\nflink run -s savepointdir -n -c mainclasspath jarpath\n\n## 3. 使用flink stop 停止job时出现无法停止现象\n### 错误现象\n```\n[org.apache.flink.runtime.rest.handler.RestHandlerException: Config key [state.savepoints.dir] is not set. Property [targetDirectory] must be provided.\n```\n### 解决方案\n使用flink stop jobid停止job时会保存数据快照，如果未设置savepointpath会导致无法保存快照，会导致无法停止job\nflink stop jobid -p savepointpath\n\n## 4.\n### 错误现象\n```\n\n```\n### 解决方案","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T11:52:16.000Z","path":"notes/Flink-Trouble.html","title":"","comments":1,"layout":"page","_id":"ck7ycejgd000cjr64md3hvmh0","content":"<h1 id=\"Flink-Trouble\"><a href=\"#Flink-Trouble\" class=\"headerlink\" title=\"Flink-Trouble\"></a>Flink-Trouble</h1><h2 id=\"1-处理时间窗口数据时未添加窗口或者设置处理的时间类型\"><a href=\"#1-处理时间窗口数据时未添加窗口或者设置处理的时间类型\" class=\"headerlink\" title=\"1. 处理时间窗口数据时未添加窗口或者设置处理的时间类型\"></a>1. 处理时间窗口数据时未添加窗口或者设置处理的时间类型</h2><h3 id=\"错误现象\"><a href=\"#错误现象\" class=\"headerlink\" title=\"错误现象\"></a>错误现象</h3><pre><code>Caused by: java.lang.RuntimeException: Record has Long.MIN_VALUE timestamp (= no timestamp marker). Is the time characteristic set to &#39;ProcessingTime&#39;, or did you forget to call &#39;DataStream.assignTimestampsAndWatermarks(...)&#39;?</code></pre><h3 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><ul>\n<li>assignTimestampsAndWatermarks</li>\n<li>env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</li>\n</ul>\n<h2 id=\"2-程序重写-删除算子-导致使用savepoint数据快照无法恢复启动\"><a href=\"#2-程序重写-删除算子-导致使用savepoint数据快照无法恢复启动\" class=\"headerlink\" title=\"2. 程序重写(删除算子)导致使用savepoint数据快照无法恢复启动\"></a>2. 程序重写(删除算子)导致使用savepoint数据快照无法恢复启动</h2><h3 id=\"错误现象-1\"><a href=\"#错误现象-1\" class=\"headerlink\" title=\"错误现象\"></a>错误现象</h3><pre><code>Failed to rollback to checkpoint/savepoint file:/Users/bulianwei/workspace/project/idea/flink_demo/data/test/savepoint-18c72f-803ce50127dc. Cannot map checkpoint/savepoint state for operator c27dcf7b54ef6bfd6cff02ca8870b681 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.</code></pre><h3 id=\"解决方案-1\"><a href=\"#解决方案-1\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><p>启动时使用 –allowNonRestoredState 或者 -n 跳过删除算子恢复<br>flink run -s savepointdir -n -c mainclasspath jarpath</p>\n<h2 id=\"3-使用flink-stop-停止job时出现无法停止现象\"><a href=\"#3-使用flink-stop-停止job时出现无法停止现象\" class=\"headerlink\" title=\"3. 使用flink stop 停止job时出现无法停止现象\"></a>3. 使用flink stop 停止job时出现无法停止现象</h2><h3 id=\"错误现象-2\"><a href=\"#错误现象-2\" class=\"headerlink\" title=\"错误现象\"></a>错误现象</h3><pre><code>[org.apache.flink.runtime.rest.handler.RestHandlerException: Config key [state.savepoints.dir] is not set. Property [targetDirectory] must be provided.</code></pre><h3 id=\"解决方案-2\"><a href=\"#解决方案-2\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><p>使用flink stop jobid停止job时会保存数据快照，如果未设置savepointpath会导致无法保存快照，会导致无法停止job<br>flink stop jobid -p savepointpath</p>\n<h2 id=\"4\"><a href=\"#4\" class=\"headerlink\" title=\"4.\"></a>4.</h2><h3 id=\"错误现象-3\"><a href=\"#错误现象-3\" class=\"headerlink\" title=\"错误现象\"></a>错误现象</h3><pre><code></code></pre><h3 id=\"解决方案-3\"><a href=\"#解决方案-3\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3>","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"Flink-Trouble\"><a href=\"#Flink-Trouble\" class=\"headerlink\" title=\"Flink-Trouble\"></a>Flink-Trouble</h1><h2 id=\"1-处理时间窗口数据时未添加窗口或者设置处理的时间类型\"><a href=\"#1-处理时间窗口数据时未添加窗口或者设置处理的时间类型\" class=\"headerlink\" title=\"1. 处理时间窗口数据时未添加窗口或者设置处理的时间类型\"></a>1. 处理时间窗口数据时未添加窗口或者设置处理的时间类型</h2><h3 id=\"错误现象\"><a href=\"#错误现象\" class=\"headerlink\" title=\"错误现象\"></a>错误现象</h3><pre><code>Caused by: java.lang.RuntimeException: Record has Long.MIN_VALUE timestamp (= no timestamp marker). Is the time characteristic set to &#39;ProcessingTime&#39;, or did you forget to call &#39;DataStream.assignTimestampsAndWatermarks(...)&#39;?</code></pre><h3 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><ul>\n<li>assignTimestampsAndWatermarks</li>\n<li>env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</li>\n</ul>\n<h2 id=\"2-程序重写-删除算子-导致使用savepoint数据快照无法恢复启动\"><a href=\"#2-程序重写-删除算子-导致使用savepoint数据快照无法恢复启动\" class=\"headerlink\" title=\"2. 程序重写(删除算子)导致使用savepoint数据快照无法恢复启动\"></a>2. 程序重写(删除算子)导致使用savepoint数据快照无法恢复启动</h2><h3 id=\"错误现象-1\"><a href=\"#错误现象-1\" class=\"headerlink\" title=\"错误现象\"></a>错误现象</h3><pre><code>Failed to rollback to checkpoint/savepoint file:/Users/bulianwei/workspace/project/idea/flink_demo/data/test/savepoint-18c72f-803ce50127dc. Cannot map checkpoint/savepoint state for operator c27dcf7b54ef6bfd6cff02ca8870b681 to the new program, because the operator is not available in the new program. If you want to allow to skip this, you can set the --allowNonRestoredState option on the CLI.</code></pre><h3 id=\"解决方案-1\"><a href=\"#解决方案-1\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><p>启动时使用 –allowNonRestoredState 或者 -n 跳过删除算子恢复<br>flink run -s savepointdir -n -c mainclasspath jarpath</p>\n<h2 id=\"3-使用flink-stop-停止job时出现无法停止现象\"><a href=\"#3-使用flink-stop-停止job时出现无法停止现象\" class=\"headerlink\" title=\"3. 使用flink stop 停止job时出现无法停止现象\"></a>3. 使用flink stop 停止job时出现无法停止现象</h2><h3 id=\"错误现象-2\"><a href=\"#错误现象-2\" class=\"headerlink\" title=\"错误现象\"></a>错误现象</h3><pre><code>[org.apache.flink.runtime.rest.handler.RestHandlerException: Config key [state.savepoints.dir] is not set. Property [targetDirectory] must be provided.</code></pre><h3 id=\"解决方案-2\"><a href=\"#解决方案-2\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3><p>使用flink stop jobid停止job时会保存数据快照，如果未设置savepointpath会导致无法保存快照，会导致无法停止job<br>flink stop jobid -p savepointpath</p>\n<h2 id=\"4\"><a href=\"#4\" class=\"headerlink\" title=\"4.\"></a>4.</h2><h3 id=\"错误现象-3\"><a href=\"#错误现象-3\" class=\"headerlink\" title=\"错误现象\"></a>错误现象</h3><pre><code></code></pre><h3 id=\"解决方案-3\"><a href=\"#解决方案-3\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h3>"},{"_content":"<center>GitHub</center>\n\n\n\n1. awesome\n\n   ```\n   awesome 关键词\n   ```\n\n   \n\n2. \n\n","source":"notes/Github.md","raw":"<center>GitHub</center>\n\n\n\n1. awesome\n\n   ```\n   awesome 关键词\n   ```\n\n   \n\n2. \n\n","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T11:52:16.000Z","path":"notes/Github.html","title":"","comments":1,"layout":"page","_id":"ck7ycejge000djr642gdyvd63","content":"<center>GitHub</center>\n\n\n\n<ol>\n<li><p>awesome</p>\n<pre><code>awesome 关键词</code></pre></li>\n</ol>\n<ol start=\"2\">\n<li></li>\n</ol>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<center>GitHub</center>\n\n\n\n<ol>\n<li><p>awesome</p>\n<pre><code>awesome 关键词</code></pre></li>\n</ol>\n<ol start=\"2\">\n<li></li>\n</ol>\n"},{"_content":"# Goland\n\nfallthrough   在switch 语句中进行穿透，链接两个case \n\n\n\ndfdsfasfsadf","source":"notes/Golang.md","raw":"# Goland\n\nfallthrough   在switch 语句中进行穿透，链接两个case \n\n\n\ndfdsfasfsadf","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T12:03:16.000Z","path":"notes/Golang.html","title":"","comments":1,"layout":"page","_id":"ck7ycejgj000ejr64cr530j8u","content":"<h1 id=\"Goland\"><a href=\"#Goland\" class=\"headerlink\" title=\"Goland\"></a>Goland</h1><p>fallthrough   在switch 语句中进行穿透，链接两个case </p>\n<p>dfdsfasfsadf</p>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"Goland\"><a href=\"#Goland\" class=\"headerlink\" title=\"Goland\"></a>Goland</h1><p>fallthrough   在switch 语句中进行穿透，链接两个case </p>\n<p>dfdsfasfsadf</p>\n"},{"_content":"1. 表的数据加载\n\n   1. insert\n   2. load\n\n2. 创建分区表（外表）\n\n3. 数据导出\n\n   1. hdfs dfs -get filename\n\n   2. hdfs dfs -text filename    //查看\n\n      1. insert overwrite 【local】 directory 'filename'\n\n         【row format delimited fields terminated by '\\t'】\n\n         select col， col1 from tablename\n\n         //local 与 row。。 只能在导出到本地时使用\n\n   3. Shell 命令加管道：hive -f/e | sed/awk/gred >filename\n\n   4. sqoop\n\n   5. 动态分区\n\n      1. 设置 set hive.exec.dynamic.partition=true //设置使用动态分区\n      2. 设置 set hive.exec.dynamic.partition.mode=nonstrict //使用无限制模式\n      3. insert into table tablename partition(dt) select col,col1 as dt from tablename  //一个分区 动态设置\n      4. Insert into table tablename partition(dt='2017',value) select col,col1 as value from tablename //两个分区 ，静态分区必须在设置的动态分区前面\n\n   6. 修改表\n\n      1. 重命名表 alter table tablename rename to newtablename;\n\n      2. 修改列名 alter table tablename change column c1 c2 int comment 'xxx';\n\n      3. 移动列的位置 alter table  tablename change column c1 c2 after c3;\n\n      4. 增加列 alter table tablename add columns(c1 string comment 'xxx',c2 int);\n\n      5. 删除列 alter table tablename replace columns(col string,col1 int) \n\n         //col col1 都是保留下来的列\n\n      6. 修改分割符 alter table tablename【partition(dt='xxxx')】 set serdeproperties('field.delim'='\\t')\n\n      7. 修改location;\n\n          alter table tablename 【partition()】set location 'path'\n\n      8. 内部表改外部表 \n\n         alter table tablename set tblproperties('EXTERNAL'='TRUE')  \n\n         //外部表改内部表 EXTERNAL=FALSE；\n\n   7. group by：\n\n      select col1,col2 from tablename group by col1,col2\n\n      //查询的列 col1，col2 必须出现在group by后面；\n\n   8. sum(col)  //col可为int，double 等数字类型也可以为string类型\n\n   9. 在hive中使用python脚本\n\n      add file filenamepath //先将脚本缓存到hive集群上\n\n      select * from ( select transform (col,col1) using 'filename' 【as coll ,coll2】from tablename  ) tablename\n\n   10. 【inner】join，left 【outer】 join，right 【outer】 join, full 【outer】 join，\n\n     /* +mapjoin(tablename) */,left semi join 相当于in \n\n   11. 函数\n\n       1. nvl(col,0) //如果col非空则显示col否则显示0\n       2. 【nvl2(col1,col2,col3) //如果col1为空则显示col2否则显示col3】\n       3. 【nullif(col1,col2) //如果col1与col2相等则返回null否则返回col1】\n       4. coalesce(col1,col2,col3….) //返回第一个非空（null）值，如果都为空则返回null\n       5. concat(col1,col2….) //字符串拼接，如果col1或col2中有一个为null则返回null\n       6. concat_ws(',',col1,col2…..) //带有分隔符的字符串拼接，如果有col2为null则不会拼接，拼接的内容可以为array\n       7. cast(1 as bigint) //转化1为bigint\n       8. round(0.0089,4) //保留4位小数\n       9. if (condition,a,b) //若condition为真则返回a，否则返回b；\n       10. explode(array(or map)) //将输入的一行数组或map转换成列输出\n       11. split(str,',') //第一个参数是要分隔的字符串，第二个参数是分隔符，分隔后的数据可看成数组\n\n   12. distribute by 打散数据 长和sort by（使每一个reduce里面的数据都有序）连用。distribute by 与group by 都是安key对数据进行划分，都使用reduce操作，但是distribute by只是将数据分散，而group by 是把相同的key聚集到一起然后进行聚合操作。sort by与order by，order by 是全局排序，只会有一个reduce ，sort by是确保每一个reduce 上的数据都是有序的，如果只有一个reduce时，sort by 和order by作用是一样的。\n\n   13. cluster by\n\n   14. unoin all\n\n   15. 自定义UDF：\n\n       重写evaluate函数\n\n   16. 优化：\n\n       1. join优化\n\n          hive.optimize.skewjoin=true\n\n          hive.skewjoin.key=n //当key的数量到达n时会主动进行优化\n\n       2. mapjoin\n\n          hive.atuo.conwert.join=true\n\n          hive.mapjoin.smalltable.filesize=n\n\n          select /*+mapjoin(a) */,col1,col2 form a join b\n\n          注：a表要非常小，mapjoin因为是在map端操作可以进行不等值操作\n\n       3. group by \n\n          hive.groupby.skewindata=true\n\n          Hive.groupby.mapaggr.checkinterval=n\n\n       4. job\n\n          hive.exec.parallel=true. //设置job的并行化\n\n          hive.exec.parallel.thread.numbe=n //设置最大线程数\n\n          hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat //合并文件，文件受mapred.max.split.size大小的限制\n\n          Hive.merge.smallfiles.avgsize=n  //设置输出文件平均大小小于改值，启动job合并文件\n\n          Hive.merge.size.per.task=n //合并之后文件的大小\n\n       5. \n\n       6. \n\n          \n\n   17. yarn queue -status mt1\n\n   18. mapred【hadoop】 queue -list\n\n   19. \n\n   20. \n\n       \n\n   21. 命令：\n\n       1. mapred[hadoop] job -list | grep o2o4 //查job\n       2. ps -ef| grep xxxx.sh | grep -v grep // 查进程\n       3. mapred[hadoop] queue -list // 查队列（所有）\n       4. yarn queue -status o2o4  //查队列（单个）\n       5. du -h //查文件大小\n       6. sed -n 's/old/new/gp' filename //-n  代表行   g 全局 p 打印  \n       7. 使用”;”时，不管command1是否执行成功都会执行command2； 使用”&&”时，只有command1执行成功后，command2才会执行，否则command2不执行；使用”||”时，command1执行成功后command2 不执行，否则去执行command2，总之command1和command2总有一条命令会执行。\n       8. \n\n\n* hive 分区下有数据但是查询表没有数据\n\n  可以是表的读入设置了压缩等其他格式，单纯txt的文件，表无法读取\n\n  STORED AS INPUTFORMAT \n    'com.hadoop.mapred.DeprecatedLzoTextInputFormat'   //设置压缩为lzo 所以只能读取lzo数据\n\n* Hive count(distinct ) 优化\n\n  可以先进行group by  然后再进行count\n\n  ```sql\n  select count(*),count(distinct col) from tablename where day_id=20180708\n  ```\n\n\n  select max(cnt), max(mdn_cnt), max(day_id) from (select null  as cnt , count(*) as mdn_cnt , day_id  as day_id from (select mdn, day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by mdn,day_id )a group by  day_id union all select count(*) as cnt, null  as mdn_cnt , day_id as day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by day_id )b;//union all\n\n\n  select b.cnt,a.mdn_cnt ,a.day_id from (\n  select count(*) as mdn_cnt ,day_id from (select mdn,day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by mdn,day_id)a1 group by day_id)a\n  left join \n  (select count(*) as cnt, day_id from dwi_m.dwi_res_regn_mergelocation_msk_d  b where day_id=20180708 group by day_id)b\n  on a.day_id=b.day_id;//join\n\n\n  join 和 union all 用的时间差不多  但相对 直接count distinct 要 少很多","source":"notes/Hive.md","raw":"1. 表的数据加载\n\n   1. insert\n   2. load\n\n2. 创建分区表（外表）\n\n3. 数据导出\n\n   1. hdfs dfs -get filename\n\n   2. hdfs dfs -text filename    //查看\n\n      1. insert overwrite 【local】 directory 'filename'\n\n         【row format delimited fields terminated by '\\t'】\n\n         select col， col1 from tablename\n\n         //local 与 row。。 只能在导出到本地时使用\n\n   3. Shell 命令加管道：hive -f/e | sed/awk/gred >filename\n\n   4. sqoop\n\n   5. 动态分区\n\n      1. 设置 set hive.exec.dynamic.partition=true //设置使用动态分区\n      2. 设置 set hive.exec.dynamic.partition.mode=nonstrict //使用无限制模式\n      3. insert into table tablename partition(dt) select col,col1 as dt from tablename  //一个分区 动态设置\n      4. Insert into table tablename partition(dt='2017',value) select col,col1 as value from tablename //两个分区 ，静态分区必须在设置的动态分区前面\n\n   6. 修改表\n\n      1. 重命名表 alter table tablename rename to newtablename;\n\n      2. 修改列名 alter table tablename change column c1 c2 int comment 'xxx';\n\n      3. 移动列的位置 alter table  tablename change column c1 c2 after c3;\n\n      4. 增加列 alter table tablename add columns(c1 string comment 'xxx',c2 int);\n\n      5. 删除列 alter table tablename replace columns(col string,col1 int) \n\n         //col col1 都是保留下来的列\n\n      6. 修改分割符 alter table tablename【partition(dt='xxxx')】 set serdeproperties('field.delim'='\\t')\n\n      7. 修改location;\n\n          alter table tablename 【partition()】set location 'path'\n\n      8. 内部表改外部表 \n\n         alter table tablename set tblproperties('EXTERNAL'='TRUE')  \n\n         //外部表改内部表 EXTERNAL=FALSE；\n\n   7. group by：\n\n      select col1,col2 from tablename group by col1,col2\n\n      //查询的列 col1，col2 必须出现在group by后面；\n\n   8. sum(col)  //col可为int，double 等数字类型也可以为string类型\n\n   9. 在hive中使用python脚本\n\n      add file filenamepath //先将脚本缓存到hive集群上\n\n      select * from ( select transform (col,col1) using 'filename' 【as coll ,coll2】from tablename  ) tablename\n\n   10. 【inner】join，left 【outer】 join，right 【outer】 join, full 【outer】 join，\n\n     /* +mapjoin(tablename) */,left semi join 相当于in \n\n   11. 函数\n\n       1. nvl(col,0) //如果col非空则显示col否则显示0\n       2. 【nvl2(col1,col2,col3) //如果col1为空则显示col2否则显示col3】\n       3. 【nullif(col1,col2) //如果col1与col2相等则返回null否则返回col1】\n       4. coalesce(col1,col2,col3….) //返回第一个非空（null）值，如果都为空则返回null\n       5. concat(col1,col2….) //字符串拼接，如果col1或col2中有一个为null则返回null\n       6. concat_ws(',',col1,col2…..) //带有分隔符的字符串拼接，如果有col2为null则不会拼接，拼接的内容可以为array\n       7. cast(1 as bigint) //转化1为bigint\n       8. round(0.0089,4) //保留4位小数\n       9. if (condition,a,b) //若condition为真则返回a，否则返回b；\n       10. explode(array(or map)) //将输入的一行数组或map转换成列输出\n       11. split(str,',') //第一个参数是要分隔的字符串，第二个参数是分隔符，分隔后的数据可看成数组\n\n   12. distribute by 打散数据 长和sort by（使每一个reduce里面的数据都有序）连用。distribute by 与group by 都是安key对数据进行划分，都使用reduce操作，但是distribute by只是将数据分散，而group by 是把相同的key聚集到一起然后进行聚合操作。sort by与order by，order by 是全局排序，只会有一个reduce ，sort by是确保每一个reduce 上的数据都是有序的，如果只有一个reduce时，sort by 和order by作用是一样的。\n\n   13. cluster by\n\n   14. unoin all\n\n   15. 自定义UDF：\n\n       重写evaluate函数\n\n   16. 优化：\n\n       1. join优化\n\n          hive.optimize.skewjoin=true\n\n          hive.skewjoin.key=n //当key的数量到达n时会主动进行优化\n\n       2. mapjoin\n\n          hive.atuo.conwert.join=true\n\n          hive.mapjoin.smalltable.filesize=n\n\n          select /*+mapjoin(a) */,col1,col2 form a join b\n\n          注：a表要非常小，mapjoin因为是在map端操作可以进行不等值操作\n\n       3. group by \n\n          hive.groupby.skewindata=true\n\n          Hive.groupby.mapaggr.checkinterval=n\n\n       4. job\n\n          hive.exec.parallel=true. //设置job的并行化\n\n          hive.exec.parallel.thread.numbe=n //设置最大线程数\n\n          hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat //合并文件，文件受mapred.max.split.size大小的限制\n\n          Hive.merge.smallfiles.avgsize=n  //设置输出文件平均大小小于改值，启动job合并文件\n\n          Hive.merge.size.per.task=n //合并之后文件的大小\n\n       5. \n\n       6. \n\n          \n\n   17. yarn queue -status mt1\n\n   18. mapred【hadoop】 queue -list\n\n   19. \n\n   20. \n\n       \n\n   21. 命令：\n\n       1. mapred[hadoop] job -list | grep o2o4 //查job\n       2. ps -ef| grep xxxx.sh | grep -v grep // 查进程\n       3. mapred[hadoop] queue -list // 查队列（所有）\n       4. yarn queue -status o2o4  //查队列（单个）\n       5. du -h //查文件大小\n       6. sed -n 's/old/new/gp' filename //-n  代表行   g 全局 p 打印  \n       7. 使用”;”时，不管command1是否执行成功都会执行command2； 使用”&&”时，只有command1执行成功后，command2才会执行，否则command2不执行；使用”||”时，command1执行成功后command2 不执行，否则去执行command2，总之command1和command2总有一条命令会执行。\n       8. \n\n\n* hive 分区下有数据但是查询表没有数据\n\n  可以是表的读入设置了压缩等其他格式，单纯txt的文件，表无法读取\n\n  STORED AS INPUTFORMAT \n    'com.hadoop.mapred.DeprecatedLzoTextInputFormat'   //设置压缩为lzo 所以只能读取lzo数据\n\n* Hive count(distinct ) 优化\n\n  可以先进行group by  然后再进行count\n\n  ```sql\n  select count(*),count(distinct col) from tablename where day_id=20180708\n  ```\n\n\n  select max(cnt), max(mdn_cnt), max(day_id) from (select null  as cnt , count(*) as mdn_cnt , day_id  as day_id from (select mdn, day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by mdn,day_id )a group by  day_id union all select count(*) as cnt, null  as mdn_cnt , day_id as day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by day_id )b;//union all\n\n\n  select b.cnt,a.mdn_cnt ,a.day_id from (\n  select count(*) as mdn_cnt ,day_id from (select mdn,day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by mdn,day_id)a1 group by day_id)a\n  left join \n  (select count(*) as cnt, day_id from dwi_m.dwi_res_regn_mergelocation_msk_d  b where day_id=20180708 group by day_id)b\n  on a.day_id=b.day_id;//join\n\n\n  join 和 union all 用的时间差不多  但相对 直接count distinct 要 少很多","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T11:52:16.000Z","path":"notes/Hive.html","title":"","comments":1,"layout":"page","_id":"ck7ycejgl000fjr64uh78p0g9","content":"<ol>\n<li><p>表的数据加载</p>\n<ol>\n<li>insert</li>\n<li>load</li>\n</ol>\n</li>\n<li><p>创建分区表（外表）</p>\n</li>\n<li><p>数据导出</p>\n<ol>\n<li><p>hdfs dfs -get filename</p>\n</li>\n<li><p>hdfs dfs -text filename    //查看</p>\n<ol>\n<li><p>insert overwrite 【local】 directory ‘filename’</p>\n<p>【row format delimited fields terminated by ‘\\t’】</p>\n<p>select col， col1 from tablename</p>\n<p>//local 与 row。。 只能在导出到本地时使用</p>\n</li>\n</ol>\n</li>\n<li><p>Shell 命令加管道：hive -f/e | sed/awk/gred &gt;filename</p>\n</li>\n<li><p>sqoop</p>\n</li>\n<li><p>动态分区</p>\n<ol>\n<li>设置 set hive.exec.dynamic.partition=true //设置使用动态分区</li>\n<li>设置 set hive.exec.dynamic.partition.mode=nonstrict //使用无限制模式</li>\n<li>insert into table tablename partition(dt) select col,col1 as dt from tablename  //一个分区 动态设置</li>\n<li>Insert into table tablename partition(dt=’2017’,value) select col,col1 as value from tablename //两个分区 ，静态分区必须在设置的动态分区前面</li>\n</ol>\n</li>\n<li><p>修改表</p>\n<ol>\n<li><p>重命名表 alter table tablename rename to newtablename;</p>\n</li>\n<li><p>修改列名 alter table tablename change column c1 c2 int comment ‘xxx’;</p>\n</li>\n<li><p>移动列的位置 alter table  tablename change column c1 c2 after c3;</p>\n</li>\n<li><p>增加列 alter table tablename add columns(c1 string comment ‘xxx’,c2 int);</p>\n</li>\n<li><p>删除列 alter table tablename replace columns(col string,col1 int) </p>\n<p>//col col1 都是保留下来的列</p>\n</li>\n<li><p>修改分割符 alter table tablename【partition(dt=’xxxx’)】 set serdeproperties(‘field.delim’=’\\t’)</p>\n</li>\n<li><p>修改location;</p>\n<p> alter table tablename 【partition()】set location ‘path’</p>\n</li>\n<li><p>内部表改外部表 </p>\n<p>alter table tablename set tblproperties(‘EXTERNAL’=’TRUE’)  </p>\n<p>//外部表改内部表 EXTERNAL=FALSE；</p>\n</li>\n</ol>\n</li>\n<li><p>group by：</p>\n<p>select col1,col2 from tablename group by col1,col2</p>\n<p>//查询的列 col1，col2 必须出现在group by后面；</p>\n</li>\n<li><p>sum(col)  //col可为int，double 等数字类型也可以为string类型</p>\n</li>\n<li><p>在hive中使用python脚本</p>\n<p>add file filenamepath //先将脚本缓存到hive集群上</p>\n<p>select * from ( select transform (col,col1) using ‘filename’ 【as coll ,coll2】from tablename  ) tablename</p>\n</li>\n<li><p>【inner】join，left 【outer】 join，right 【outer】 join, full 【outer】 join，</p>\n<p>/* +mapjoin(tablename) */,left semi join 相当于in </p>\n</li>\n<li><p>函数</p>\n<ol>\n<li>nvl(col,0) //如果col非空则显示col否则显示0</li>\n<li>【nvl2(col1,col2,col3) //如果col1为空则显示col2否则显示col3】</li>\n<li>【nullif(col1,col2) //如果col1与col2相等则返回null否则返回col1】</li>\n<li>coalesce(col1,col2,col3….) //返回第一个非空（null）值，如果都为空则返回null</li>\n<li>concat(col1,col2….) //字符串拼接，如果col1或col2中有一个为null则返回null</li>\n<li>concat_ws(‘,’,col1,col2…..) //带有分隔符的字符串拼接，如果有col2为null则不会拼接，拼接的内容可以为array</li>\n<li>cast(1 as bigint) //转化1为bigint</li>\n<li>round(0.0089,4) //保留4位小数</li>\n<li>if (condition,a,b) //若condition为真则返回a，否则返回b；</li>\n<li>explode(array(or map)) //将输入的一行数组或map转换成列输出</li>\n<li>split(str,’,’) //第一个参数是要分隔的字符串，第二个参数是分隔符，分隔后的数据可看成数组</li>\n</ol>\n</li>\n<li><p>distribute by 打散数据 长和sort by（使每一个reduce里面的数据都有序）连用。distribute by 与group by 都是安key对数据进行划分，都使用reduce操作，但是distribute by只是将数据分散，而group by 是把相同的key聚集到一起然后进行聚合操作。sort by与order by，order by 是全局排序，只会有一个reduce ，sort by是确保每一个reduce 上的数据都是有序的，如果只有一个reduce时，sort by 和order by作用是一样的。</p>\n</li>\n<li><p>cluster by</p>\n</li>\n<li><p>unoin all</p>\n</li>\n<li><p>自定义UDF：</p>\n<p>重写evaluate函数</p>\n</li>\n<li><p>优化：</p>\n<ol>\n<li><p>join优化</p>\n<p>hive.optimize.skewjoin=true</p>\n<p>hive.skewjoin.key=n //当key的数量到达n时会主动进行优化</p>\n</li>\n<li><p>mapjoin</p>\n<p>hive.atuo.conwert.join=true</p>\n<p>hive.mapjoin.smalltable.filesize=n</p>\n<p>select /*+mapjoin(a) */,col1,col2 form a join b</p>\n<p>注：a表要非常小，mapjoin因为是在map端操作可以进行不等值操作</p>\n</li>\n<li><p>group by </p>\n<p>hive.groupby.skewindata=true</p>\n<p>Hive.groupby.mapaggr.checkinterval=n</p>\n</li>\n<li><p>job</p>\n<p>hive.exec.parallel=true. //设置job的并行化</p>\n<p>hive.exec.parallel.thread.numbe=n //设置最大线程数</p>\n<p>hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat //合并文件，文件受mapred.max.split.size大小的限制</p>\n<p>Hive.merge.smallfiles.avgsize=n  //设置输出文件平均大小小于改值，启动job合并文件</p>\n<p>Hive.merge.size.per.task=n //合并之后文件的大小</p>\n</li>\n<li></li>\n<li></li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n<ol start=\"17\">\n<li><p>yarn queue -status mt1</p>\n</li>\n<li><p>mapred【hadoop】 queue -list</p>\n</li>\n<li></li>\n<li></li>\n</ol>\n<ol start=\"21\">\n<li><p>命令：</p>\n<ol>\n<li>mapred[hadoop] job -list | grep o2o4 //查job</li>\n<li>ps -ef| grep xxxx.sh | grep -v grep // 查进程</li>\n<li>mapred[hadoop] queue -list // 查队列（所有）</li>\n<li>yarn queue -status o2o4  //查队列（单个）</li>\n<li>du -h //查文件大小</li>\n<li>sed -n ‘s/old/new/gp’ filename //-n  代表行   g 全局 p 打印  </li>\n<li>使用”;”时，不管command1是否执行成功都会执行command2； 使用”&amp;&amp;”时，只有command1执行成功后，command2才会执行，否则command2不执行；使用”||”时，command1执行成功后command2 不执行，否则去执行command2，总之command1和command2总有一条命令会执行。</li>\n<li></li>\n</ol>\n</li>\n</ol>\n<ul>\n<li><p>hive 分区下有数据但是查询表没有数据</p>\n<p>可以是表的读入设置了压缩等其他格式，单纯txt的文件，表无法读取</p>\n<p>STORED AS INPUTFORMAT<br>  ‘com.hadoop.mapred.DeprecatedLzoTextInputFormat’   //设置压缩为lzo 所以只能读取lzo数据</p>\n</li>\n<li><p>Hive count(distinct ) 优化</p>\n<p>可以先进行group by  然后再进行count</p>\n<pre class=\" language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">select</span> <span class=\"token function\">count</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token function\">count</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">distinct</span> col<span class=\"token punctuation\">)</span> <span class=\"token keyword\">from</span> tablename <span class=\"token keyword\">where</span> day_id<span class=\"token operator\">=</span><span class=\"token number\">20180708</span></code></pre>\n</li>\n</ul>\n<p>  select max(cnt), max(mdn_cnt), max(day_id) from (select null  as cnt , count(<em>) as mdn_cnt , day_id  as day_id from (select mdn, day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by mdn,day_id )a group by  day_id union all select count(</em>) as cnt, null  as mdn_cnt , day_id as day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by day_id )b;//union all</p>\n<p>  select b.cnt,a.mdn_cnt ,a.day_id from (<br>  select count(<em>) as mdn_cnt ,day_id from (select mdn,day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by mdn,day_id)a1 group by day_id)a<br>  left join<br>  (select count(</em>) as cnt, day_id from dwi_m.dwi_res_regn_mergelocation_msk_d  b where day_id=20180708 group by day_id)b<br>  on a.day_id=b.day_id;//join</p>\n<p>  join 和 union all 用的时间差不多  但相对 直接count distinct 要 少很多</p>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<ol>\n<li><p>表的数据加载</p>\n<ol>\n<li>insert</li>\n<li>load</li>\n</ol>\n</li>\n<li><p>创建分区表（外表）</p>\n</li>\n<li><p>数据导出</p>\n<ol>\n<li><p>hdfs dfs -get filename</p>\n</li>\n<li><p>hdfs dfs -text filename    //查看</p>\n<ol>\n<li><p>insert overwrite 【local】 directory ‘filename’</p>\n<p>【row format delimited fields terminated by ‘\\t’】</p>\n<p>select col， col1 from tablename</p>\n<p>//local 与 row。。 只能在导出到本地时使用</p>\n</li>\n</ol>\n</li>\n<li><p>Shell 命令加管道：hive -f/e | sed/awk/gred &gt;filename</p>\n</li>\n<li><p>sqoop</p>\n</li>\n<li><p>动态分区</p>\n<ol>\n<li>设置 set hive.exec.dynamic.partition=true //设置使用动态分区</li>\n<li>设置 set hive.exec.dynamic.partition.mode=nonstrict //使用无限制模式</li>\n<li>insert into table tablename partition(dt) select col,col1 as dt from tablename  //一个分区 动态设置</li>\n<li>Insert into table tablename partition(dt=’2017’,value) select col,col1 as value from tablename //两个分区 ，静态分区必须在设置的动态分区前面</li>\n</ol>\n</li>\n<li><p>修改表</p>\n<ol>\n<li><p>重命名表 alter table tablename rename to newtablename;</p>\n</li>\n<li><p>修改列名 alter table tablename change column c1 c2 int comment ‘xxx’;</p>\n</li>\n<li><p>移动列的位置 alter table  tablename change column c1 c2 after c3;</p>\n</li>\n<li><p>增加列 alter table tablename add columns(c1 string comment ‘xxx’,c2 int);</p>\n</li>\n<li><p>删除列 alter table tablename replace columns(col string,col1 int) </p>\n<p>//col col1 都是保留下来的列</p>\n</li>\n<li><p>修改分割符 alter table tablename【partition(dt=’xxxx’)】 set serdeproperties(‘field.delim’=’\\t’)</p>\n</li>\n<li><p>修改location;</p>\n<p> alter table tablename 【partition()】set location ‘path’</p>\n</li>\n<li><p>内部表改外部表 </p>\n<p>alter table tablename set tblproperties(‘EXTERNAL’=’TRUE’)  </p>\n<p>//外部表改内部表 EXTERNAL=FALSE；</p>\n</li>\n</ol>\n</li>\n<li><p>group by：</p>\n<p>select col1,col2 from tablename group by col1,col2</p>\n<p>//查询的列 col1，col2 必须出现在group by后面；</p>\n</li>\n<li><p>sum(col)  //col可为int，double 等数字类型也可以为string类型</p>\n</li>\n<li><p>在hive中使用python脚本</p>\n<p>add file filenamepath //先将脚本缓存到hive集群上</p>\n<p>select * from ( select transform (col,col1) using ‘filename’ 【as coll ,coll2】from tablename  ) tablename</p>\n</li>\n<li><p>【inner】join，left 【outer】 join，right 【outer】 join, full 【outer】 join，</p>\n<p>/* +mapjoin(tablename) */,left semi join 相当于in </p>\n</li>\n<li><p>函数</p>\n<ol>\n<li>nvl(col,0) //如果col非空则显示col否则显示0</li>\n<li>【nvl2(col1,col2,col3) //如果col1为空则显示col2否则显示col3】</li>\n<li>【nullif(col1,col2) //如果col1与col2相等则返回null否则返回col1】</li>\n<li>coalesce(col1,col2,col3….) //返回第一个非空（null）值，如果都为空则返回null</li>\n<li>concat(col1,col2….) //字符串拼接，如果col1或col2中有一个为null则返回null</li>\n<li>concat_ws(‘,’,col1,col2…..) //带有分隔符的字符串拼接，如果有col2为null则不会拼接，拼接的内容可以为array</li>\n<li>cast(1 as bigint) //转化1为bigint</li>\n<li>round(0.0089,4) //保留4位小数</li>\n<li>if (condition,a,b) //若condition为真则返回a，否则返回b；</li>\n<li>explode(array(or map)) //将输入的一行数组或map转换成列输出</li>\n<li>split(str,’,’) //第一个参数是要分隔的字符串，第二个参数是分隔符，分隔后的数据可看成数组</li>\n</ol>\n</li>\n<li><p>distribute by 打散数据 长和sort by（使每一个reduce里面的数据都有序）连用。distribute by 与group by 都是安key对数据进行划分，都使用reduce操作，但是distribute by只是将数据分散，而group by 是把相同的key聚集到一起然后进行聚合操作。sort by与order by，order by 是全局排序，只会有一个reduce ，sort by是确保每一个reduce 上的数据都是有序的，如果只有一个reduce时，sort by 和order by作用是一样的。</p>\n</li>\n<li><p>cluster by</p>\n</li>\n<li><p>unoin all</p>\n</li>\n<li><p>自定义UDF：</p>\n<p>重写evaluate函数</p>\n</li>\n<li><p>优化：</p>\n<ol>\n<li><p>join优化</p>\n<p>hive.optimize.skewjoin=true</p>\n<p>hive.skewjoin.key=n //当key的数量到达n时会主动进行优化</p>\n</li>\n<li><p>mapjoin</p>\n<p>hive.atuo.conwert.join=true</p>\n<p>hive.mapjoin.smalltable.filesize=n</p>\n<p>select /*+mapjoin(a) */,col1,col2 form a join b</p>\n<p>注：a表要非常小，mapjoin因为是在map端操作可以进行不等值操作</p>\n</li>\n<li><p>group by </p>\n<p>hive.groupby.skewindata=true</p>\n<p>Hive.groupby.mapaggr.checkinterval=n</p>\n</li>\n<li><p>job</p>\n<p>hive.exec.parallel=true. //设置job的并行化</p>\n<p>hive.exec.parallel.thread.numbe=n //设置最大线程数</p>\n<p>hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat //合并文件，文件受mapred.max.split.size大小的限制</p>\n<p>Hive.merge.smallfiles.avgsize=n  //设置输出文件平均大小小于改值，启动job合并文件</p>\n<p>Hive.merge.size.per.task=n //合并之后文件的大小</p>\n</li>\n<li></li>\n<li></li>\n</ol>\n</li>\n</ol>\n</li>\n</ol>\n<ol start=\"17\">\n<li><p>yarn queue -status mt1</p>\n</li>\n<li><p>mapred【hadoop】 queue -list</p>\n</li>\n<li></li>\n<li></li>\n</ol>\n<ol start=\"21\">\n<li><p>命令：</p>\n<ol>\n<li>mapred[hadoop] job -list | grep o2o4 //查job</li>\n<li>ps -ef| grep xxxx.sh | grep -v grep // 查进程</li>\n<li>mapred[hadoop] queue -list // 查队列（所有）</li>\n<li>yarn queue -status o2o4  //查队列（单个）</li>\n<li>du -h //查文件大小</li>\n<li>sed -n ‘s/old/new/gp’ filename //-n  代表行   g 全局 p 打印  </li>\n<li>使用”;”时，不管command1是否执行成功都会执行command2； 使用”&amp;&amp;”时，只有command1执行成功后，command2才会执行，否则command2不执行；使用”||”时，command1执行成功后command2 不执行，否则去执行command2，总之command1和command2总有一条命令会执行。</li>\n<li></li>\n</ol>\n</li>\n</ol>\n<ul>\n<li><p>hive 分区下有数据但是查询表没有数据</p>\n<p>可以是表的读入设置了压缩等其他格式，单纯txt的文件，表无法读取</p>\n<p>STORED AS INPUTFORMAT<br>  ‘com.hadoop.mapred.DeprecatedLzoTextInputFormat’   //设置压缩为lzo 所以只能读取lzo数据</p>\n</li>\n<li><p>Hive count(distinct ) 优化</p>\n<p>可以先进行group by  然后再进行count</p>\n<pre><code class=\"sql\">select count(*),count(distinct col) from tablename where day_id=20180708</code></pre>\n</li>\n</ul>\n<p>  select max(cnt), max(mdn_cnt), max(day_id) from (select null  as cnt , count(<em>) as mdn_cnt , day_id  as day_id from (select mdn, day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by mdn,day_id )a group by  day_id union all select count(</em>) as cnt, null  as mdn_cnt , day_id as day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by day_id )b;//union all</p>\n<p>  select b.cnt,a.mdn_cnt ,a.day_id from (<br>  select count(<em>) as mdn_cnt ,day_id from (select mdn,day_id from dwi_m.dwi_res_regn_mergelocation_msk_d where day_id=20180708 group by mdn,day_id)a1 group by day_id)a<br>  left join<br>  (select count(</em>) as cnt, day_id from dwi_m.dwi_res_regn_mergelocation_msk_d  b where day_id=20180708 group by day_id)b<br>  on a.day_id=b.day_id;//join</p>\n<p>  join 和 union all 用的时间差不多  但相对 直接count distinct 要 少很多</p>\n"},{"title":"Kafka","date":"2020-01-13T07:22:21.000Z","_content":"# Kafka\n\n一个分区内的数据才能保证幂等性和有序性\n\n##  架构\n\n![架构图](img/kafka架构.png \"dd\")\n\n## 名次解释\n+ Broker：Kafka服务器\n+ Producer：生产者，生产消息\n+ Consumer：消费者，消费数据\n+ Consumer Group：消费者组，某一个分区只能被同一个消费者组内的一个消费者消费\n+ Topic：消息主题。逻辑概念\n+ Partition：消息分区。消息物理存储上概念\n+ Offset：偏移量\n+ Replia：副本，同一个分区（Partition）内的副本分Leader和Follower\n+ Leader：主副本。对外提供服务\n+ Follower：从副本。做数据同步工作\n+ acks：acknowledgments，消息接收后的确认值\n+ AR（Assigned Replicas）：分区中所有副本\n+ ISR（In-Sync Replicas）：所有与Leader 部分保持一定程度的副本（包含Leader副本在内）组成ISR\n+ OSR（Out-Sync Replicas）：与Leader副本同步滞后过多的副本\n+ HW（High Watermark）：指消费者能够见到的最大Offset，ISR队列中最小的LEO。高水位，标志一个特定的Offset，消费者只能拉取到这个Offset之前的消息，Consumer可见数据的最大偏移量。保证副本数据一致性\n+ LEO（Log End Offset）：每个副本最后一个Offset。即日志末端位移，记录了该副本底层日志中下一条消息的位移值。注意是下一条消息，也就是说如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0,9]。\n>1 Topic = n Partition\n>1 Partition = n Replica\n\n## Producer（生产者）\n\n拦截器->序列化器->分区器\n\n### 命令\n\n#### 使用脚本\nkafka-console-producer.sh\n\n#### 常用参数\n+ --broker-list\t//kafka集群地址\n+ --topic\t//生产主题\n\n\n#### 举例\nkafka-console-producer.sh\t--broker-list\tlocalhost:9092\t--topic\ttest\n\n### 分区策略\n1. 指定分区（Partition）：直接按照分区号\n2. 没有分区（Partition）但是有Key：（默认）根据Key的Hash值获取分区号\n3. 即没有分区（Partition）也没有Key：第一次险随机获取一个分区号，然后在这个分区号的基础上轮训所有分区号。\n\t\n\t>如果有3个分区，0、1、2当随机获取的分区号为1时，然后1、2、0、1、2、0、1 ... 这样轮询下去\n\n### 发送返回值\nACKS值：\n+ 0：Producer不等待Leader所在的Broker的acks，这一操作保证了最低延迟，Leader所在的Broker一接到消息不等消息保存就会先将ack的值返回给Producer。此值下当Leader所在的Broker宕机时会造成数据丢失\n+ 1：Producer等待Leader所在的Broker的acks，当Leader所在Broker接收到消息保存完消息后才会将ack值返回给Producer。在Leader的消息落盘后还没等Follower同步数据其所在Broker宕机后，就会导致数据丢失。（默认）\n+ -1（all）：Producer等待Leader所在的Broker的acks，当Leader和ISR中的Follower全部将消息落盘之后所在的Broker才会将ack的值返回给Producer。但是如果Follower将消息同步完以后，Leader所在的Broker还没将ack的值返回给Producer之前，Leader所在的Broker发生故障，会造成数据重复\n\n### 幂等性\n设置：enable.idempotence=true\n解决单次会话单个分区数据重复问题\n开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带SequenceNumber，而Broker会对<PID，Partition，SequenceNumber>做缓存，当具有相同主键的消息提及时，Broker只会持久化一条。\n\n## Consumer（消费者）\n\n当前消费者需要提交的消费位移是offset+1\n\n### 命令\n\n#### 使用脚本\nkafka-console-consumer.sh\t\n\n#### 常用参数\n+ --bootstrap-server\t//目标服务器地址\n+ --topic\t//消费主题\n\n\n#### 举例\nkafka-console-consumer.sh\t--bootstrap-server\tlocalhost:9092\t--topic\ttest\n\n### 分配策略\n+ RoundRobin\t将订阅的所有主题看作一个整体。（按组分配）\n+ Range\t将订阅的单个主题看作整体。（按主题分配）\n\n## Topic（主题）\n\n###\t命令\n\n#### 使用脚本\nkafka-configs.sh\t//0.9中已经废弃kafka-topics.sh脚本，改用kafka-configs.sh来配置主题\n\n#### 常用参数\n+ --zookeeper\t//zookeeper地址，多个用逗号链接\n+ --create\t//创建命令\n+ --list\t//列出所有主题\n+ --alter\t//修改主题\n+ --delete\t//删除主题\n+ --topic\t//主题\n+ --partitions\t//主题分区数，分区数要小于等于broker数\n+ --replication-fator\t//主题副本数\n\n\n#### 举例\n+ 创建主题：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--create\t--topic\ttest\t--partition\t2\t--replication-fator\t3\n+ 列出所有主题：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--list\n+ 列出主题详情：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--describe\t--topic test\n+ 修改主题：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--alter\t--topic test\n+ 删除主题：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--delete\t--topic test\n\n## Kafka高读写\n\n+ 顺序读写\n+ 零复制\n\t![普通拷贝](./img/普通拷贝.png) \n\t![零拷贝 ](./img/零拷贝.png)\n\t\n\t","source":"notes/Kafka.md","raw":"---\ntitle: Kafka\ndate: 2020-01-13 15:22:21\n---\n# Kafka\n\n一个分区内的数据才能保证幂等性和有序性\n\n##  架构\n\n![架构图](img/kafka架构.png \"dd\")\n\n## 名次解释\n+ Broker：Kafka服务器\n+ Producer：生产者，生产消息\n+ Consumer：消费者，消费数据\n+ Consumer Group：消费者组，某一个分区只能被同一个消费者组内的一个消费者消费\n+ Topic：消息主题。逻辑概念\n+ Partition：消息分区。消息物理存储上概念\n+ Offset：偏移量\n+ Replia：副本，同一个分区（Partition）内的副本分Leader和Follower\n+ Leader：主副本。对外提供服务\n+ Follower：从副本。做数据同步工作\n+ acks：acknowledgments，消息接收后的确认值\n+ AR（Assigned Replicas）：分区中所有副本\n+ ISR（In-Sync Replicas）：所有与Leader 部分保持一定程度的副本（包含Leader副本在内）组成ISR\n+ OSR（Out-Sync Replicas）：与Leader副本同步滞后过多的副本\n+ HW（High Watermark）：指消费者能够见到的最大Offset，ISR队列中最小的LEO。高水位，标志一个特定的Offset，消费者只能拉取到这个Offset之前的消息，Consumer可见数据的最大偏移量。保证副本数据一致性\n+ LEO（Log End Offset）：每个副本最后一个Offset。即日志末端位移，记录了该副本底层日志中下一条消息的位移值。注意是下一条消息，也就是说如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0,9]。\n>1 Topic = n Partition\n>1 Partition = n Replica\n\n## Producer（生产者）\n\n拦截器->序列化器->分区器\n\n### 命令\n\n#### 使用脚本\nkafka-console-producer.sh\n\n#### 常用参数\n+ --broker-list\t//kafka集群地址\n+ --topic\t//生产主题\n\n\n#### 举例\nkafka-console-producer.sh\t--broker-list\tlocalhost:9092\t--topic\ttest\n\n### 分区策略\n1. 指定分区（Partition）：直接按照分区号\n2. 没有分区（Partition）但是有Key：（默认）根据Key的Hash值获取分区号\n3. 即没有分区（Partition）也没有Key：第一次险随机获取一个分区号，然后在这个分区号的基础上轮训所有分区号。\n\t\n\t>如果有3个分区，0、1、2当随机获取的分区号为1时，然后1、2、0、1、2、0、1 ... 这样轮询下去\n\n### 发送返回值\nACKS值：\n+ 0：Producer不等待Leader所在的Broker的acks，这一操作保证了最低延迟，Leader所在的Broker一接到消息不等消息保存就会先将ack的值返回给Producer。此值下当Leader所在的Broker宕机时会造成数据丢失\n+ 1：Producer等待Leader所在的Broker的acks，当Leader所在Broker接收到消息保存完消息后才会将ack值返回给Producer。在Leader的消息落盘后还没等Follower同步数据其所在Broker宕机后，就会导致数据丢失。（默认）\n+ -1（all）：Producer等待Leader所在的Broker的acks，当Leader和ISR中的Follower全部将消息落盘之后所在的Broker才会将ack的值返回给Producer。但是如果Follower将消息同步完以后，Leader所在的Broker还没将ack的值返回给Producer之前，Leader所在的Broker发生故障，会造成数据重复\n\n### 幂等性\n设置：enable.idempotence=true\n解决单次会话单个分区数据重复问题\n开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带SequenceNumber，而Broker会对<PID，Partition，SequenceNumber>做缓存，当具有相同主键的消息提及时，Broker只会持久化一条。\n\n## Consumer（消费者）\n\n当前消费者需要提交的消费位移是offset+1\n\n### 命令\n\n#### 使用脚本\nkafka-console-consumer.sh\t\n\n#### 常用参数\n+ --bootstrap-server\t//目标服务器地址\n+ --topic\t//消费主题\n\n\n#### 举例\nkafka-console-consumer.sh\t--bootstrap-server\tlocalhost:9092\t--topic\ttest\n\n### 分配策略\n+ RoundRobin\t将订阅的所有主题看作一个整体。（按组分配）\n+ Range\t将订阅的单个主题看作整体。（按主题分配）\n\n## Topic（主题）\n\n###\t命令\n\n#### 使用脚本\nkafka-configs.sh\t//0.9中已经废弃kafka-topics.sh脚本，改用kafka-configs.sh来配置主题\n\n#### 常用参数\n+ --zookeeper\t//zookeeper地址，多个用逗号链接\n+ --create\t//创建命令\n+ --list\t//列出所有主题\n+ --alter\t//修改主题\n+ --delete\t//删除主题\n+ --topic\t//主题\n+ --partitions\t//主题分区数，分区数要小于等于broker数\n+ --replication-fator\t//主题副本数\n\n\n#### 举例\n+ 创建主题：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--create\t--topic\ttest\t--partition\t2\t--replication-fator\t3\n+ 列出所有主题：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--list\n+ 列出主题详情：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--describe\t--topic test\n+ 修改主题：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--alter\t--topic test\n+ 删除主题：kafka-configs.sh\t--zookeeper\tlocalhost:9092\t--delete\t--topic test\n\n## Kafka高读写\n\n+ 顺序读写\n+ 零复制\n\t![普通拷贝](./img/普通拷贝.png) \n\t![零拷贝 ](./img/零拷贝.png)\n\t\n\t","updated":"2020-03-18T11:52:16.000Z","path":"notes/Kafka.html","comments":1,"layout":"page","_id":"ck7ycejgn000gjr64hbplsd7e","content":"<h1 id=\"Kafka\"><a href=\"#Kafka\" class=\"headerlink\" title=\"Kafka\"></a>Kafka</h1><p>一个分区内的数据才能保证幂等性和有序性</p>\n<h2 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h2><p><img src=\"img/kafka%E6%9E%B6%E6%9E%84.png\" alt=\"架构图\" title=\"dd\"></p>\n<h2 id=\"名次解释\"><a href=\"#名次解释\" class=\"headerlink\" title=\"名次解释\"></a>名次解释</h2><ul>\n<li>Broker：Kafka服务器</li>\n<li>Producer：生产者，生产消息</li>\n<li>Consumer：消费者，消费数据</li>\n<li>Consumer Group：消费者组，某一个分区只能被同一个消费者组内的一个消费者消费</li>\n<li>Topic：消息主题。逻辑概念</li>\n<li>Partition：消息分区。消息物理存储上概念</li>\n<li>Offset：偏移量</li>\n<li>Replia：副本，同一个分区（Partition）内的副本分Leader和Follower</li>\n<li>Leader：主副本。对外提供服务</li>\n<li>Follower：从副本。做数据同步工作</li>\n<li>acks：acknowledgments，消息接收后的确认值</li>\n<li>AR（Assigned Replicas）：分区中所有副本</li>\n<li>ISR（In-Sync Replicas）：所有与Leader 部分保持一定程度的副本（包含Leader副本在内）组成ISR</li>\n<li>OSR（Out-Sync Replicas）：与Leader副本同步滞后过多的副本</li>\n<li>HW（High Watermark）：指消费者能够见到的最大Offset，ISR队列中最小的LEO。高水位，标志一个特定的Offset，消费者只能拉取到这个Offset之前的消息，Consumer可见数据的最大偏移量。保证副本数据一致性</li>\n<li>LEO（Log End Offset）：每个副本最后一个Offset。即日志末端位移，记录了该副本底层日志中下一条消息的位移值。注意是下一条消息，也就是说如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0,9]。<blockquote>\n<p>1 Topic = n Partition<br>1 Partition = n Replica</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Producer（生产者）\"><a href=\"#Producer（生产者）\" class=\"headerlink\" title=\"Producer（生产者）\"></a>Producer（生产者）</h2><p>拦截器-&gt;序列化器-&gt;分区器</p>\n<h3 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h3><h4 id=\"使用脚本\"><a href=\"#使用脚本\" class=\"headerlink\" title=\"使用脚本\"></a>使用脚本</h4><p>kafka-console-producer.sh</p>\n<h4 id=\"常用参数\"><a href=\"#常用参数\" class=\"headerlink\" title=\"常用参数\"></a>常用参数</h4><ul>\n<li>–broker-list    //kafka集群地址</li>\n<li>–topic    //生产主题</li>\n</ul>\n<h4 id=\"举例\"><a href=\"#举例\" class=\"headerlink\" title=\"举例\"></a>举例</h4><p>kafka-console-producer.sh    –broker-list    localhost:9092    –topic    test</p>\n<h3 id=\"分区策略\"><a href=\"#分区策略\" class=\"headerlink\" title=\"分区策略\"></a>分区策略</h3><ol>\n<li><p>指定分区（Partition）：直接按照分区号</p>\n</li>\n<li><p>没有分区（Partition）但是有Key：（默认）根据Key的Hash值获取分区号</p>\n</li>\n<li><p>即没有分区（Partition）也没有Key：第一次险随机获取一个分区号，然后在这个分区号的基础上轮训所有分区号。</p>\n<blockquote>\n<p>如果有3个分区，0、1、2当随机获取的分区号为1时，然后1、2、0、1、2、0、1 … 这样轮询下去</p>\n</blockquote>\n</li>\n</ol>\n<h3 id=\"发送返回值\"><a href=\"#发送返回值\" class=\"headerlink\" title=\"发送返回值\"></a>发送返回值</h3><p>ACKS值：</p>\n<ul>\n<li>0：Producer不等待Leader所在的Broker的acks，这一操作保证了最低延迟，Leader所在的Broker一接到消息不等消息保存就会先将ack的值返回给Producer。此值下当Leader所在的Broker宕机时会造成数据丢失</li>\n<li>1：Producer等待Leader所在的Broker的acks，当Leader所在Broker接收到消息保存完消息后才会将ack值返回给Producer。在Leader的消息落盘后还没等Follower同步数据其所在Broker宕机后，就会导致数据丢失。（默认）</li>\n<li>-1（all）：Producer等待Leader所在的Broker的acks，当Leader和ISR中的Follower全部将消息落盘之后所在的Broker才会将ack的值返回给Producer。但是如果Follower将消息同步完以后，Leader所在的Broker还没将ack的值返回给Producer之前，Leader所在的Broker发生故障，会造成数据重复</li>\n</ul>\n<h3 id=\"幂等性\"><a href=\"#幂等性\" class=\"headerlink\" title=\"幂等性\"></a>幂等性</h3><p>设置：enable.idempotence=true<br>解决单次会话单个分区数据重复问题<br>开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带SequenceNumber，而Broker会对&lt;PID，Partition，SequenceNumber&gt;做缓存，当具有相同主键的消息提及时，Broker只会持久化一条。</p>\n<h2 id=\"Consumer（消费者）\"><a href=\"#Consumer（消费者）\" class=\"headerlink\" title=\"Consumer（消费者）\"></a>Consumer（消费者）</h2><p>当前消费者需要提交的消费位移是offset+1</p>\n<h3 id=\"命令-1\"><a href=\"#命令-1\" class=\"headerlink\" title=\"命令\"></a>命令</h3><h4 id=\"使用脚本-1\"><a href=\"#使用脚本-1\" class=\"headerlink\" title=\"使用脚本\"></a>使用脚本</h4><p>kafka-console-consumer.sh    </p>\n<h4 id=\"常用参数-1\"><a href=\"#常用参数-1\" class=\"headerlink\" title=\"常用参数\"></a>常用参数</h4><ul>\n<li>–bootstrap-server    //目标服务器地址</li>\n<li>–topic    //消费主题</li>\n</ul>\n<h4 id=\"举例-1\"><a href=\"#举例-1\" class=\"headerlink\" title=\"举例\"></a>举例</h4><p>kafka-console-consumer.sh    –bootstrap-server    localhost:9092    –topic    test</p>\n<h3 id=\"分配策略\"><a href=\"#分配策略\" class=\"headerlink\" title=\"分配策略\"></a>分配策略</h3><ul>\n<li>RoundRobin    将订阅的所有主题看作一个整体。（按组分配）</li>\n<li>Range    将订阅的单个主题看作整体。（按主题分配）</li>\n</ul>\n<h2 id=\"Topic（主题）\"><a href=\"#Topic（主题）\" class=\"headerlink\" title=\"Topic（主题）\"></a>Topic（主题）</h2><h3 id=\"命令-2\"><a href=\"#命令-2\" class=\"headerlink\" title=\"命令\"></a>命令</h3><h4 id=\"使用脚本-2\"><a href=\"#使用脚本-2\" class=\"headerlink\" title=\"使用脚本\"></a>使用脚本</h4><p>kafka-configs.sh    //0.9中已经废弃kafka-topics.sh脚本，改用kafka-configs.sh来配置主题</p>\n<h4 id=\"常用参数-2\"><a href=\"#常用参数-2\" class=\"headerlink\" title=\"常用参数\"></a>常用参数</h4><ul>\n<li>–zookeeper    //zookeeper地址，多个用逗号链接</li>\n<li>–create    //创建命令</li>\n<li>–list    //列出所有主题</li>\n<li>–alter    //修改主题</li>\n<li>–delete    //删除主题</li>\n<li>–topic    //主题</li>\n<li>–partitions    //主题分区数，分区数要小于等于broker数</li>\n<li>–replication-fator    //主题副本数</li>\n</ul>\n<h4 id=\"举例-2\"><a href=\"#举例-2\" class=\"headerlink\" title=\"举例\"></a>举例</h4><ul>\n<li>创建主题：kafka-configs.sh    –zookeeper    localhost:9092    –create    –topic    test    –partition    2    –replication-fator    3</li>\n<li>列出所有主题：kafka-configs.sh    –zookeeper    localhost:9092    –list</li>\n<li>列出主题详情：kafka-configs.sh    –zookeeper    localhost:9092    –describe    –topic test</li>\n<li>修改主题：kafka-configs.sh    –zookeeper    localhost:9092    –alter    –topic test</li>\n<li>删除主题：kafka-configs.sh    –zookeeper    localhost:9092    –delete    –topic test</li>\n</ul>\n<h2 id=\"Kafka高读写\"><a href=\"#Kafka高读写\" class=\"headerlink\" title=\"Kafka高读写\"></a>Kafka高读写</h2><ul>\n<li>顺序读写</li>\n<li>零复制<br>  <img src=\"./img/%E6%99%AE%E9%80%9A%E6%8B%B7%E8%B4%9D.png\" alt=\"普通拷贝\"><br>  <img src=\"./img/%E9%9B%B6%E6%8B%B7%E8%B4%9D.png\" alt=\"零拷贝 \"></li>\n</ul>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"Kafka\"><a href=\"#Kafka\" class=\"headerlink\" title=\"Kafka\"></a>Kafka</h1><p>一个分区内的数据才能保证幂等性和有序性</p>\n<h2 id=\"架构\"><a href=\"#架构\" class=\"headerlink\" title=\"架构\"></a>架构</h2><p><img src=\"img/kafka%E6%9E%B6%E6%9E%84.png\" alt=\"架构图\" title=\"dd\"></p>\n<h2 id=\"名次解释\"><a href=\"#名次解释\" class=\"headerlink\" title=\"名次解释\"></a>名次解释</h2><ul>\n<li>Broker：Kafka服务器</li>\n<li>Producer：生产者，生产消息</li>\n<li>Consumer：消费者，消费数据</li>\n<li>Consumer Group：消费者组，某一个分区只能被同一个消费者组内的一个消费者消费</li>\n<li>Topic：消息主题。逻辑概念</li>\n<li>Partition：消息分区。消息物理存储上概念</li>\n<li>Offset：偏移量</li>\n<li>Replia：副本，同一个分区（Partition）内的副本分Leader和Follower</li>\n<li>Leader：主副本。对外提供服务</li>\n<li>Follower：从副本。做数据同步工作</li>\n<li>acks：acknowledgments，消息接收后的确认值</li>\n<li>AR（Assigned Replicas）：分区中所有副本</li>\n<li>ISR（In-Sync Replicas）：所有与Leader 部分保持一定程度的副本（包含Leader副本在内）组成ISR</li>\n<li>OSR（Out-Sync Replicas）：与Leader副本同步滞后过多的副本</li>\n<li>HW（High Watermark）：指消费者能够见到的最大Offset，ISR队列中最小的LEO。高水位，标志一个特定的Offset，消费者只能拉取到这个Offset之前的消息，Consumer可见数据的最大偏移量。保证副本数据一致性</li>\n<li>LEO（Log End Offset）：每个副本最后一个Offset。即日志末端位移，记录了该副本底层日志中下一条消息的位移值。注意是下一条消息，也就是说如果LEO=10，那么表示该副本保存了10条消息，位移值范围是[0,9]。<blockquote>\n<p>1 Topic = n Partition<br>1 Partition = n Replica</p>\n</blockquote>\n</li>\n</ul>\n<h2 id=\"Producer（生产者）\"><a href=\"#Producer（生产者）\" class=\"headerlink\" title=\"Producer（生产者）\"></a>Producer（生产者）</h2><p>拦截器-&gt;序列化器-&gt;分区器</p>\n<h3 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h3><h4 id=\"使用脚本\"><a href=\"#使用脚本\" class=\"headerlink\" title=\"使用脚本\"></a>使用脚本</h4><p>kafka-console-producer.sh</p>\n<h4 id=\"常用参数\"><a href=\"#常用参数\" class=\"headerlink\" title=\"常用参数\"></a>常用参数</h4><ul>\n<li>–broker-list    //kafka集群地址</li>\n<li>–topic    //生产主题</li>\n</ul>\n<h4 id=\"举例\"><a href=\"#举例\" class=\"headerlink\" title=\"举例\"></a>举例</h4><p>kafka-console-producer.sh    –broker-list    localhost:9092    –topic    test</p>\n<h3 id=\"分区策略\"><a href=\"#分区策略\" class=\"headerlink\" title=\"分区策略\"></a>分区策略</h3><ol>\n<li><p>指定分区（Partition）：直接按照分区号</p>\n</li>\n<li><p>没有分区（Partition）但是有Key：（默认）根据Key的Hash值获取分区号</p>\n</li>\n<li><p>即没有分区（Partition）也没有Key：第一次险随机获取一个分区号，然后在这个分区号的基础上轮训所有分区号。</p>\n<blockquote>\n<p>如果有3个分区，0、1、2当随机获取的分区号为1时，然后1、2、0、1、2、0、1 … 这样轮询下去</p>\n</blockquote>\n</li>\n</ol>\n<h3 id=\"发送返回值\"><a href=\"#发送返回值\" class=\"headerlink\" title=\"发送返回值\"></a>发送返回值</h3><p>ACKS值：</p>\n<ul>\n<li>0：Producer不等待Leader所在的Broker的acks，这一操作保证了最低延迟，Leader所在的Broker一接到消息不等消息保存就会先将ack的值返回给Producer。此值下当Leader所在的Broker宕机时会造成数据丢失</li>\n<li>1：Producer等待Leader所在的Broker的acks，当Leader所在Broker接收到消息保存完消息后才会将ack值返回给Producer。在Leader的消息落盘后还没等Follower同步数据其所在Broker宕机后，就会导致数据丢失。（默认）</li>\n<li>-1（all）：Producer等待Leader所在的Broker的acks，当Leader和ISR中的Follower全部将消息落盘之后所在的Broker才会将ack的值返回给Producer。但是如果Follower将消息同步完以后，Leader所在的Broker还没将ack的值返回给Producer之前，Leader所在的Broker发生故障，会造成数据重复</li>\n</ul>\n<h3 id=\"幂等性\"><a href=\"#幂等性\" class=\"headerlink\" title=\"幂等性\"></a>幂等性</h3><p>设置：enable.idempotence=true<br>解决单次会话单个分区数据重复问题<br>开启幂等性的Producer在初始化的时候会被分配一个PID，发往同一Partition的消息会附带SequenceNumber，而Broker会对&lt;PID，Partition，SequenceNumber&gt;做缓存，当具有相同主键的消息提及时，Broker只会持久化一条。</p>\n<h2 id=\"Consumer（消费者）\"><a href=\"#Consumer（消费者）\" class=\"headerlink\" title=\"Consumer（消费者）\"></a>Consumer（消费者）</h2><p>当前消费者需要提交的消费位移是offset+1</p>\n<h3 id=\"命令-1\"><a href=\"#命令-1\" class=\"headerlink\" title=\"命令\"></a>命令</h3><h4 id=\"使用脚本-1\"><a href=\"#使用脚本-1\" class=\"headerlink\" title=\"使用脚本\"></a>使用脚本</h4><p>kafka-console-consumer.sh    </p>\n<h4 id=\"常用参数-1\"><a href=\"#常用参数-1\" class=\"headerlink\" title=\"常用参数\"></a>常用参数</h4><ul>\n<li>–bootstrap-server    //目标服务器地址</li>\n<li>–topic    //消费主题</li>\n</ul>\n<h4 id=\"举例-1\"><a href=\"#举例-1\" class=\"headerlink\" title=\"举例\"></a>举例</h4><p>kafka-console-consumer.sh    –bootstrap-server    localhost:9092    –topic    test</p>\n<h3 id=\"分配策略\"><a href=\"#分配策略\" class=\"headerlink\" title=\"分配策略\"></a>分配策略</h3><ul>\n<li>RoundRobin    将订阅的所有主题看作一个整体。（按组分配）</li>\n<li>Range    将订阅的单个主题看作整体。（按主题分配）</li>\n</ul>\n<h2 id=\"Topic（主题）\"><a href=\"#Topic（主题）\" class=\"headerlink\" title=\"Topic（主题）\"></a>Topic（主题）</h2><h3 id=\"命令-2\"><a href=\"#命令-2\" class=\"headerlink\" title=\"命令\"></a>命令</h3><h4 id=\"使用脚本-2\"><a href=\"#使用脚本-2\" class=\"headerlink\" title=\"使用脚本\"></a>使用脚本</h4><p>kafka-configs.sh    //0.9中已经废弃kafka-topics.sh脚本，改用kafka-configs.sh来配置主题</p>\n<h4 id=\"常用参数-2\"><a href=\"#常用参数-2\" class=\"headerlink\" title=\"常用参数\"></a>常用参数</h4><ul>\n<li>–zookeeper    //zookeeper地址，多个用逗号链接</li>\n<li>–create    //创建命令</li>\n<li>–list    //列出所有主题</li>\n<li>–alter    //修改主题</li>\n<li>–delete    //删除主题</li>\n<li>–topic    //主题</li>\n<li>–partitions    //主题分区数，分区数要小于等于broker数</li>\n<li>–replication-fator    //主题副本数</li>\n</ul>\n<h4 id=\"举例-2\"><a href=\"#举例-2\" class=\"headerlink\" title=\"举例\"></a>举例</h4><ul>\n<li>创建主题：kafka-configs.sh    –zookeeper    localhost:9092    –create    –topic    test    –partition    2    –replication-fator    3</li>\n<li>列出所有主题：kafka-configs.sh    –zookeeper    localhost:9092    –list</li>\n<li>列出主题详情：kafka-configs.sh    –zookeeper    localhost:9092    –describe    –topic test</li>\n<li>修改主题：kafka-configs.sh    –zookeeper    localhost:9092    –alter    –topic test</li>\n<li>删除主题：kafka-configs.sh    –zookeeper    localhost:9092    –delete    –topic test</li>\n</ul>\n<h2 id=\"Kafka高读写\"><a href=\"#Kafka高读写\" class=\"headerlink\" title=\"Kafka高读写\"></a>Kafka高读写</h2><ul>\n<li>顺序读写</li>\n<li>零复制<br>  <img src=\"./img/%E6%99%AE%E9%80%9A%E6%8B%B7%E8%B4%9D.png\" alt=\"普通拷贝\"><br>  <img src=\"./img/%E9%9B%B6%E6%8B%B7%E8%B4%9D.png\" alt=\"零拷贝 \"></li>\n</ul>\n"},{"_content":"\n\n#Linux\n\n### 1. 挂载\n\n~~~\nmount -t iso9660 -o ro /dev/cdrom /mnt/cdrom  //将文件类型为iso9660的文件 以只读（ro）方式从 /dev/cdrom 挂载到/mnt/cdrom\n~~~\n\n### 2. 设置开机自动挂载\n~~~\nvi /etc/fstab\n/dev/cdrom\t/mnt/cdrom\tiso9660\tdefaults\t0\t0\n~~~\n\n### 3. 关闭防火墙\n~~~\nchkconfig iptables --list\nchkconfig iptables  off   //重启时也自动关闭\n~~~\n### 4. 设置yum本地源\n~~~\ncd /etc/yum.repos.d/\n修改baseurl=file:///或http://\n~~~\n\n### 5. 将自己的包配置成yum库\n~~~\n进入到repo目录\n执行命令：createrepo  .  \n~~~\n\n### 6. rename 批量重命名\n~~~\nrename .a .b *.a   //将所有.a结尾的文件重命名成.b结尾的文件\n~~~\n\n### 7. 服务\n~~~\nservice httpd status  //查看服务状态\nservice httpd start   //开启服务\nservice httpd restart //重启服务\nservice httpd stop  //关闭服务\n~~~\n\n### 8. 关闭SELinux\n~~~\nvi /etc/selinux/config\n修改SE Linux=disabled\n~~~\n\n\n### 9. 添加字符串到文件\n~~~\n“>” 将一条命令重定向到一个文件，会覆盖原文件；\n“>>” 将一条命令追加到一个文件，不会覆盖，在文件末尾添加；\n~~~\n\n### 10. vi快捷键\n\n#### 一般模式下：\n~~~\ny\t复制，3yy 复制三行；\nd\t删除， 5dd 删除附近五行；\n~~~\n#### 命令行模式：\n~~~\n%s／abc／efg\t字符串替换。将所有的abc替换成efg\n／abc\t查找字符串abc ，按n查找下一个，N查找上一个；\n~~~\n### 11. 修改文件权限\n~~~\nchmod\tu+／-r\t文件\t\t给文件拥有者添加／减少 可读权限（u为拥有者，g为用户组，o其他用户）\nchmod\t777\t\t文件\t\t修改文件为所有人可读可写可执行（r：4，w：2，x：1）\nchmod\t-r\t把文件夹及其下子文件修改为一致；\nchown 用户 目录或文件名\t修改文件所属用户\n~~~\n### 12. 用户管理\n\n#### 添加用户：\n~~~\nuseradd\t用户名\t添加用户\npasswd\t用户名\t修改用户密码\n~~~\n### 13. 查看大小\n\n#### 查看文件夹：\n~~~\ndu\t-sh\t文件夹\n~~~\n#### 查看分区：\n~~~\ndf\t-h\n~~~\n\n### 14. 跟踪日志文件\n~~~\ntail -10\t文件\t\t跟踪显示后10行\ntail\t-f\t文件\t\t实时跟踪显示文件（只跟踪文件indo号）\ntail\t-F\t文件\t\t实时跟踪显示文件 （跟踪文件的名称）\n~~~\n### 15. cut\n~~~\ncut\t-d\t‘ ：’\t-f\t1\t截取以 ：分割的第一个\n~~~\n### 16. sort\n~~~\nsort\t-t\t' : '\t-k  2nr\t  将用  ：分割的字符串以第二列数字倒序排列\n~~~\n### 17. sed\n~~~\nsed\t‘2d’\tfilename\t删除filename第二行\t不会改变源文件里面的数据加sed -i 会改变源文件里面的数据\nsed\t‘/test/’d \tfilename\t删除匹配test的行\nsed\t‘2，$d’\tfilename\t删除filename2到结束\nsed\t‘s/aa/bb/g’\tfilename\t全局将带有aa的行替换成bb\n~~~\n### 18. awk\n~~~\nawk\t-F\t‘ ：’  ‘ { print $1 \",\" $7 }   将以 ：分割的字符串打印第1和第7列中间用 ，分割\n~~~\n### 19. expor\n~~~\nexport 定义的变量只对本会话和子会话生效（bash），要想使在子对话定义的变量在父会话中生效，要使用source  /etc/profile**1.添加字符串到文件**\n“>” 将一条命令重定向到一个文件，会覆盖原文件；\n“>>” 将一条命令追加到一个文件，不会覆盖，在文件末尾添加；\n~~~\n\n### 20. 安装centos mini版本后应该处理的问题\n\n#### 1. ifconfig后没有eth0 网卡\n~~~\nvi /etc/sysconfig/network-scripts/ifcfg-eth0 \nONBOOT=no 改为 yes\n~~~\n\n#### 2. 安装ssh\n~~~\nyum install openssh-server  安装ssh服务\nyum install opens-clients    安装ssh客户端\n查看ssh是否启动\nnetstat -antp | grep ssh    可以看到22号端口是否启动\n~~~\n\n#### 3. 永久关闭防火墙\n~~~\nchkconfig  iptables off\nservice iptables stop （重启以后不会再生效，不能永久关闭）\n~~~\n\n#### 4. 修改主机名\n~~~\nvi  /etc/sysconfig/network\nhostname=xxxx     xxx改为需要的名称\n~~~\n#### 5. 增加用户并为用户添加密码\n~~~\nadduser   xxx    xxx为用户\npasswd    xxx    xxx为用户\n~~~\n#### 6. 为用户添加权限\n~~~\nvi /etc/sudoers\n找到root all=（all） all\n添加xxx  all=（all） all\n~~~\n#### 7. 当复制一个虚拟机时网卡eth0启动不了\n~~~\nvi /etc/udev/rules.d/70-persistent-net.rules\n将eth0的删除，将eth1的改成eth0\n或者查看eth1的序列号将网卡ifcfg-eth0的序列号改成eth1的\n~~~\n#### 8. 添加几台虚拟机的网址主机名映射\n~~~\nvi /etc/hosts\n添加如192.168.1.101  hadoop01\n~~~\n#### 9. 为几台虚拟机设置免密登录\n\n### 21. Linux下轻量级的集群管理利器ClusterShell","source":"notes/Linux.md","raw":"\n\n#Linux\n\n### 1. 挂载\n\n~~~\nmount -t iso9660 -o ro /dev/cdrom /mnt/cdrom  //将文件类型为iso9660的文件 以只读（ro）方式从 /dev/cdrom 挂载到/mnt/cdrom\n~~~\n\n### 2. 设置开机自动挂载\n~~~\nvi /etc/fstab\n/dev/cdrom\t/mnt/cdrom\tiso9660\tdefaults\t0\t0\n~~~\n\n### 3. 关闭防火墙\n~~~\nchkconfig iptables --list\nchkconfig iptables  off   //重启时也自动关闭\n~~~\n### 4. 设置yum本地源\n~~~\ncd /etc/yum.repos.d/\n修改baseurl=file:///或http://\n~~~\n\n### 5. 将自己的包配置成yum库\n~~~\n进入到repo目录\n执行命令：createrepo  .  \n~~~\n\n### 6. rename 批量重命名\n~~~\nrename .a .b *.a   //将所有.a结尾的文件重命名成.b结尾的文件\n~~~\n\n### 7. 服务\n~~~\nservice httpd status  //查看服务状态\nservice httpd start   //开启服务\nservice httpd restart //重启服务\nservice httpd stop  //关闭服务\n~~~\n\n### 8. 关闭SELinux\n~~~\nvi /etc/selinux/config\n修改SE Linux=disabled\n~~~\n\n\n### 9. 添加字符串到文件\n~~~\n“>” 将一条命令重定向到一个文件，会覆盖原文件；\n“>>” 将一条命令追加到一个文件，不会覆盖，在文件末尾添加；\n~~~\n\n### 10. vi快捷键\n\n#### 一般模式下：\n~~~\ny\t复制，3yy 复制三行；\nd\t删除， 5dd 删除附近五行；\n~~~\n#### 命令行模式：\n~~~\n%s／abc／efg\t字符串替换。将所有的abc替换成efg\n／abc\t查找字符串abc ，按n查找下一个，N查找上一个；\n~~~\n### 11. 修改文件权限\n~~~\nchmod\tu+／-r\t文件\t\t给文件拥有者添加／减少 可读权限（u为拥有者，g为用户组，o其他用户）\nchmod\t777\t\t文件\t\t修改文件为所有人可读可写可执行（r：4，w：2，x：1）\nchmod\t-r\t把文件夹及其下子文件修改为一致；\nchown 用户 目录或文件名\t修改文件所属用户\n~~~\n### 12. 用户管理\n\n#### 添加用户：\n~~~\nuseradd\t用户名\t添加用户\npasswd\t用户名\t修改用户密码\n~~~\n### 13. 查看大小\n\n#### 查看文件夹：\n~~~\ndu\t-sh\t文件夹\n~~~\n#### 查看分区：\n~~~\ndf\t-h\n~~~\n\n### 14. 跟踪日志文件\n~~~\ntail -10\t文件\t\t跟踪显示后10行\ntail\t-f\t文件\t\t实时跟踪显示文件（只跟踪文件indo号）\ntail\t-F\t文件\t\t实时跟踪显示文件 （跟踪文件的名称）\n~~~\n### 15. cut\n~~~\ncut\t-d\t‘ ：’\t-f\t1\t截取以 ：分割的第一个\n~~~\n### 16. sort\n~~~\nsort\t-t\t' : '\t-k  2nr\t  将用  ：分割的字符串以第二列数字倒序排列\n~~~\n### 17. sed\n~~~\nsed\t‘2d’\tfilename\t删除filename第二行\t不会改变源文件里面的数据加sed -i 会改变源文件里面的数据\nsed\t‘/test/’d \tfilename\t删除匹配test的行\nsed\t‘2，$d’\tfilename\t删除filename2到结束\nsed\t‘s/aa/bb/g’\tfilename\t全局将带有aa的行替换成bb\n~~~\n### 18. awk\n~~~\nawk\t-F\t‘ ：’  ‘ { print $1 \",\" $7 }   将以 ：分割的字符串打印第1和第7列中间用 ，分割\n~~~\n### 19. expor\n~~~\nexport 定义的变量只对本会话和子会话生效（bash），要想使在子对话定义的变量在父会话中生效，要使用source  /etc/profile**1.添加字符串到文件**\n“>” 将一条命令重定向到一个文件，会覆盖原文件；\n“>>” 将一条命令追加到一个文件，不会覆盖，在文件末尾添加；\n~~~\n\n### 20. 安装centos mini版本后应该处理的问题\n\n#### 1. ifconfig后没有eth0 网卡\n~~~\nvi /etc/sysconfig/network-scripts/ifcfg-eth0 \nONBOOT=no 改为 yes\n~~~\n\n#### 2. 安装ssh\n~~~\nyum install openssh-server  安装ssh服务\nyum install opens-clients    安装ssh客户端\n查看ssh是否启动\nnetstat -antp | grep ssh    可以看到22号端口是否启动\n~~~\n\n#### 3. 永久关闭防火墙\n~~~\nchkconfig  iptables off\nservice iptables stop （重启以后不会再生效，不能永久关闭）\n~~~\n\n#### 4. 修改主机名\n~~~\nvi  /etc/sysconfig/network\nhostname=xxxx     xxx改为需要的名称\n~~~\n#### 5. 增加用户并为用户添加密码\n~~~\nadduser   xxx    xxx为用户\npasswd    xxx    xxx为用户\n~~~\n#### 6. 为用户添加权限\n~~~\nvi /etc/sudoers\n找到root all=（all） all\n添加xxx  all=（all） all\n~~~\n#### 7. 当复制一个虚拟机时网卡eth0启动不了\n~~~\nvi /etc/udev/rules.d/70-persistent-net.rules\n将eth0的删除，将eth1的改成eth0\n或者查看eth1的序列号将网卡ifcfg-eth0的序列号改成eth1的\n~~~\n#### 8. 添加几台虚拟机的网址主机名映射\n~~~\nvi /etc/hosts\n添加如192.168.1.101  hadoop01\n~~~\n#### 9. 为几台虚拟机设置免密登录\n\n### 21. Linux下轻量级的集群管理利器ClusterShell","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T11:52:16.000Z","path":"notes/Linux.html","title":"","comments":1,"layout":"page","_id":"ck7ycejgo000hjr64wdax4mbc","content":"<p>#Linux</p>\n<h3 id=\"1-挂载\"><a href=\"#1-挂载\" class=\"headerlink\" title=\"1. 挂载\"></a>1. 挂载</h3><pre><code>mount -t iso9660 -o ro /dev/cdrom /mnt/cdrom  //将文件类型为iso9660的文件 以只读（ro）方式从 /dev/cdrom 挂载到/mnt/cdrom</code></pre><h3 id=\"2-设置开机自动挂载\"><a href=\"#2-设置开机自动挂载\" class=\"headerlink\" title=\"2. 设置开机自动挂载\"></a>2. 设置开机自动挂载</h3><pre><code>vi /etc/fstab\n/dev/cdrom    /mnt/cdrom    iso9660    defaults    0    0</code></pre><h3 id=\"3-关闭防火墙\"><a href=\"#3-关闭防火墙\" class=\"headerlink\" title=\"3. 关闭防火墙\"></a>3. 关闭防火墙</h3><pre><code>chkconfig iptables --list\nchkconfig iptables  off   //重启时也自动关闭</code></pre><h3 id=\"4-设置yum本地源\"><a href=\"#4-设置yum本地源\" class=\"headerlink\" title=\"4. 设置yum本地源\"></a>4. 设置yum本地源</h3><pre><code>cd /etc/yum.repos.d/\n修改baseurl=file:///或http://</code></pre><h3 id=\"5-将自己的包配置成yum库\"><a href=\"#5-将自己的包配置成yum库\" class=\"headerlink\" title=\"5. 将自己的包配置成yum库\"></a>5. 将自己的包配置成yum库</h3><pre><code>进入到repo目录\n执行命令：createrepo  .  </code></pre><h3 id=\"6-rename-批量重命名\"><a href=\"#6-rename-批量重命名\" class=\"headerlink\" title=\"6. rename 批量重命名\"></a>6. rename 批量重命名</h3><pre><code>rename .a .b *.a   //将所有.a结尾的文件重命名成.b结尾的文件</code></pre><h3 id=\"7-服务\"><a href=\"#7-服务\" class=\"headerlink\" title=\"7. 服务\"></a>7. 服务</h3><pre><code>service httpd status  //查看服务状态\nservice httpd start   //开启服务\nservice httpd restart //重启服务\nservice httpd stop  //关闭服务</code></pre><h3 id=\"8-关闭SELinux\"><a href=\"#8-关闭SELinux\" class=\"headerlink\" title=\"8. 关闭SELinux\"></a>8. 关闭SELinux</h3><pre><code>vi /etc/selinux/config\n修改SE Linux=disabled</code></pre><h3 id=\"9-添加字符串到文件\"><a href=\"#9-添加字符串到文件\" class=\"headerlink\" title=\"9. 添加字符串到文件\"></a>9. 添加字符串到文件</h3><pre><code>“&gt;” 将一条命令重定向到一个文件，会覆盖原文件；\n“&gt;&gt;” 将一条命令追加到一个文件，不会覆盖，在文件末尾添加；</code></pre><h3 id=\"10-vi快捷键\"><a href=\"#10-vi快捷键\" class=\"headerlink\" title=\"10. vi快捷键\"></a>10. vi快捷键</h3><h4 id=\"一般模式下：\"><a href=\"#一般模式下：\" class=\"headerlink\" title=\"一般模式下：\"></a>一般模式下：</h4><pre><code>y    复制，3yy 复制三行；\nd    删除， 5dd 删除附近五行；</code></pre><h4 id=\"命令行模式：\"><a href=\"#命令行模式：\" class=\"headerlink\" title=\"命令行模式：\"></a>命令行模式：</h4><pre><code>%s／abc／efg    字符串替换。将所有的abc替换成efg\n／abc    查找字符串abc ，按n查找下一个，N查找上一个；</code></pre><h3 id=\"11-修改文件权限\"><a href=\"#11-修改文件权限\" class=\"headerlink\" title=\"11. 修改文件权限\"></a>11. 修改文件权限</h3><pre><code>chmod    u+／-r    文件        给文件拥有者添加／减少 可读权限（u为拥有者，g为用户组，o其他用户）\nchmod    777        文件        修改文件为所有人可读可写可执行（r：4，w：2，x：1）\nchmod    -r    把文件夹及其下子文件修改为一致；\nchown 用户 目录或文件名    修改文件所属用户</code></pre><h3 id=\"12-用户管理\"><a href=\"#12-用户管理\" class=\"headerlink\" title=\"12. 用户管理\"></a>12. 用户管理</h3><h4 id=\"添加用户：\"><a href=\"#添加用户：\" class=\"headerlink\" title=\"添加用户：\"></a>添加用户：</h4><pre><code>useradd    用户名    添加用户\npasswd    用户名    修改用户密码</code></pre><h3 id=\"13-查看大小\"><a href=\"#13-查看大小\" class=\"headerlink\" title=\"13. 查看大小\"></a>13. 查看大小</h3><h4 id=\"查看文件夹：\"><a href=\"#查看文件夹：\" class=\"headerlink\" title=\"查看文件夹：\"></a>查看文件夹：</h4><pre><code>du    -sh    文件夹</code></pre><h4 id=\"查看分区：\"><a href=\"#查看分区：\" class=\"headerlink\" title=\"查看分区：\"></a>查看分区：</h4><pre><code>df    -h</code></pre><h3 id=\"14-跟踪日志文件\"><a href=\"#14-跟踪日志文件\" class=\"headerlink\" title=\"14. 跟踪日志文件\"></a>14. 跟踪日志文件</h3><pre><code>tail -10    文件        跟踪显示后10行\ntail    -f    文件        实时跟踪显示文件（只跟踪文件indo号）\ntail    -F    文件        实时跟踪显示文件 （跟踪文件的名称）</code></pre><h3 id=\"15-cut\"><a href=\"#15-cut\" class=\"headerlink\" title=\"15. cut\"></a>15. cut</h3><pre><code>cut    -d    ‘ ：’    -f    1    截取以 ：分割的第一个</code></pre><h3 id=\"16-sort\"><a href=\"#16-sort\" class=\"headerlink\" title=\"16. sort\"></a>16. sort</h3><pre><code>sort    -t    &#39; : &#39;    -k  2nr      将用  ：分割的字符串以第二列数字倒序排列</code></pre><h3 id=\"17-sed\"><a href=\"#17-sed\" class=\"headerlink\" title=\"17. sed\"></a>17. sed</h3><pre><code>sed    ‘2d’    filename    删除filename第二行    不会改变源文件里面的数据加sed -i 会改变源文件里面的数据\nsed    ‘/test/’d     filename    删除匹配test的行\nsed    ‘2，$d’    filename    删除filename2到结束\nsed    ‘s/aa/bb/g’    filename    全局将带有aa的行替换成bb</code></pre><h3 id=\"18-awk\"><a href=\"#18-awk\" class=\"headerlink\" title=\"18. awk\"></a>18. awk</h3><pre><code>awk    -F    ‘ ：’  ‘ { print $1 &quot;,&quot; $7 }   将以 ：分割的字符串打印第1和第7列中间用 ，分割</code></pre><h3 id=\"19-expor\"><a href=\"#19-expor\" class=\"headerlink\" title=\"19. expor\"></a>19. expor</h3><pre><code>export 定义的变量只对本会话和子会话生效（bash），要想使在子对话定义的变量在父会话中生效，要使用source  /etc/profile**1.添加字符串到文件**\n“&gt;” 将一条命令重定向到一个文件，会覆盖原文件；\n“&gt;&gt;” 将一条命令追加到一个文件，不会覆盖，在文件末尾添加；</code></pre><h3 id=\"20-安装centos-mini版本后应该处理的问题\"><a href=\"#20-安装centos-mini版本后应该处理的问题\" class=\"headerlink\" title=\"20. 安装centos mini版本后应该处理的问题\"></a>20. 安装centos mini版本后应该处理的问题</h3><h4 id=\"1-ifconfig后没有eth0-网卡\"><a href=\"#1-ifconfig后没有eth0-网卡\" class=\"headerlink\" title=\"1. ifconfig后没有eth0 网卡\"></a>1. ifconfig后没有eth0 网卡</h4><pre><code>vi /etc/sysconfig/network-scripts/ifcfg-eth0 \nONBOOT=no 改为 yes</code></pre><h4 id=\"2-安装ssh\"><a href=\"#2-安装ssh\" class=\"headerlink\" title=\"2. 安装ssh\"></a>2. 安装ssh</h4><pre><code>yum install openssh-server  安装ssh服务\nyum install opens-clients    安装ssh客户端\n查看ssh是否启动\nnetstat -antp | grep ssh    可以看到22号端口是否启动</code></pre><h4 id=\"3-永久关闭防火墙\"><a href=\"#3-永久关闭防火墙\" class=\"headerlink\" title=\"3. 永久关闭防火墙\"></a>3. 永久关闭防火墙</h4><pre><code>chkconfig  iptables off\nservice iptables stop （重启以后不会再生效，不能永久关闭）</code></pre><h4 id=\"4-修改主机名\"><a href=\"#4-修改主机名\" class=\"headerlink\" title=\"4. 修改主机名\"></a>4. 修改主机名</h4><pre><code>vi  /etc/sysconfig/network\nhostname=xxxx     xxx改为需要的名称</code></pre><h4 id=\"5-增加用户并为用户添加密码\"><a href=\"#5-增加用户并为用户添加密码\" class=\"headerlink\" title=\"5. 增加用户并为用户添加密码\"></a>5. 增加用户并为用户添加密码</h4><pre><code>adduser   xxx    xxx为用户\npasswd    xxx    xxx为用户</code></pre><h4 id=\"6-为用户添加权限\"><a href=\"#6-为用户添加权限\" class=\"headerlink\" title=\"6. 为用户添加权限\"></a>6. 为用户添加权限</h4><pre><code>vi /etc/sudoers\n找到root all=（all） all\n添加xxx  all=（all） all</code></pre><h4 id=\"7-当复制一个虚拟机时网卡eth0启动不了\"><a href=\"#7-当复制一个虚拟机时网卡eth0启动不了\" class=\"headerlink\" title=\"7. 当复制一个虚拟机时网卡eth0启动不了\"></a>7. 当复制一个虚拟机时网卡eth0启动不了</h4><pre><code>vi /etc/udev/rules.d/70-persistent-net.rules\n将eth0的删除，将eth1的改成eth0\n或者查看eth1的序列号将网卡ifcfg-eth0的序列号改成eth1的</code></pre><h4 id=\"8-添加几台虚拟机的网址主机名映射\"><a href=\"#8-添加几台虚拟机的网址主机名映射\" class=\"headerlink\" title=\"8. 添加几台虚拟机的网址主机名映射\"></a>8. 添加几台虚拟机的网址主机名映射</h4><pre><code>vi /etc/hosts\n添加如192.168.1.101  hadoop01</code></pre><h4 id=\"9-为几台虚拟机设置免密登录\"><a href=\"#9-为几台虚拟机设置免密登录\" class=\"headerlink\" title=\"9. 为几台虚拟机设置免密登录\"></a>9. 为几台虚拟机设置免密登录</h4><h3 id=\"21-Linux下轻量级的集群管理利器ClusterShell\"><a href=\"#21-Linux下轻量级的集群管理利器ClusterShell\" class=\"headerlink\" title=\"21. Linux下轻量级的集群管理利器ClusterShell\"></a>21. Linux下轻量级的集群管理利器ClusterShell</h3>","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<p>#Linux</p>\n<h3 id=\"1-挂载\"><a href=\"#1-挂载\" class=\"headerlink\" title=\"1. 挂载\"></a>1. 挂载</h3><pre><code>mount -t iso9660 -o ro /dev/cdrom /mnt/cdrom  //将文件类型为iso9660的文件 以只读（ro）方式从 /dev/cdrom 挂载到/mnt/cdrom</code></pre><h3 id=\"2-设置开机自动挂载\"><a href=\"#2-设置开机自动挂载\" class=\"headerlink\" title=\"2. 设置开机自动挂载\"></a>2. 设置开机自动挂载</h3><pre><code>vi /etc/fstab\n/dev/cdrom    /mnt/cdrom    iso9660    defaults    0    0</code></pre><h3 id=\"3-关闭防火墙\"><a href=\"#3-关闭防火墙\" class=\"headerlink\" title=\"3. 关闭防火墙\"></a>3. 关闭防火墙</h3><pre><code>chkconfig iptables --list\nchkconfig iptables  off   //重启时也自动关闭</code></pre><h3 id=\"4-设置yum本地源\"><a href=\"#4-设置yum本地源\" class=\"headerlink\" title=\"4. 设置yum本地源\"></a>4. 设置yum本地源</h3><pre><code>cd /etc/yum.repos.d/\n修改baseurl=file:///或http://</code></pre><h3 id=\"5-将自己的包配置成yum库\"><a href=\"#5-将自己的包配置成yum库\" class=\"headerlink\" title=\"5. 将自己的包配置成yum库\"></a>5. 将自己的包配置成yum库</h3><pre><code>进入到repo目录\n执行命令：createrepo  .  </code></pre><h3 id=\"6-rename-批量重命名\"><a href=\"#6-rename-批量重命名\" class=\"headerlink\" title=\"6. rename 批量重命名\"></a>6. rename 批量重命名</h3><pre><code>rename .a .b *.a   //将所有.a结尾的文件重命名成.b结尾的文件</code></pre><h3 id=\"7-服务\"><a href=\"#7-服务\" class=\"headerlink\" title=\"7. 服务\"></a>7. 服务</h3><pre><code>service httpd status  //查看服务状态\nservice httpd start   //开启服务\nservice httpd restart //重启服务\nservice httpd stop  //关闭服务</code></pre><h3 id=\"8-关闭SELinux\"><a href=\"#8-关闭SELinux\" class=\"headerlink\" title=\"8. 关闭SELinux\"></a>8. 关闭SELinux</h3><pre><code>vi /etc/selinux/config\n修改SE Linux=disabled</code></pre><h3 id=\"9-添加字符串到文件\"><a href=\"#9-添加字符串到文件\" class=\"headerlink\" title=\"9. 添加字符串到文件\"></a>9. 添加字符串到文件</h3><pre><code>“&gt;” 将一条命令重定向到一个文件，会覆盖原文件；\n“&gt;&gt;” 将一条命令追加到一个文件，不会覆盖，在文件末尾添加；</code></pre><h3 id=\"10-vi快捷键\"><a href=\"#10-vi快捷键\" class=\"headerlink\" title=\"10. vi快捷键\"></a>10. vi快捷键</h3><h4 id=\"一般模式下：\"><a href=\"#一般模式下：\" class=\"headerlink\" title=\"一般模式下：\"></a>一般模式下：</h4><pre><code>y    复制，3yy 复制三行；\nd    删除， 5dd 删除附近五行；</code></pre><h4 id=\"命令行模式：\"><a href=\"#命令行模式：\" class=\"headerlink\" title=\"命令行模式：\"></a>命令行模式：</h4><pre><code>%s／abc／efg    字符串替换。将所有的abc替换成efg\n／abc    查找字符串abc ，按n查找下一个，N查找上一个；</code></pre><h3 id=\"11-修改文件权限\"><a href=\"#11-修改文件权限\" class=\"headerlink\" title=\"11. 修改文件权限\"></a>11. 修改文件权限</h3><pre><code>chmod    u+／-r    文件        给文件拥有者添加／减少 可读权限（u为拥有者，g为用户组，o其他用户）\nchmod    777        文件        修改文件为所有人可读可写可执行（r：4，w：2，x：1）\nchmod    -r    把文件夹及其下子文件修改为一致；\nchown 用户 目录或文件名    修改文件所属用户</code></pre><h3 id=\"12-用户管理\"><a href=\"#12-用户管理\" class=\"headerlink\" title=\"12. 用户管理\"></a>12. 用户管理</h3><h4 id=\"添加用户：\"><a href=\"#添加用户：\" class=\"headerlink\" title=\"添加用户：\"></a>添加用户：</h4><pre><code>useradd    用户名    添加用户\npasswd    用户名    修改用户密码</code></pre><h3 id=\"13-查看大小\"><a href=\"#13-查看大小\" class=\"headerlink\" title=\"13. 查看大小\"></a>13. 查看大小</h3><h4 id=\"查看文件夹：\"><a href=\"#查看文件夹：\" class=\"headerlink\" title=\"查看文件夹：\"></a>查看文件夹：</h4><pre><code>du    -sh    文件夹</code></pre><h4 id=\"查看分区：\"><a href=\"#查看分区：\" class=\"headerlink\" title=\"查看分区：\"></a>查看分区：</h4><pre><code>df    -h</code></pre><h3 id=\"14-跟踪日志文件\"><a href=\"#14-跟踪日志文件\" class=\"headerlink\" title=\"14. 跟踪日志文件\"></a>14. 跟踪日志文件</h3><pre><code>tail -10    文件        跟踪显示后10行\ntail    -f    文件        实时跟踪显示文件（只跟踪文件indo号）\ntail    -F    文件        实时跟踪显示文件 （跟踪文件的名称）</code></pre><h3 id=\"15-cut\"><a href=\"#15-cut\" class=\"headerlink\" title=\"15. cut\"></a>15. cut</h3><pre><code>cut    -d    ‘ ：’    -f    1    截取以 ：分割的第一个</code></pre><h3 id=\"16-sort\"><a href=\"#16-sort\" class=\"headerlink\" title=\"16. sort\"></a>16. sort</h3><pre><code>sort    -t    &#39; : &#39;    -k  2nr      将用  ：分割的字符串以第二列数字倒序排列</code></pre><h3 id=\"17-sed\"><a href=\"#17-sed\" class=\"headerlink\" title=\"17. sed\"></a>17. sed</h3><pre><code>sed    ‘2d’    filename    删除filename第二行    不会改变源文件里面的数据加sed -i 会改变源文件里面的数据\nsed    ‘/test/’d     filename    删除匹配test的行\nsed    ‘2，$d’    filename    删除filename2到结束\nsed    ‘s/aa/bb/g’    filename    全局将带有aa的行替换成bb</code></pre><h3 id=\"18-awk\"><a href=\"#18-awk\" class=\"headerlink\" title=\"18. awk\"></a>18. awk</h3><pre><code>awk    -F    ‘ ：’  ‘ { print $1 &quot;,&quot; $7 }   将以 ：分割的字符串打印第1和第7列中间用 ，分割</code></pre><h3 id=\"19-expor\"><a href=\"#19-expor\" class=\"headerlink\" title=\"19. expor\"></a>19. expor</h3><pre><code>export 定义的变量只对本会话和子会话生效（bash），要想使在子对话定义的变量在父会话中生效，要使用source  /etc/profile**1.添加字符串到文件**\n“&gt;” 将一条命令重定向到一个文件，会覆盖原文件；\n“&gt;&gt;” 将一条命令追加到一个文件，不会覆盖，在文件末尾添加；</code></pre><h3 id=\"20-安装centos-mini版本后应该处理的问题\"><a href=\"#20-安装centos-mini版本后应该处理的问题\" class=\"headerlink\" title=\"20. 安装centos mini版本后应该处理的问题\"></a>20. 安装centos mini版本后应该处理的问题</h3><h4 id=\"1-ifconfig后没有eth0-网卡\"><a href=\"#1-ifconfig后没有eth0-网卡\" class=\"headerlink\" title=\"1. ifconfig后没有eth0 网卡\"></a>1. ifconfig后没有eth0 网卡</h4><pre><code>vi /etc/sysconfig/network-scripts/ifcfg-eth0 \nONBOOT=no 改为 yes</code></pre><h4 id=\"2-安装ssh\"><a href=\"#2-安装ssh\" class=\"headerlink\" title=\"2. 安装ssh\"></a>2. 安装ssh</h4><pre><code>yum install openssh-server  安装ssh服务\nyum install opens-clients    安装ssh客户端\n查看ssh是否启动\nnetstat -antp | grep ssh    可以看到22号端口是否启动</code></pre><h4 id=\"3-永久关闭防火墙\"><a href=\"#3-永久关闭防火墙\" class=\"headerlink\" title=\"3. 永久关闭防火墙\"></a>3. 永久关闭防火墙</h4><pre><code>chkconfig  iptables off\nservice iptables stop （重启以后不会再生效，不能永久关闭）</code></pre><h4 id=\"4-修改主机名\"><a href=\"#4-修改主机名\" class=\"headerlink\" title=\"4. 修改主机名\"></a>4. 修改主机名</h4><pre><code>vi  /etc/sysconfig/network\nhostname=xxxx     xxx改为需要的名称</code></pre><h4 id=\"5-增加用户并为用户添加密码\"><a href=\"#5-增加用户并为用户添加密码\" class=\"headerlink\" title=\"5. 增加用户并为用户添加密码\"></a>5. 增加用户并为用户添加密码</h4><pre><code>adduser   xxx    xxx为用户\npasswd    xxx    xxx为用户</code></pre><h4 id=\"6-为用户添加权限\"><a href=\"#6-为用户添加权限\" class=\"headerlink\" title=\"6. 为用户添加权限\"></a>6. 为用户添加权限</h4><pre><code>vi /etc/sudoers\n找到root all=（all） all\n添加xxx  all=（all） all</code></pre><h4 id=\"7-当复制一个虚拟机时网卡eth0启动不了\"><a href=\"#7-当复制一个虚拟机时网卡eth0启动不了\" class=\"headerlink\" title=\"7. 当复制一个虚拟机时网卡eth0启动不了\"></a>7. 当复制一个虚拟机时网卡eth0启动不了</h4><pre><code>vi /etc/udev/rules.d/70-persistent-net.rules\n将eth0的删除，将eth1的改成eth0\n或者查看eth1的序列号将网卡ifcfg-eth0的序列号改成eth1的</code></pre><h4 id=\"8-添加几台虚拟机的网址主机名映射\"><a href=\"#8-添加几台虚拟机的网址主机名映射\" class=\"headerlink\" title=\"8. 添加几台虚拟机的网址主机名映射\"></a>8. 添加几台虚拟机的网址主机名映射</h4><pre><code>vi /etc/hosts\n添加如192.168.1.101  hadoop01</code></pre><h4 id=\"9-为几台虚拟机设置免密登录\"><a href=\"#9-为几台虚拟机设置免密登录\" class=\"headerlink\" title=\"9. 为几台虚拟机设置免密登录\"></a>9. 为几台虚拟机设置免密登录</h4><h3 id=\"21-Linux下轻量级的集群管理利器ClusterShell\"><a href=\"#21-Linux下轻量级的集群管理利器ClusterShell\" class=\"headerlink\" title=\"21. Linux下轻量级的集群管理利器ClusterShell\"></a>21. Linux下轻量级的集群管理利器ClusterShell</h3>"},{"_content":"# Scala\n\n## 1. 半生类和半生对象\n\n```scala\n//半生类\nclass A{\ndef apply()={\n\n}\n\n}\n\n//半生对象\nobject A{\n\ndef apply()={\n\n}\n\n}\n\n\nval a=A()   //调用的是object.apply\nval a1=new A()\nal()       //调用的是class.apply\n\n\n//类名()    object.apply\n//对象名()  class.apply\n\n\n最佳实践是在object的apply里面 new Class\n```\n\n\n\n## 2. 尾递归求和\n\n```scala\ndef sum(nums:Int*)={\n\tif(nums==0){\n\t\t0\n\t}else{\n\t\tnums.head+sum(nums.tail:_*)\n\t}\n\n}\n```\n\n\n\n## 3. Range\n\nto // 闭区间\n\nuntil //左闭右开\n\nRange //左闭右开\n\n```\n1 to 4 \t\t\t => [1,4]  =>  1,2,3,4\n1 until 4 \t => [1,4)  =>  1,2,3\nRange(1,4)   => [1,4)  =>  1,2,3\n```\n\n\n\n","source":"notes/Scala.md","raw":"# Scala\n\n## 1. 半生类和半生对象\n\n```scala\n//半生类\nclass A{\ndef apply()={\n\n}\n\n}\n\n//半生对象\nobject A{\n\ndef apply()={\n\n}\n\n}\n\n\nval a=A()   //调用的是object.apply\nval a1=new A()\nal()       //调用的是class.apply\n\n\n//类名()    object.apply\n//对象名()  class.apply\n\n\n最佳实践是在object的apply里面 new Class\n```\n\n\n\n## 2. 尾递归求和\n\n```scala\ndef sum(nums:Int*)={\n\tif(nums==0){\n\t\t0\n\t}else{\n\t\tnums.head+sum(nums.tail:_*)\n\t}\n\n}\n```\n\n\n\n## 3. Range\n\nto // 闭区间\n\nuntil //左闭右开\n\nRange //左闭右开\n\n```\n1 to 4 \t\t\t => [1,4]  =>  1,2,3,4\n1 until 4 \t => [1,4)  =>  1,2,3\nRange(1,4)   => [1,4)  =>  1,2,3\n```\n\n\n\n","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T11:52:16.000Z","path":"notes/Scala.html","title":"","comments":1,"layout":"page","_id":"ck7ycejgq000ijr64uy6l2bbw","content":"<h1 id=\"Scala\"><a href=\"#Scala\" class=\"headerlink\" title=\"Scala\"></a>Scala</h1><h2 id=\"1-半生类和半生对象\"><a href=\"#1-半生类和半生对象\" class=\"headerlink\" title=\"1. 半生类和半生对象\"></a>1. 半生类和半生对象</h2><pre class=\" language-scala\"><code class=\"language-scala\"><span class=\"token comment\" spellcheck=\"true\">//半生类</span>\n<span class=\"token keyword\">class</span> A<span class=\"token punctuation\">{</span>\n<span class=\"token keyword\">def</span> apply<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">=</span><span class=\"token punctuation\">{</span>\n\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token comment\" spellcheck=\"true\">//半生对象</span>\n<span class=\"token keyword\">object</span> A<span class=\"token punctuation\">{</span>\n\n<span class=\"token keyword\">def</span> apply<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">=</span><span class=\"token punctuation\">{</span>\n\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token punctuation\">}</span>\n\n\n<span class=\"token keyword\">val</span> a<span class=\"token operator\">=</span>A<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\" spellcheck=\"true\">//调用的是object.apply</span>\n<span class=\"token keyword\">val</span> a1<span class=\"token operator\">=</span><span class=\"token keyword\">new</span> A<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nal<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>       <span class=\"token comment\" spellcheck=\"true\">//调用的是class.apply</span>\n\n\n<span class=\"token comment\" spellcheck=\"true\">//类名()    object.apply</span>\n<span class=\"token comment\" spellcheck=\"true\">//对象名()  class.apply</span>\n\n\n最佳实践是在<span class=\"token keyword\">object</span>的apply里面 <span class=\"token keyword\">new</span> Class</code></pre>\n<h2 id=\"2-尾递归求和\"><a href=\"#2-尾递归求和\" class=\"headerlink\" title=\"2. 尾递归求和\"></a>2. 尾递归求和</h2><pre class=\" language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">def</span> sum<span class=\"token punctuation\">(</span>nums<span class=\"token operator\">:</span><span class=\"token builtin\">Int</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span><span class=\"token operator\">=</span><span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span>nums<span class=\"token operator\">==</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n        <span class=\"token number\">0</span>\n    <span class=\"token punctuation\">}</span><span class=\"token keyword\">else</span><span class=\"token punctuation\">{</span>\n        nums<span class=\"token punctuation\">.</span>head<span class=\"token operator\">+</span>sum<span class=\"token punctuation\">(</span>nums<span class=\"token punctuation\">.</span>tail<span class=\"token operator\">:</span>_<span class=\"token operator\">*</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">}</span>\n\n<span class=\"token punctuation\">}</span></code></pre>\n<h2 id=\"3-Range\"><a href=\"#3-Range\" class=\"headerlink\" title=\"3. Range\"></a>3. Range</h2><p>to // 闭区间</p>\n<p>until //左闭右开</p>\n<p>Range //左闭右开</p>\n<pre><code>1 to 4              =&gt; [1,4]  =&gt;  1,2,3,4\n1 until 4      =&gt; [1,4)  =&gt;  1,2,3\nRange(1,4)   =&gt; [1,4)  =&gt;  1,2,3</code></pre>","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"Scala\"><a href=\"#Scala\" class=\"headerlink\" title=\"Scala\"></a>Scala</h1><h2 id=\"1-半生类和半生对象\"><a href=\"#1-半生类和半生对象\" class=\"headerlink\" title=\"1. 半生类和半生对象\"></a>1. 半生类和半生对象</h2><pre><code class=\"scala\">//半生类\nclass A{\ndef apply()={\n\n}\n\n}\n\n//半生对象\nobject A{\n\ndef apply()={\n\n}\n\n}\n\n\nval a=A()   //调用的是object.apply\nval a1=new A()\nal()       //调用的是class.apply\n\n\n//类名()    object.apply\n//对象名()  class.apply\n\n\n最佳实践是在object的apply里面 new Class</code></pre>\n<h2 id=\"2-尾递归求和\"><a href=\"#2-尾递归求和\" class=\"headerlink\" title=\"2. 尾递归求和\"></a>2. 尾递归求和</h2><pre><code class=\"scala\">def sum(nums:Int*)={\n    if(nums==0){\n        0\n    }else{\n        nums.head+sum(nums.tail:_*)\n    }\n\n}</code></pre>\n<h2 id=\"3-Range\"><a href=\"#3-Range\" class=\"headerlink\" title=\"3. Range\"></a>3. Range</h2><p>to // 闭区间</p>\n<p>until //左闭右开</p>\n<p>Range //左闭右开</p>\n<pre><code>1 to 4              =&gt; [1,4]  =&gt;  1,2,3,4\n1 until 4      =&gt; [1,4)  =&gt;  1,2,3\nRange(1,4)   =&gt; [1,4)  =&gt;  1,2,3</code></pre>"},{"_content":"# Spark 编码\n\n## 1. map 和 mapPartitions\n\n~~~\nmap是对rdd中的每一个元素进行操作；\nmapPartitions则是对rdd中的每个分区的迭代器进行操作\nMapPartitions的优点：\n如果是普通的map，比如一个partition中有1万条数据。ok，那么你的function要执行和计算1万次。\n使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有\n的partition数据。只要执行一次就可以了，性能比较高。如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。\nSparkSql或DataFrame默认会对程序进行mapPartition的优化。\nMapPartitions的缺点：\n如果是普通的map操作，一次function的执行就处理一条数据；那么如果内存不够用的情况下， 比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。\n所以说普通的map操作通常不会导致内存的OOM异常。 \n\n但是MapPartitions操作，对于大量数据来说，比如甚至一个partition，100万数据，\n一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就OOM，内存溢出。\n\n~~~\n\n## 2. Drive 和 Executo\n\n~~~\n所有RDD算子的计算功能都是由Excutor执行\n~~~\n\n## 3. Shuffle\n\n~~~\n将RDD中一个分区的数据打乱重组到其他不同分区的操作\n~~~\n\n## 4. Task 和 Partition\n\n~~~\n一个分区划分一个任务，一个任务会被分配到一个excutor\n~~~\n\n## 5. reduceByKey 和groupByKey\n\n~~~\nreduceByKey 在shuffle之前有combine(预聚合)操作，性能相对groupByKey要好\n~~~\n\n## 6. stage 划分\n\n~~~\nstage划分根据宽依赖，stage个数=1+shuffle个数\n~~~\n\n## 7. 更新map\n\n```scala\n/**\n *简化if else 结构\n **/\nif(map.contains(v))\n\tmap+=(v->0)\nmap.update(v,map(v)+1) //if外边\n```\n\n## 8. 数组切片\n\n```scala\nval l=List(1,2,3,4,5,6,7)\nval sl=l.slice(0,l.size-1) //[1,2,3,4,5,6]\nval zl=sl.zip(sl.tail) //[(1,2),(2,3),...]\nval zl.map((x,y)=>x+\"_\"+y) //[1_2,2_3,...]\n```\n\n","source":"notes/Spark.md","raw":"# Spark 编码\n\n## 1. map 和 mapPartitions\n\n~~~\nmap是对rdd中的每一个元素进行操作；\nmapPartitions则是对rdd中的每个分区的迭代器进行操作\nMapPartitions的优点：\n如果是普通的map，比如一个partition中有1万条数据。ok，那么你的function要执行和计算1万次。\n使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有\n的partition数据。只要执行一次就可以了，性能比较高。如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。\nSparkSql或DataFrame默认会对程序进行mapPartition的优化。\nMapPartitions的缺点：\n如果是普通的map操作，一次function的执行就处理一条数据；那么如果内存不够用的情况下， 比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。\n所以说普通的map操作通常不会导致内存的OOM异常。 \n\n但是MapPartitions操作，对于大量数据来说，比如甚至一个partition，100万数据，\n一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就OOM，内存溢出。\n\n~~~\n\n## 2. Drive 和 Executo\n\n~~~\n所有RDD算子的计算功能都是由Excutor执行\n~~~\n\n## 3. Shuffle\n\n~~~\n将RDD中一个分区的数据打乱重组到其他不同分区的操作\n~~~\n\n## 4. Task 和 Partition\n\n~~~\n一个分区划分一个任务，一个任务会被分配到一个excutor\n~~~\n\n## 5. reduceByKey 和groupByKey\n\n~~~\nreduceByKey 在shuffle之前有combine(预聚合)操作，性能相对groupByKey要好\n~~~\n\n## 6. stage 划分\n\n~~~\nstage划分根据宽依赖，stage个数=1+shuffle个数\n~~~\n\n## 7. 更新map\n\n```scala\n/**\n *简化if else 结构\n **/\nif(map.contains(v))\n\tmap+=(v->0)\nmap.update(v,map(v)+1) //if外边\n```\n\n## 8. 数组切片\n\n```scala\nval l=List(1,2,3,4,5,6,7)\nval sl=l.slice(0,l.size-1) //[1,2,3,4,5,6]\nval zl=sl.zip(sl.tail) //[(1,2),(2,3),...]\nval zl.map((x,y)=>x+\"_\"+y) //[1_2,2_3,...]\n```\n\n","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T11:52:16.000Z","path":"notes/Spark.html","title":"","comments":1,"layout":"page","_id":"ck7ycejgr000jjr64vp6duhi9","content":"<h1 id=\"Spark-编码\"><a href=\"#Spark-编码\" class=\"headerlink\" title=\"Spark 编码\"></a>Spark 编码</h1><h2 id=\"1-map-和-mapPartitions\"><a href=\"#1-map-和-mapPartitions\" class=\"headerlink\" title=\"1. map 和 mapPartitions\"></a>1. map 和 mapPartitions</h2><pre><code>map是对rdd中的每一个元素进行操作；\nmapPartitions则是对rdd中的每个分区的迭代器进行操作\nMapPartitions的优点：\n如果是普通的map，比如一个partition中有1万条数据。ok，那么你的function要执行和计算1万次。\n使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有\n的partition数据。只要执行一次就可以了，性能比较高。如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。\nSparkSql或DataFrame默认会对程序进行mapPartition的优化。\nMapPartitions的缺点：\n如果是普通的map操作，一次function的执行就处理一条数据；那么如果内存不够用的情况下， 比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。\n所以说普通的map操作通常不会导致内存的OOM异常。 \n\n但是MapPartitions操作，对于大量数据来说，比如甚至一个partition，100万数据，\n一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就OOM，内存溢出。\n</code></pre><h2 id=\"2-Drive-和-Executo\"><a href=\"#2-Drive-和-Executo\" class=\"headerlink\" title=\"2. Drive 和 Executo\"></a>2. Drive 和 Executo</h2><pre><code>所有RDD算子的计算功能都是由Excutor执行</code></pre><h2 id=\"3-Shuffle\"><a href=\"#3-Shuffle\" class=\"headerlink\" title=\"3. Shuffle\"></a>3. Shuffle</h2><pre><code>将RDD中一个分区的数据打乱重组到其他不同分区的操作</code></pre><h2 id=\"4-Task-和-Partition\"><a href=\"#4-Task-和-Partition\" class=\"headerlink\" title=\"4. Task 和 Partition\"></a>4. Task 和 Partition</h2><pre><code>一个分区划分一个任务，一个任务会被分配到一个excutor</code></pre><h2 id=\"5-reduceByKey-和groupByKey\"><a href=\"#5-reduceByKey-和groupByKey\" class=\"headerlink\" title=\"5. reduceByKey 和groupByKey\"></a>5. reduceByKey 和groupByKey</h2><pre><code>reduceByKey 在shuffle之前有combine(预聚合)操作，性能相对groupByKey要好</code></pre><h2 id=\"6-stage-划分\"><a href=\"#6-stage-划分\" class=\"headerlink\" title=\"6. stage 划分\"></a>6. stage 划分</h2><pre><code>stage划分根据宽依赖，stage个数=1+shuffle个数</code></pre><h2 id=\"7-更新map\"><a href=\"#7-更新map\" class=\"headerlink\" title=\"7. 更新map\"></a>7. 更新map</h2><pre class=\" language-scala\"><code class=\"language-scala\"><span class=\"token comment\" spellcheck=\"true\">/**\n *简化if else 结构\n **/</span>\n<span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span>map<span class=\"token punctuation\">.</span>contains<span class=\"token punctuation\">(</span>v<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    map<span class=\"token operator\">+=</span><span class=\"token punctuation\">(</span>v<span class=\"token operator\">-</span><span class=\"token operator\">></span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\nmap<span class=\"token punctuation\">.</span>update<span class=\"token punctuation\">(</span>v<span class=\"token punctuation\">,</span>map<span class=\"token punctuation\">(</span>v<span class=\"token punctuation\">)</span><span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">//if外边</span></code></pre>\n<h2 id=\"8-数组切片\"><a href=\"#8-数组切片\" class=\"headerlink\" title=\"8. 数组切片\"></a>8. 数组切片</h2><pre class=\" language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> l<span class=\"token operator\">=</span>List<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span><span class=\"token number\">6</span><span class=\"token punctuation\">,</span><span class=\"token number\">7</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> sl<span class=\"token operator\">=</span>l<span class=\"token punctuation\">.</span>slice<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span>l<span class=\"token punctuation\">.</span>size<span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">//[1,2,3,4,5,6]</span>\n<span class=\"token keyword\">val</span> zl<span class=\"token operator\">=</span>sl<span class=\"token punctuation\">.</span>zip<span class=\"token punctuation\">(</span>sl<span class=\"token punctuation\">.</span>tail<span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">//[(1,2),(2,3),...]</span>\n<span class=\"token keyword\">val</span> zl<span class=\"token punctuation\">.</span>map<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span>y<span class=\"token punctuation\">)</span><span class=\"token keyword\">=></span>x<span class=\"token operator\">+</span><span class=\"token string\">\"_\"</span><span class=\"token operator\">+</span>y<span class=\"token punctuation\">)</span> <span class=\"token comment\" spellcheck=\"true\">//[1_2,2_3,...]</span></code></pre>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"Spark-编码\"><a href=\"#Spark-编码\" class=\"headerlink\" title=\"Spark 编码\"></a>Spark 编码</h1><h2 id=\"1-map-和-mapPartitions\"><a href=\"#1-map-和-mapPartitions\" class=\"headerlink\" title=\"1. map 和 mapPartitions\"></a>1. map 和 mapPartitions</h2><pre><code>map是对rdd中的每一个元素进行操作；\nmapPartitions则是对rdd中的每个分区的迭代器进行操作\nMapPartitions的优点：\n如果是普通的map，比如一个partition中有1万条数据。ok，那么你的function要执行和计算1万次。\n使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有\n的partition数据。只要执行一次就可以了，性能比较高。如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。\nSparkSql或DataFrame默认会对程序进行mapPartition的优化。\nMapPartitions的缺点：\n如果是普通的map操作，一次function的执行就处理一条数据；那么如果内存不够用的情况下， 比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。\n所以说普通的map操作通常不会导致内存的OOM异常。 \n\n但是MapPartitions操作，对于大量数据来说，比如甚至一个partition，100万数据，\n一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就OOM，内存溢出。\n</code></pre><h2 id=\"2-Drive-和-Executo\"><a href=\"#2-Drive-和-Executo\" class=\"headerlink\" title=\"2. Drive 和 Executo\"></a>2. Drive 和 Executo</h2><pre><code>所有RDD算子的计算功能都是由Excutor执行</code></pre><h2 id=\"3-Shuffle\"><a href=\"#3-Shuffle\" class=\"headerlink\" title=\"3. Shuffle\"></a>3. Shuffle</h2><pre><code>将RDD中一个分区的数据打乱重组到其他不同分区的操作</code></pre><h2 id=\"4-Task-和-Partition\"><a href=\"#4-Task-和-Partition\" class=\"headerlink\" title=\"4. Task 和 Partition\"></a>4. Task 和 Partition</h2><pre><code>一个分区划分一个任务，一个任务会被分配到一个excutor</code></pre><h2 id=\"5-reduceByKey-和groupByKey\"><a href=\"#5-reduceByKey-和groupByKey\" class=\"headerlink\" title=\"5. reduceByKey 和groupByKey\"></a>5. reduceByKey 和groupByKey</h2><pre><code>reduceByKey 在shuffle之前有combine(预聚合)操作，性能相对groupByKey要好</code></pre><h2 id=\"6-stage-划分\"><a href=\"#6-stage-划分\" class=\"headerlink\" title=\"6. stage 划分\"></a>6. stage 划分</h2><pre><code>stage划分根据宽依赖，stage个数=1+shuffle个数</code></pre><h2 id=\"7-更新map\"><a href=\"#7-更新map\" class=\"headerlink\" title=\"7. 更新map\"></a>7. 更新map</h2><pre><code class=\"scala\">/**\n *简化if else 结构\n **/\nif(map.contains(v))\n    map+=(v-&gt;0)\nmap.update(v,map(v)+1) //if外边</code></pre>\n<h2 id=\"8-数组切片\"><a href=\"#8-数组切片\" class=\"headerlink\" title=\"8. 数组切片\"></a>8. 数组切片</h2><pre><code class=\"scala\">val l=List(1,2,3,4,5,6,7)\nval sl=l.slice(0,l.size-1) //[1,2,3,4,5,6]\nval zl=sl.zip(sl.tail) //[(1,2),(2,3),...]\nval zl.map((x,y)=&gt;x+&quot;_&quot;+y) //[1_2,2_3,...]</code></pre>\n"},{"title":"笔记","date":"2019-12-26T15:50:21.000Z","_content":"\n[Bigdata](./Bigdata.html)\n\n[Hive](./Hive.html)\n\n[Spark](./Spark.html)\n\n[Redis](./Redis.html)\n\n[Kafka](./Kafka.html)\n\n[Linux](./Linux.html)\n\n[GitHub](./Github.html)\n\n[Scala](./Scala.html)\n\n[Docker](./Docker.html)\n\n[Goland](./Goland.html)\n\n","source":"notes/index.md","raw":"---\ntitle: 笔记\ndate: 2019-12-26 23:50:21\n---\n\n[Bigdata](./Bigdata.html)\n\n[Hive](./Hive.html)\n\n[Spark](./Spark.html)\n\n[Redis](./Redis.html)\n\n[Kafka](./Kafka.html)\n\n[Linux](./Linux.html)\n\n[GitHub](./Github.html)\n\n[Scala](./Scala.html)\n\n[Docker](./Docker.html)\n\n[Goland](./Goland.html)\n\n","updated":"2020-03-18T12:03:45.000Z","path":"notes/index.html","comments":1,"layout":"page","_id":"ck7ycejgs000kjr64z7kjanjb","content":"<p><a href=\"./Bigdata.html\">Bigdata</a></p>\n<p><a href=\"./Hive.html\">Hive</a></p>\n<p><a href=\"./Spark.html\">Spark</a></p>\n<p><a href=\"./Redis.html\">Redis</a></p>\n<p><a href=\"./Kafka.html\">Kafka</a></p>\n<p><a href=\"./Linux.html\">Linux</a></p>\n<p><a href=\"./Github.html\">GitHub</a></p>\n<p><a href=\"./Scala.html\">Scala</a></p>\n<p><a href=\"./Docker.html\">Docker</a></p>\n<p><a href=\"./Goland.html\">Goland</a></p>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<p><a href=\"./Bigdata.html\">Bigdata</a></p>\n<p><a href=\"./Hive.html\">Hive</a></p>\n<p><a href=\"./Spark.html\">Spark</a></p>\n<p><a href=\"./Redis.html\">Redis</a></p>\n<p><a href=\"./Kafka.html\">Kafka</a></p>\n<p><a href=\"./Linux.html\">Linux</a></p>\n<p><a href=\"./Github.html\">GitHub</a></p>\n<p><a href=\"./Scala.html\">Scala</a></p>\n<p><a href=\"./Docker.html\">Docker</a></p>\n<p><a href=\"./Goland.html\">Goland</a></p>\n"},{"title":"tags","date":"2018-09-12T15:24:17.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-09-12 23:24:17\ntype: \"tags\"\nlayout: \"tags\"\n---\n","updated":"2020-03-19T04:51:26.000Z","path":"tags/index.html","comments":1,"_id":"ck7ycejgt000ljr64u9maajsz","content":"","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":""},{"_content":"# Flink\n\n## Application\n+ 启动：flink run -c mainclasspath jarpath\n+ 取消：flink cancel jobid\n+ 停止：flink stop jobid\n### Job\n## Task\njob中的一个阶段就是一个task，一个task包括链条连接的多个subtask，一个task运行在一个线程里面，task平分slot里面的内存资源共享slot里面的cpu资源\n## SubTask\nflink job中最小执行单元\n## 算子\n\n### Source\n\n使用EventTime，划分滚动窗口，如果使用的是并行的Source，例如KafkaSource，创建Kafka的Topic时有多个分区，每个Source的分区都要满足触发的条件，整个窗口才会被触发\n\n### Transformation\n+ map\t//DataStream - > DataStream\n+\tflatMap\t//DataStream -> DataStream\n+\tfilter\t//DataStream -> DataStream\n+\tkeyBy\t//DataStream -> KeyStream\n### Sink\n\nwriteAsCsv必须是元组才能正常写入\n\n## WaterMark\n决定一个窗口什么时候激活（触发），这时的窗口的最大长度为\n\nmaterMark>=上一个窗口的结束边界就会触发窗口执行\n\nwatermark是flink中窗口延迟触发的机制\n\n在每个算子内部都自己有一个事件时间时钟，事件时间时钟是根据watermark来更新的，。流入算子的数据可能是单分区也可能是多分区的，每个流入算子的分区端都会有一个自己的partition watermark标记，当该分区内进入新的高于之前watermark的watermark数据时，partition watermark标记才会被更新，task内部也维护一个task的watermark数据。如果某个分区的partition watermark < task watermark，那么task watermark会更新为该partition watermark数据，然后把task 把当前更新的task watermark数据发向下游task。\n\n### AssignerWithPeriodicWatermarks\nwindowmax=watermark+windowsize\nwaterMark=数据携带的时间（窗口中最大的时间）-延迟执行的时间\n\n周期性的生成watermark，定期向分区数据流中插入时间水印。默认周期时间为200毫秒，可以使用setAutoWatermakrIntaval（）来设置\nBoundedOutOfOrdernessTimestampExtractor继承自AssignerWithPeriodicWatermarks 属于周期性watermark\n\n>周期性数据水印在特定条件下可能会造成数据错误\n>例如：env.fromCollection(List((1, \"a1\", 158324361000l), (1, \"a2\", 158324369000l), (1, \"a3\", 158324364000l), (1, \"a4\", 158324361000l), (1, \"a5\",158324365000l), (1, \"a6\", 158324362000l), (1, \"a7\",158324367000l))) \n>.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[(Int, String, Long)](Time.milliseconds(0)) {\n>override def extractTimestamp(element: (Int, String, Long)): Long = {\n> element._3\n> }\n> })\n> .keyBy(0)\n> .window(TumblingEventTimeWindows.of(Time.seconds(4)))\n> .sum(2)\n> .print()\n> 当使用周期性水印AssignerWithPeriodicWatermarks时就会造成数据的错误计算\n> 是因为周期性水印是定期产生的（默认200毫秒）但是在这个周期里可能会出现有多个数据已经过去，这多个数据用于一个水印从而造成数据计算错误\n\n### AssignerWithPunctuatedWatermarks\n根据事件生成watermark。可以用于根据具体数据来生成watermark，\n## Window\n\n### Keyed Window\n使用keyby后流的窗口\n#### GlobalWindow\n#### CountWindow\n#### TimeWindow\n+ Tumbling\n+ Sliding\n+ Session\n\n### Non-Keyed Windows\n未使用keyby后流的窗口\n#### windowAll\n\n## Window 之后的算子\n### Trigger\nwindow 数据触发器，keyed or non-keyed window 都可以使用\n+ EventTimeTrigger：事件时间触发器\n+ ProcessingTimeTrigger：程序时间触发器\n+ CountTrigger：数量出发器。只发送窗口触发信号\n+ PurgingTrigger：代理模式触发器，发送窗口触发和数据清理信号\n### Evictor\nwindow 数据剔除器，可以在window执行前或者执行后剔除window内的元素\n+ CountEvictor：数量剔除器。在Window中保留指定数量的元素，并从窗口头部开始丢弃其余元素。\n+ DeltaEvictor： 阈值剔除器。计算Window中最后一个元素与其余每个元素之间的增量，丢弃增量大于或等于阈值的元素。\n+ TimeEvictor：时间剔除器。保留Window中最近一段时间内的元素，并丢弃其余元素。\n### AllowedLateness\n\n决定一个窗口什么时候销毁，window 延迟数据是否保留计算，可能会造成窗口的二次触发，会导致结果数据的更新，造成数据不一致。这时窗口的最大长度为windowmax=waterma+windowsize+allowedleteness\n\n## State\n### Managed State\n#### Operator State\noperator state 绑定到每个算子的实例上，各个实例拥有自己的state，一个实例无法获取同并行的其他实例的state数据\n\noperator state ：记录的是每一个分区的偏移量\n\n#### Keyed State\n\nkeyedstate：在一个subtask中可能有多个state，一个组对应一个key的状态\n\n\n\n### Raw State\n## CheckPoint\n全自动程序管理，轻量快捷算子级数据快照\n\n开启flink checkpoint 后设置精准一次消费，kafka的offset会保存在savepoint设置的路径里面，还会降offset保存在kafka 特殊topic里面，如果程序重启时没有指定savepiont保存数据的地址会默认根据kafka 特殊topic保存的偏移量消费数据，可以设置不降offset保存在kafka 特殊topic里面使用，setCommitOffsetOnCheckpoints(false)\n\n开启检查点机制\n\n```\n// start a checkpoint every 1000 ms\nenv.enableCheckpointing(1000);\n\n// advanced options:\n\n// set mode to exactly-once (this is the default)\nenv.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n\n// make sure 500 ms of progress happen between checkpoints\nenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);\n\n// checkpoints have to complete within one minute, or are discarded\nenv.getCheckpointConfig().setCheckpointTimeout(60000);\n\n// allow only one checkpoint to be in progress at the same time\nenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n\n// enable externalized checkpoints which are retained after job cancellation\nenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n\n// allow job recovery fallback to checkpoint when there is a more recent savepoint\nenv.getCheckpointConfig().setPreferCheckpointForRecovery(true);\n```\n### Barrier\n算子checkpoint的依据，是exactly-once 和at-least-once语义的根据\n## SavePoint\n\n人工参与管理的application级别的数据快照\n\n### 手动保存数据快照\n+ flink stop jobid\n  停止job并保存快照，如果在flink-conf.yaml上配置了state.savepoints.dir，停止任务后会自动将快照保存。\n  \n+ flink stop jobid -p dirpath\n  停止job并将快照保存在dirpath\n  \n+ flink savepoint jobid [dirpath] \n  在不结束job的情况下保存快照。如果带有dirpath则会将快照保存在此目录否则会保存在默认配置的保存目录\n### 从数据快照恢复程序\n+ 直接从savepoint目录恢复\n  flink run -s dirpath\n  从dirpath目录恢复程序\n  \n+ 跳过无法恢复的算子恢复\n  flink run -s dirpath -n\n  \n### 手动清除数据快照\n+ flink savepoint -d itemdirpath\n  手动将数据某个具体快照删除（itemdirpath 快照具体根目录）\n  \n  \n  \n## CEP（Complex Event Processing）\n\nNFA（Nondeterministic Finite Automaton）\n\n## 反压\nflink 三层buff缓存（resultsubpartition，nettybuff，netty中通过高水位来控制buff是否还可以接收数据，socketbuff）\n\n1.5 之前是基于tcp 窗口的反压机制，发送端根据接收端返回的ack和windox size 来发送数据，当window size 为0时，发送端则不会再附上数据，而是发送一个zerowindow 的探测性数据来确定是否可以再次发送数据，当接收端继续可以接收数据时，发送端才会继续发送数据\n\n基于tcp窗口的反压机制缺点\n1.单个task造成的反压，会阻断整个TM-TM的socket，连chekcpoint barrier 也无法发送\n2.反压路径较长，导致生效延迟较大\n\n\n1.5 引入credit 机制实现反压，credit 反压机制是类似于tcp 窗口反压实现的另一种反压机制，resultsubpartition在发送数据时会带有resultbuff里面还存有的数据大小 backlog size，inputchanel在接收到时会计算自己当前还能接收到的数据大小，当inputchanel无法再接收数据时会将credit置为0，告诉result不能再接收消息。result每次发送消息时会检测当前自己的credit数据，当credit为0时 则不会再向netty发送数据从而实现反压机制\n\n\n\n## 其他\n\n两个流join 必须有等值字段必须都在同一个窗口里面\n\nduplicate key update  mysql数据库的更新插入 合为一条sql\n\n并行度： **算子级别** > **env级别** > **Client级别** > **系统默认级别**\n\n在所有Task共享资源槽点名字相同，默认情况下 （pipline）\n同一个job的同一个Task中的多个subTask不能在同一个slot槽中\n\n>具有并行度的subtask 不能在一个slot槽中\n对于同一个job，不同Task【阶段】的subTask可以在同一个资源槽中\n\n\n","source":"notes/Flink.md","raw":"# Flink\n\n## Application\n+ 启动：flink run -c mainclasspath jarpath\n+ 取消：flink cancel jobid\n+ 停止：flink stop jobid\n### Job\n## Task\njob中的一个阶段就是一个task，一个task包括链条连接的多个subtask，一个task运行在一个线程里面，task平分slot里面的内存资源共享slot里面的cpu资源\n## SubTask\nflink job中最小执行单元\n## 算子\n\n### Source\n\n使用EventTime，划分滚动窗口，如果使用的是并行的Source，例如KafkaSource，创建Kafka的Topic时有多个分区，每个Source的分区都要满足触发的条件，整个窗口才会被触发\n\n### Transformation\n+ map\t//DataStream - > DataStream\n+\tflatMap\t//DataStream -> DataStream\n+\tfilter\t//DataStream -> DataStream\n+\tkeyBy\t//DataStream -> KeyStream\n### Sink\n\nwriteAsCsv必须是元组才能正常写入\n\n## WaterMark\n决定一个窗口什么时候激活（触发），这时的窗口的最大长度为\n\nmaterMark>=上一个窗口的结束边界就会触发窗口执行\n\nwatermark是flink中窗口延迟触发的机制\n\n在每个算子内部都自己有一个事件时间时钟，事件时间时钟是根据watermark来更新的，。流入算子的数据可能是单分区也可能是多分区的，每个流入算子的分区端都会有一个自己的partition watermark标记，当该分区内进入新的高于之前watermark的watermark数据时，partition watermark标记才会被更新，task内部也维护一个task的watermark数据。如果某个分区的partition watermark < task watermark，那么task watermark会更新为该partition watermark数据，然后把task 把当前更新的task watermark数据发向下游task。\n\n### AssignerWithPeriodicWatermarks\nwindowmax=watermark+windowsize\nwaterMark=数据携带的时间（窗口中最大的时间）-延迟执行的时间\n\n周期性的生成watermark，定期向分区数据流中插入时间水印。默认周期时间为200毫秒，可以使用setAutoWatermakrIntaval（）来设置\nBoundedOutOfOrdernessTimestampExtractor继承自AssignerWithPeriodicWatermarks 属于周期性watermark\n\n>周期性数据水印在特定条件下可能会造成数据错误\n>例如：env.fromCollection(List((1, \"a1\", 158324361000l), (1, \"a2\", 158324369000l), (1, \"a3\", 158324364000l), (1, \"a4\", 158324361000l), (1, \"a5\",158324365000l), (1, \"a6\", 158324362000l), (1, \"a7\",158324367000l))) \n>.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[(Int, String, Long)](Time.milliseconds(0)) {\n>override def extractTimestamp(element: (Int, String, Long)): Long = {\n> element._3\n> }\n> })\n> .keyBy(0)\n> .window(TumblingEventTimeWindows.of(Time.seconds(4)))\n> .sum(2)\n> .print()\n> 当使用周期性水印AssignerWithPeriodicWatermarks时就会造成数据的错误计算\n> 是因为周期性水印是定期产生的（默认200毫秒）但是在这个周期里可能会出现有多个数据已经过去，这多个数据用于一个水印从而造成数据计算错误\n\n### AssignerWithPunctuatedWatermarks\n根据事件生成watermark。可以用于根据具体数据来生成watermark，\n## Window\n\n### Keyed Window\n使用keyby后流的窗口\n#### GlobalWindow\n#### CountWindow\n#### TimeWindow\n+ Tumbling\n+ Sliding\n+ Session\n\n### Non-Keyed Windows\n未使用keyby后流的窗口\n#### windowAll\n\n## Window 之后的算子\n### Trigger\nwindow 数据触发器，keyed or non-keyed window 都可以使用\n+ EventTimeTrigger：事件时间触发器\n+ ProcessingTimeTrigger：程序时间触发器\n+ CountTrigger：数量出发器。只发送窗口触发信号\n+ PurgingTrigger：代理模式触发器，发送窗口触发和数据清理信号\n### Evictor\nwindow 数据剔除器，可以在window执行前或者执行后剔除window内的元素\n+ CountEvictor：数量剔除器。在Window中保留指定数量的元素，并从窗口头部开始丢弃其余元素。\n+ DeltaEvictor： 阈值剔除器。计算Window中最后一个元素与其余每个元素之间的增量，丢弃增量大于或等于阈值的元素。\n+ TimeEvictor：时间剔除器。保留Window中最近一段时间内的元素，并丢弃其余元素。\n### AllowedLateness\n\n决定一个窗口什么时候销毁，window 延迟数据是否保留计算，可能会造成窗口的二次触发，会导致结果数据的更新，造成数据不一致。这时窗口的最大长度为windowmax=waterma+windowsize+allowedleteness\n\n## State\n### Managed State\n#### Operator State\noperator state 绑定到每个算子的实例上，各个实例拥有自己的state，一个实例无法获取同并行的其他实例的state数据\n\noperator state ：记录的是每一个分区的偏移量\n\n#### Keyed State\n\nkeyedstate：在一个subtask中可能有多个state，一个组对应一个key的状态\n\n\n\n### Raw State\n## CheckPoint\n全自动程序管理，轻量快捷算子级数据快照\n\n开启flink checkpoint 后设置精准一次消费，kafka的offset会保存在savepoint设置的路径里面，还会降offset保存在kafka 特殊topic里面，如果程序重启时没有指定savepiont保存数据的地址会默认根据kafka 特殊topic保存的偏移量消费数据，可以设置不降offset保存在kafka 特殊topic里面使用，setCommitOffsetOnCheckpoints(false)\n\n开启检查点机制\n\n```\n// start a checkpoint every 1000 ms\nenv.enableCheckpointing(1000);\n\n// advanced options:\n\n// set mode to exactly-once (this is the default)\nenv.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n\n// make sure 500 ms of progress happen between checkpoints\nenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);\n\n// checkpoints have to complete within one minute, or are discarded\nenv.getCheckpointConfig().setCheckpointTimeout(60000);\n\n// allow only one checkpoint to be in progress at the same time\nenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n\n// enable externalized checkpoints which are retained after job cancellation\nenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n\n// allow job recovery fallback to checkpoint when there is a more recent savepoint\nenv.getCheckpointConfig().setPreferCheckpointForRecovery(true);\n```\n### Barrier\n算子checkpoint的依据，是exactly-once 和at-least-once语义的根据\n## SavePoint\n\n人工参与管理的application级别的数据快照\n\n### 手动保存数据快照\n+ flink stop jobid\n  停止job并保存快照，如果在flink-conf.yaml上配置了state.savepoints.dir，停止任务后会自动将快照保存。\n  \n+ flink stop jobid -p dirpath\n  停止job并将快照保存在dirpath\n  \n+ flink savepoint jobid [dirpath] \n  在不结束job的情况下保存快照。如果带有dirpath则会将快照保存在此目录否则会保存在默认配置的保存目录\n### 从数据快照恢复程序\n+ 直接从savepoint目录恢复\n  flink run -s dirpath\n  从dirpath目录恢复程序\n  \n+ 跳过无法恢复的算子恢复\n  flink run -s dirpath -n\n  \n### 手动清除数据快照\n+ flink savepoint -d itemdirpath\n  手动将数据某个具体快照删除（itemdirpath 快照具体根目录）\n  \n  \n  \n## CEP（Complex Event Processing）\n\nNFA（Nondeterministic Finite Automaton）\n\n## 反压\nflink 三层buff缓存（resultsubpartition，nettybuff，netty中通过高水位来控制buff是否还可以接收数据，socketbuff）\n\n1.5 之前是基于tcp 窗口的反压机制，发送端根据接收端返回的ack和windox size 来发送数据，当window size 为0时，发送端则不会再附上数据，而是发送一个zerowindow 的探测性数据来确定是否可以再次发送数据，当接收端继续可以接收数据时，发送端才会继续发送数据\n\n基于tcp窗口的反压机制缺点\n1.单个task造成的反压，会阻断整个TM-TM的socket，连chekcpoint barrier 也无法发送\n2.反压路径较长，导致生效延迟较大\n\n\n1.5 引入credit 机制实现反压，credit 反压机制是类似于tcp 窗口反压实现的另一种反压机制，resultsubpartition在发送数据时会带有resultbuff里面还存有的数据大小 backlog size，inputchanel在接收到时会计算自己当前还能接收到的数据大小，当inputchanel无法再接收数据时会将credit置为0，告诉result不能再接收消息。result每次发送消息时会检测当前自己的credit数据，当credit为0时 则不会再向netty发送数据从而实现反压机制\n\n\n\n## 其他\n\n两个流join 必须有等值字段必须都在同一个窗口里面\n\nduplicate key update  mysql数据库的更新插入 合为一条sql\n\n并行度： **算子级别** > **env级别** > **Client级别** > **系统默认级别**\n\n在所有Task共享资源槽点名字相同，默认情况下 （pipline）\n同一个job的同一个Task中的多个subTask不能在同一个slot槽中\n\n>具有并行度的subtask 不能在一个slot槽中\n对于同一个job，不同Task【阶段】的subTask可以在同一个资源槽中\n\n\n","date":"2020-03-19T05:09:33.000Z","updated":"2020-03-18T11:52:16.000Z","path":"notes/Flink.html","title":"","comments":1,"layout":"page","_id":"ck7ycejhc000njr647k1pm2cz","content":"<h1 id=\"Flink\"><a href=\"#Flink\" class=\"headerlink\" title=\"Flink\"></a>Flink</h1><h2 id=\"Application\"><a href=\"#Application\" class=\"headerlink\" title=\"Application\"></a>Application</h2><ul>\n<li>启动：flink run -c mainclasspath jarpath</li>\n<li>取消：flink cancel jobid</li>\n<li>停止：flink stop jobid<h3 id=\"Job\"><a href=\"#Job\" class=\"headerlink\" title=\"Job\"></a>Job</h3><h2 id=\"Task\"><a href=\"#Task\" class=\"headerlink\" title=\"Task\"></a>Task</h2>job中的一个阶段就是一个task，一个task包括链条连接的多个subtask，一个task运行在一个线程里面，task平分slot里面的内存资源共享slot里面的cpu资源<h2 id=\"SubTask\"><a href=\"#SubTask\" class=\"headerlink\" title=\"SubTask\"></a>SubTask</h2>flink job中最小执行单元<h2 id=\"算子\"><a href=\"#算子\" class=\"headerlink\" title=\"算子\"></a>算子</h2></li>\n</ul>\n<h3 id=\"Source\"><a href=\"#Source\" class=\"headerlink\" title=\"Source\"></a>Source</h3><p>使用EventTime，划分滚动窗口，如果使用的是并行的Source，例如KafkaSource，创建Kafka的Topic时有多个分区，每个Source的分区都要满足触发的条件，整个窗口才会被触发</p>\n<h3 id=\"Transformation\"><a href=\"#Transformation\" class=\"headerlink\" title=\"Transformation\"></a>Transformation</h3><ul>\n<li>map    //DataStream - &gt; DataStream</li>\n<li>flatMap    //DataStream -&gt; DataStream</li>\n<li>filter    //DataStream -&gt; DataStream</li>\n<li>keyBy    //DataStream -&gt; KeyStream<h3 id=\"Sink\"><a href=\"#Sink\" class=\"headerlink\" title=\"Sink\"></a>Sink</h3></li>\n</ul>\n<p>writeAsCsv必须是元组才能正常写入</p>\n<h2 id=\"WaterMark\"><a href=\"#WaterMark\" class=\"headerlink\" title=\"WaterMark\"></a>WaterMark</h2><p>决定一个窗口什么时候激活（触发），这时的窗口的最大长度为</p>\n<p>materMark&gt;=上一个窗口的结束边界就会触发窗口执行</p>\n<p>watermark是flink中窗口延迟触发的机制</p>\n<p>在每个算子内部都自己有一个事件时间时钟，事件时间时钟是根据watermark来更新的，。流入算子的数据可能是单分区也可能是多分区的，每个流入算子的分区端都会有一个自己的partition watermark标记，当该分区内进入新的高于之前watermark的watermark数据时，partition watermark标记才会被更新，task内部也维护一个task的watermark数据。如果某个分区的partition watermark &lt; task watermark，那么task watermark会更新为该partition watermark数据，然后把task 把当前更新的task watermark数据发向下游task。</p>\n<h3 id=\"AssignerWithPeriodicWatermarks\"><a href=\"#AssignerWithPeriodicWatermarks\" class=\"headerlink\" title=\"AssignerWithPeriodicWatermarks\"></a>AssignerWithPeriodicWatermarks</h3><p>windowmax=watermark+windowsize<br>waterMark=数据携带的时间（窗口中最大的时间）-延迟执行的时间</p>\n<p>周期性的生成watermark，定期向分区数据流中插入时间水印。默认周期时间为200毫秒，可以使用setAutoWatermakrIntaval（）来设置<br>BoundedOutOfOrdernessTimestampExtractor继承自AssignerWithPeriodicWatermarks 属于周期性watermark</p>\n<blockquote>\n<p>周期性数据水印在特定条件下可能会造成数据错误<br>例如：env.fromCollection(List((1, “a1”, 158324361000l), (1, “a2”, 158324369000l), (1, “a3”, 158324364000l), (1, “a4”, 158324361000l), (1, “a5”,158324365000l), (1, “a6”, 158324362000l), (1, “a7”,158324367000l)))<br>.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<a href=\"Time.milliseconds(0)\">(Int, String, Long)</a> {<br>override def extractTimestamp(element: (Int, String, Long)): Long = {<br>element._3<br>}<br>})<br>.keyBy(0)<br>.window(TumblingEventTimeWindows.of(Time.seconds(4)))<br>.sum(2)<br>.print()<br>当使用周期性水印AssignerWithPeriodicWatermarks时就会造成数据的错误计算<br>是因为周期性水印是定期产生的（默认200毫秒）但是在这个周期里可能会出现有多个数据已经过去，这多个数据用于一个水印从而造成数据计算错误</p>\n</blockquote>\n<h3 id=\"AssignerWithPunctuatedWatermarks\"><a href=\"#AssignerWithPunctuatedWatermarks\" class=\"headerlink\" title=\"AssignerWithPunctuatedWatermarks\"></a>AssignerWithPunctuatedWatermarks</h3><p>根据事件生成watermark。可以用于根据具体数据来生成watermark，</p>\n<h2 id=\"Window\"><a href=\"#Window\" class=\"headerlink\" title=\"Window\"></a>Window</h2><h3 id=\"Keyed-Window\"><a href=\"#Keyed-Window\" class=\"headerlink\" title=\"Keyed Window\"></a>Keyed Window</h3><p>使用keyby后流的窗口</p>\n<h4 id=\"GlobalWindow\"><a href=\"#GlobalWindow\" class=\"headerlink\" title=\"GlobalWindow\"></a>GlobalWindow</h4><h4 id=\"CountWindow\"><a href=\"#CountWindow\" class=\"headerlink\" title=\"CountWindow\"></a>CountWindow</h4><h4 id=\"TimeWindow\"><a href=\"#TimeWindow\" class=\"headerlink\" title=\"TimeWindow\"></a>TimeWindow</h4><ul>\n<li>Tumbling</li>\n<li>Sliding</li>\n<li>Session</li>\n</ul>\n<h3 id=\"Non-Keyed-Windows\"><a href=\"#Non-Keyed-Windows\" class=\"headerlink\" title=\"Non-Keyed Windows\"></a>Non-Keyed Windows</h3><p>未使用keyby后流的窗口</p>\n<h4 id=\"windowAll\"><a href=\"#windowAll\" class=\"headerlink\" title=\"windowAll\"></a>windowAll</h4><h2 id=\"Window-之后的算子\"><a href=\"#Window-之后的算子\" class=\"headerlink\" title=\"Window 之后的算子\"></a>Window 之后的算子</h2><h3 id=\"Trigger\"><a href=\"#Trigger\" class=\"headerlink\" title=\"Trigger\"></a>Trigger</h3><p>window 数据触发器，keyed or non-keyed window 都可以使用</p>\n<ul>\n<li>EventTimeTrigger：事件时间触发器</li>\n<li>ProcessingTimeTrigger：程序时间触发器</li>\n<li>CountTrigger：数量出发器。只发送窗口触发信号</li>\n<li>PurgingTrigger：代理模式触发器，发送窗口触发和数据清理信号<h3 id=\"Evictor\"><a href=\"#Evictor\" class=\"headerlink\" title=\"Evictor\"></a>Evictor</h3>window 数据剔除器，可以在window执行前或者执行后剔除window内的元素</li>\n<li>CountEvictor：数量剔除器。在Window中保留指定数量的元素，并从窗口头部开始丢弃其余元素。</li>\n<li>DeltaEvictor： 阈值剔除器。计算Window中最后一个元素与其余每个元素之间的增量，丢弃增量大于或等于阈值的元素。</li>\n<li>TimeEvictor：时间剔除器。保留Window中最近一段时间内的元素，并丢弃其余元素。<h3 id=\"AllowedLateness\"><a href=\"#AllowedLateness\" class=\"headerlink\" title=\"AllowedLateness\"></a>AllowedLateness</h3></li>\n</ul>\n<p>决定一个窗口什么时候销毁，window 延迟数据是否保留计算，可能会造成窗口的二次触发，会导致结果数据的更新，造成数据不一致。这时窗口的最大长度为windowmax=waterma+windowsize+allowedleteness</p>\n<h2 id=\"State\"><a href=\"#State\" class=\"headerlink\" title=\"State\"></a>State</h2><h3 id=\"Managed-State\"><a href=\"#Managed-State\" class=\"headerlink\" title=\"Managed State\"></a>Managed State</h3><h4 id=\"Operator-State\"><a href=\"#Operator-State\" class=\"headerlink\" title=\"Operator State\"></a>Operator State</h4><p>operator state 绑定到每个算子的实例上，各个实例拥有自己的state，一个实例无法获取同并行的其他实例的state数据</p>\n<p>operator state ：记录的是每一个分区的偏移量</p>\n<h4 id=\"Keyed-State\"><a href=\"#Keyed-State\" class=\"headerlink\" title=\"Keyed State\"></a>Keyed State</h4><p>keyedstate：在一个subtask中可能有多个state，一个组对应一个key的状态</p>\n<h3 id=\"Raw-State\"><a href=\"#Raw-State\" class=\"headerlink\" title=\"Raw State\"></a>Raw State</h3><h2 id=\"CheckPoint\"><a href=\"#CheckPoint\" class=\"headerlink\" title=\"CheckPoint\"></a>CheckPoint</h2><p>全自动程序管理，轻量快捷算子级数据快照</p>\n<p>开启flink checkpoint 后设置精准一次消费，kafka的offset会保存在savepoint设置的路径里面，还会降offset保存在kafka 特殊topic里面，如果程序重启时没有指定savepiont保存数据的地址会默认根据kafka 特殊topic保存的偏移量消费数据，可以设置不降offset保存在kafka 特殊topic里面使用，setCommitOffsetOnCheckpoints(false)</p>\n<p>开启检查点机制</p>\n<pre><code>// start a checkpoint every 1000 ms\nenv.enableCheckpointing(1000);\n\n// advanced options:\n\n// set mode to exactly-once (this is the default)\nenv.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n\n// make sure 500 ms of progress happen between checkpoints\nenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);\n\n// checkpoints have to complete within one minute, or are discarded\nenv.getCheckpointConfig().setCheckpointTimeout(60000);\n\n// allow only one checkpoint to be in progress at the same time\nenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n\n// enable externalized checkpoints which are retained after job cancellation\nenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n\n// allow job recovery fallback to checkpoint when there is a more recent savepoint\nenv.getCheckpointConfig().setPreferCheckpointForRecovery(true);</code></pre><h3 id=\"Barrier\"><a href=\"#Barrier\" class=\"headerlink\" title=\"Barrier\"></a>Barrier</h3><p>算子checkpoint的依据，是exactly-once 和at-least-once语义的根据</p>\n<h2 id=\"SavePoint\"><a href=\"#SavePoint\" class=\"headerlink\" title=\"SavePoint\"></a>SavePoint</h2><p>人工参与管理的application级别的数据快照</p>\n<h3 id=\"手动保存数据快照\"><a href=\"#手动保存数据快照\" class=\"headerlink\" title=\"手动保存数据快照\"></a>手动保存数据快照</h3><ul>\n<li><p>flink stop jobid<br>停止job并保存快照，如果在flink-conf.yaml上配置了state.savepoints.dir，停止任务后会自动将快照保存。</p>\n</li>\n<li><p>flink stop jobid -p dirpath<br>停止job并将快照保存在dirpath</p>\n</li>\n<li><p>flink savepoint jobid [dirpath]<br>在不结束job的情况下保存快照。如果带有dirpath则会将快照保存在此目录否则会保存在默认配置的保存目录</p>\n<h3 id=\"从数据快照恢复程序\"><a href=\"#从数据快照恢复程序\" class=\"headerlink\" title=\"从数据快照恢复程序\"></a>从数据快照恢复程序</h3></li>\n<li><p>直接从savepoint目录恢复<br>flink run -s dirpath<br>从dirpath目录恢复程序</p>\n</li>\n<li><p>跳过无法恢复的算子恢复<br>flink run -s dirpath -n</p>\n</li>\n</ul>\n<h3 id=\"手动清除数据快照\"><a href=\"#手动清除数据快照\" class=\"headerlink\" title=\"手动清除数据快照\"></a>手动清除数据快照</h3><ul>\n<li>flink savepoint -d itemdirpath<br>手动将数据某个具体快照删除（itemdirpath 快照具体根目录）</li>\n</ul>\n<h2 id=\"CEP（Complex-Event-Processing）\"><a href=\"#CEP（Complex-Event-Processing）\" class=\"headerlink\" title=\"CEP（Complex Event Processing）\"></a>CEP（Complex Event Processing）</h2><p>NFA（Nondeterministic Finite Automaton）</p>\n<h2 id=\"反压\"><a href=\"#反压\" class=\"headerlink\" title=\"反压\"></a>反压</h2><p>flink 三层buff缓存（resultsubpartition，nettybuff，netty中通过高水位来控制buff是否还可以接收数据，socketbuff）</p>\n<p>1.5 之前是基于tcp 窗口的反压机制，发送端根据接收端返回的ack和windox size 来发送数据，当window size 为0时，发送端则不会再附上数据，而是发送一个zerowindow 的探测性数据来确定是否可以再次发送数据，当接收端继续可以接收数据时，发送端才会继续发送数据</p>\n<p>基于tcp窗口的反压机制缺点<br>1.单个task造成的反压，会阻断整个TM-TM的socket，连chekcpoint barrier 也无法发送<br>2.反压路径较长，导致生效延迟较大</p>\n<p>1.5 引入credit 机制实现反压，credit 反压机制是类似于tcp 窗口反压实现的另一种反压机制，resultsubpartition在发送数据时会带有resultbuff里面还存有的数据大小 backlog size，inputchanel在接收到时会计算自己当前还能接收到的数据大小，当inputchanel无法再接收数据时会将credit置为0，告诉result不能再接收消息。result每次发送消息时会检测当前自己的credit数据，当credit为0时 则不会再向netty发送数据从而实现反压机制</p>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><p>两个流join 必须有等值字段必须都在同一个窗口里面</p>\n<p>duplicate key update  mysql数据库的更新插入 合为一条sql</p>\n<p>并行度： <strong>算子级别</strong> &gt; <strong>env级别</strong> &gt; <strong>Client级别</strong> &gt; <strong>系统默认级别</strong></p>\n<p>在所有Task共享资源槽点名字相同，默认情况下 （pipline）<br>同一个job的同一个Task中的多个subTask不能在同一个slot槽中</p>\n<blockquote>\n<p>具有并行度的subtask 不能在一个slot槽中<br>对于同一个job，不同Task【阶段】的subTask可以在同一个资源槽中</p>\n</blockquote>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"Flink\"><a href=\"#Flink\" class=\"headerlink\" title=\"Flink\"></a>Flink</h1><h2 id=\"Application\"><a href=\"#Application\" class=\"headerlink\" title=\"Application\"></a>Application</h2><ul>\n<li>启动：flink run -c mainclasspath jarpath</li>\n<li>取消：flink cancel jobid</li>\n<li>停止：flink stop jobid<h3 id=\"Job\"><a href=\"#Job\" class=\"headerlink\" title=\"Job\"></a>Job</h3><h2 id=\"Task\"><a href=\"#Task\" class=\"headerlink\" title=\"Task\"></a>Task</h2>job中的一个阶段就是一个task，一个task包括链条连接的多个subtask，一个task运行在一个线程里面，task平分slot里面的内存资源共享slot里面的cpu资源<h2 id=\"SubTask\"><a href=\"#SubTask\" class=\"headerlink\" title=\"SubTask\"></a>SubTask</h2>flink job中最小执行单元<h2 id=\"算子\"><a href=\"#算子\" class=\"headerlink\" title=\"算子\"></a>算子</h2></li>\n</ul>\n<h3 id=\"Source\"><a href=\"#Source\" class=\"headerlink\" title=\"Source\"></a>Source</h3><p>使用EventTime，划分滚动窗口，如果使用的是并行的Source，例如KafkaSource，创建Kafka的Topic时有多个分区，每个Source的分区都要满足触发的条件，整个窗口才会被触发</p>\n<h3 id=\"Transformation\"><a href=\"#Transformation\" class=\"headerlink\" title=\"Transformation\"></a>Transformation</h3><ul>\n<li>map    //DataStream - &gt; DataStream</li>\n<li>flatMap    //DataStream -&gt; DataStream</li>\n<li>filter    //DataStream -&gt; DataStream</li>\n<li>keyBy    //DataStream -&gt; KeyStream<h3 id=\"Sink\"><a href=\"#Sink\" class=\"headerlink\" title=\"Sink\"></a>Sink</h3></li>\n</ul>\n<p>writeAsCsv必须是元组才能正常写入</p>\n<h2 id=\"WaterMark\"><a href=\"#WaterMark\" class=\"headerlink\" title=\"WaterMark\"></a>WaterMark</h2><p>决定一个窗口什么时候激活（触发），这时的窗口的最大长度为</p>\n<p>materMark&gt;=上一个窗口的结束边界就会触发窗口执行</p>\n<p>watermark是flink中窗口延迟触发的机制</p>\n<p>在每个算子内部都自己有一个事件时间时钟，事件时间时钟是根据watermark来更新的，。流入算子的数据可能是单分区也可能是多分区的，每个流入算子的分区端都会有一个自己的partition watermark标记，当该分区内进入新的高于之前watermark的watermark数据时，partition watermark标记才会被更新，task内部也维护一个task的watermark数据。如果某个分区的partition watermark &lt; task watermark，那么task watermark会更新为该partition watermark数据，然后把task 把当前更新的task watermark数据发向下游task。</p>\n<h3 id=\"AssignerWithPeriodicWatermarks\"><a href=\"#AssignerWithPeriodicWatermarks\" class=\"headerlink\" title=\"AssignerWithPeriodicWatermarks\"></a>AssignerWithPeriodicWatermarks</h3><p>windowmax=watermark+windowsize<br>waterMark=数据携带的时间（窗口中最大的时间）-延迟执行的时间</p>\n<p>周期性的生成watermark，定期向分区数据流中插入时间水印。默认周期时间为200毫秒，可以使用setAutoWatermakrIntaval（）来设置<br>BoundedOutOfOrdernessTimestampExtractor继承自AssignerWithPeriodicWatermarks 属于周期性watermark</p>\n<blockquote>\n<p>周期性数据水印在特定条件下可能会造成数据错误<br>例如：env.fromCollection(List((1, “a1”, 158324361000l), (1, “a2”, 158324369000l), (1, “a3”, 158324364000l), (1, “a4”, 158324361000l), (1, “a5”,158324365000l), (1, “a6”, 158324362000l), (1, “a7”,158324367000l)))<br>.assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor<a href=\"Time.milliseconds(0)\">(Int, String, Long)</a> {<br>override def extractTimestamp(element: (Int, String, Long)): Long = {<br>element._3<br>}<br>})<br>.keyBy(0)<br>.window(TumblingEventTimeWindows.of(Time.seconds(4)))<br>.sum(2)<br>.print()<br>当使用周期性水印AssignerWithPeriodicWatermarks时就会造成数据的错误计算<br>是因为周期性水印是定期产生的（默认200毫秒）但是在这个周期里可能会出现有多个数据已经过去，这多个数据用于一个水印从而造成数据计算错误</p>\n</blockquote>\n<h3 id=\"AssignerWithPunctuatedWatermarks\"><a href=\"#AssignerWithPunctuatedWatermarks\" class=\"headerlink\" title=\"AssignerWithPunctuatedWatermarks\"></a>AssignerWithPunctuatedWatermarks</h3><p>根据事件生成watermark。可以用于根据具体数据来生成watermark，</p>\n<h2 id=\"Window\"><a href=\"#Window\" class=\"headerlink\" title=\"Window\"></a>Window</h2><h3 id=\"Keyed-Window\"><a href=\"#Keyed-Window\" class=\"headerlink\" title=\"Keyed Window\"></a>Keyed Window</h3><p>使用keyby后流的窗口</p>\n<h4 id=\"GlobalWindow\"><a href=\"#GlobalWindow\" class=\"headerlink\" title=\"GlobalWindow\"></a>GlobalWindow</h4><h4 id=\"CountWindow\"><a href=\"#CountWindow\" class=\"headerlink\" title=\"CountWindow\"></a>CountWindow</h4><h4 id=\"TimeWindow\"><a href=\"#TimeWindow\" class=\"headerlink\" title=\"TimeWindow\"></a>TimeWindow</h4><ul>\n<li>Tumbling</li>\n<li>Sliding</li>\n<li>Session</li>\n</ul>\n<h3 id=\"Non-Keyed-Windows\"><a href=\"#Non-Keyed-Windows\" class=\"headerlink\" title=\"Non-Keyed Windows\"></a>Non-Keyed Windows</h3><p>未使用keyby后流的窗口</p>\n<h4 id=\"windowAll\"><a href=\"#windowAll\" class=\"headerlink\" title=\"windowAll\"></a>windowAll</h4><h2 id=\"Window-之后的算子\"><a href=\"#Window-之后的算子\" class=\"headerlink\" title=\"Window 之后的算子\"></a>Window 之后的算子</h2><h3 id=\"Trigger\"><a href=\"#Trigger\" class=\"headerlink\" title=\"Trigger\"></a>Trigger</h3><p>window 数据触发器，keyed or non-keyed window 都可以使用</p>\n<ul>\n<li>EventTimeTrigger：事件时间触发器</li>\n<li>ProcessingTimeTrigger：程序时间触发器</li>\n<li>CountTrigger：数量出发器。只发送窗口触发信号</li>\n<li>PurgingTrigger：代理模式触发器，发送窗口触发和数据清理信号<h3 id=\"Evictor\"><a href=\"#Evictor\" class=\"headerlink\" title=\"Evictor\"></a>Evictor</h3>window 数据剔除器，可以在window执行前或者执行后剔除window内的元素</li>\n<li>CountEvictor：数量剔除器。在Window中保留指定数量的元素，并从窗口头部开始丢弃其余元素。</li>\n<li>DeltaEvictor： 阈值剔除器。计算Window中最后一个元素与其余每个元素之间的增量，丢弃增量大于或等于阈值的元素。</li>\n<li>TimeEvictor：时间剔除器。保留Window中最近一段时间内的元素，并丢弃其余元素。<h3 id=\"AllowedLateness\"><a href=\"#AllowedLateness\" class=\"headerlink\" title=\"AllowedLateness\"></a>AllowedLateness</h3></li>\n</ul>\n<p>决定一个窗口什么时候销毁，window 延迟数据是否保留计算，可能会造成窗口的二次触发，会导致结果数据的更新，造成数据不一致。这时窗口的最大长度为windowmax=waterma+windowsize+allowedleteness</p>\n<h2 id=\"State\"><a href=\"#State\" class=\"headerlink\" title=\"State\"></a>State</h2><h3 id=\"Managed-State\"><a href=\"#Managed-State\" class=\"headerlink\" title=\"Managed State\"></a>Managed State</h3><h4 id=\"Operator-State\"><a href=\"#Operator-State\" class=\"headerlink\" title=\"Operator State\"></a>Operator State</h4><p>operator state 绑定到每个算子的实例上，各个实例拥有自己的state，一个实例无法获取同并行的其他实例的state数据</p>\n<p>operator state ：记录的是每一个分区的偏移量</p>\n<h4 id=\"Keyed-State\"><a href=\"#Keyed-State\" class=\"headerlink\" title=\"Keyed State\"></a>Keyed State</h4><p>keyedstate：在一个subtask中可能有多个state，一个组对应一个key的状态</p>\n<h3 id=\"Raw-State\"><a href=\"#Raw-State\" class=\"headerlink\" title=\"Raw State\"></a>Raw State</h3><h2 id=\"CheckPoint\"><a href=\"#CheckPoint\" class=\"headerlink\" title=\"CheckPoint\"></a>CheckPoint</h2><p>全自动程序管理，轻量快捷算子级数据快照</p>\n<p>开启flink checkpoint 后设置精准一次消费，kafka的offset会保存在savepoint设置的路径里面，还会降offset保存在kafka 特殊topic里面，如果程序重启时没有指定savepiont保存数据的地址会默认根据kafka 特殊topic保存的偏移量消费数据，可以设置不降offset保存在kafka 特殊topic里面使用，setCommitOffsetOnCheckpoints(false)</p>\n<p>开启检查点机制</p>\n<pre><code>// start a checkpoint every 1000 ms\nenv.enableCheckpointing(1000);\n\n// advanced options:\n\n// set mode to exactly-once (this is the default)\nenv.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n\n// make sure 500 ms of progress happen between checkpoints\nenv.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);\n\n// checkpoints have to complete within one minute, or are discarded\nenv.getCheckpointConfig().setCheckpointTimeout(60000);\n\n// allow only one checkpoint to be in progress at the same time\nenv.getCheckpointConfig().setMaxConcurrentCheckpoints(1);\n\n// enable externalized checkpoints which are retained after job cancellation\nenv.getCheckpointConfig().enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);\n\n// allow job recovery fallback to checkpoint when there is a more recent savepoint\nenv.getCheckpointConfig().setPreferCheckpointForRecovery(true);</code></pre><h3 id=\"Barrier\"><a href=\"#Barrier\" class=\"headerlink\" title=\"Barrier\"></a>Barrier</h3><p>算子checkpoint的依据，是exactly-once 和at-least-once语义的根据</p>\n<h2 id=\"SavePoint\"><a href=\"#SavePoint\" class=\"headerlink\" title=\"SavePoint\"></a>SavePoint</h2><p>人工参与管理的application级别的数据快照</p>\n<h3 id=\"手动保存数据快照\"><a href=\"#手动保存数据快照\" class=\"headerlink\" title=\"手动保存数据快照\"></a>手动保存数据快照</h3><ul>\n<li><p>flink stop jobid<br>停止job并保存快照，如果在flink-conf.yaml上配置了state.savepoints.dir，停止任务后会自动将快照保存。</p>\n</li>\n<li><p>flink stop jobid -p dirpath<br>停止job并将快照保存在dirpath</p>\n</li>\n<li><p>flink savepoint jobid [dirpath]<br>在不结束job的情况下保存快照。如果带有dirpath则会将快照保存在此目录否则会保存在默认配置的保存目录</p>\n<h3 id=\"从数据快照恢复程序\"><a href=\"#从数据快照恢复程序\" class=\"headerlink\" title=\"从数据快照恢复程序\"></a>从数据快照恢复程序</h3></li>\n<li><p>直接从savepoint目录恢复<br>flink run -s dirpath<br>从dirpath目录恢复程序</p>\n</li>\n<li><p>跳过无法恢复的算子恢复<br>flink run -s dirpath -n</p>\n</li>\n</ul>\n<h3 id=\"手动清除数据快照\"><a href=\"#手动清除数据快照\" class=\"headerlink\" title=\"手动清除数据快照\"></a>手动清除数据快照</h3><ul>\n<li>flink savepoint -d itemdirpath<br>手动将数据某个具体快照删除（itemdirpath 快照具体根目录）</li>\n</ul>\n<h2 id=\"CEP（Complex-Event-Processing）\"><a href=\"#CEP（Complex-Event-Processing）\" class=\"headerlink\" title=\"CEP（Complex Event Processing）\"></a>CEP（Complex Event Processing）</h2><p>NFA（Nondeterministic Finite Automaton）</p>\n<h2 id=\"反压\"><a href=\"#反压\" class=\"headerlink\" title=\"反压\"></a>反压</h2><p>flink 三层buff缓存（resultsubpartition，nettybuff，netty中通过高水位来控制buff是否还可以接收数据，socketbuff）</p>\n<p>1.5 之前是基于tcp 窗口的反压机制，发送端根据接收端返回的ack和windox size 来发送数据，当window size 为0时，发送端则不会再附上数据，而是发送一个zerowindow 的探测性数据来确定是否可以再次发送数据，当接收端继续可以接收数据时，发送端才会继续发送数据</p>\n<p>基于tcp窗口的反压机制缺点<br>1.单个task造成的反压，会阻断整个TM-TM的socket，连chekcpoint barrier 也无法发送<br>2.反压路径较长，导致生效延迟较大</p>\n<p>1.5 引入credit 机制实现反压，credit 反压机制是类似于tcp 窗口反压实现的另一种反压机制，resultsubpartition在发送数据时会带有resultbuff里面还存有的数据大小 backlog size，inputchanel在接收到时会计算自己当前还能接收到的数据大小，当inputchanel无法再接收数据时会将credit置为0，告诉result不能再接收消息。result每次发送消息时会检测当前自己的credit数据，当credit为0时 则不会再向netty发送数据从而实现反压机制</p>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><p>两个流join 必须有等值字段必须都在同一个窗口里面</p>\n<p>duplicate key update  mysql数据库的更新插入 合为一条sql</p>\n<p>并行度： <strong>算子级别</strong> &gt; <strong>env级别</strong> &gt; <strong>Client级别</strong> &gt; <strong>系统默认级别</strong></p>\n<p>在所有Task共享资源槽点名字相同，默认情况下 （pipline）<br>同一个job的同一个Task中的多个subTask不能在同一个slot槽中</p>\n<blockquote>\n<p>具有并行度的subtask 不能在一个slot槽中<br>对于同一个job，不同Task【阶段】的subTask可以在同一个资源槽中</p>\n</blockquote>\n"},{"title":"Redis","date":"2019-12-26T15:50:21.000Z","_content":"\n# Redis（Remote Dictionary Server）\n\n\n\n## Redis 运行快速的原因\n\n1. 完全基于内存操作\n\n2. 数据结构简单，数据操作也简单\n\n3. 使用多路I/O复用模型\n\n## 数据类型\n### String\n#### 单条操作\n1. 增：set\tkey\tvalue\n2. 查：get\tkey\n3. 删：del\tkey\n\n#### 多条操作\n1. 增：mset\tkey\tvalue\t[key1\tvalue1]\n2. 查：mget\tkey\t[key1]\n\n#### 其他命令\n1. strlen\tkey\t//获取字符串长度\n2. append\tkey\tvalue\t//有则追加，无则新建\n3. setnx\tkey\tvalue\t//不存在就设置，存在就不设置\n4. incr\tkey\t//自增 1\n5. incrby\tkey\tnum  //给key的值增加num（int 类型），num 正数则为加，num 为负数 则为减\n6. incrbyfloat\tkey\tnum\t//给key的值增加num（float 类型）\n7. decr\tkey  //自减 1\n8. decrby\tkey\tnum\t//给key的值减num\n9. setex\tkey\tsecond\tvalue\t//设置key的值为value存活时间为second秒\n10. psetex\tkey\tmillisecond\tvalue\t//设置key的值为value存活时间为millisecond毫秒\n\n>注：\n>字符串值最大值为512m\n\n### Hash\n和字符串相似，可理解为字符串厘米套字符串\n\n#### 单条操作\n1. 增：hset\tkey\tfield\tvalue\t\n2. 查：hget\tkey\tfield\n3. 删：hdel\tkey\tfield\n\n#### 多条操作\n1. 增：hmset\tkey\tfield\tvalue\t[field1\tvalue2]\n2. 查：hmget\tkey\tfield\t[field1]\n\n#### 其他命令\n1. hgetall\tkey\t//获取key的全部的值\n2. hlen\tkey\t//获取key的值的数量\n3. hexists\tkey\tfield\t//是否存在field\n4. hkeys\tkey\t//所有key的字段（field）\n5. hvals\tkey\t//所有key的值 \n6. hincrby\tkey\tfield\tnum\t//给key的field的值增加num （num 为int值）\n7. hincrybyfloat\tkey\tfield\tnum\t//给key的field的值增加num （num 为float）\n8. hsetnx\tkey\tfield\tvalue\t//存在不设置，不存在设置\n\n>注：\n>1. Hash类型的value只能存字符串，不允许再嵌套其他类型，如果数据为空为Nil\n>2. 每个Hash可以存储$2^{32}-1$个键的值对\n>3. Hash类型十分贴近对象的数据存储，并且可以灵活添加、删除对象属性。但Hash类型设计并不是存在量而设计的，切记不可滥用，更不可将Hash作为对象列表使用\n>4. hgetall操作可以获取全部属性，如果内部field过多，遍历整体数据时效率会降低，很有可能成为数据访问的瓶颈\n\n### String存储对象（Json） VS Hash存储对象\n1. String存在对象讲究整体性，以读为主\n2. Hash存储对象讲究分散性，以写为主\n\n### List\n讲究顺序\n#### 添加\n1. lpush\tkey\tvalue\t[value1]\t//从左添加\n2. rpush\tkey\tvalue\t[value1]\t//从右添加\n\n#### 获取\n1. lrange\tkey\tstart\tstop\n2. lindex\tkey\tindex\n3. llen\tkey\n\n#### 获取并移除\n1. lpop\tkey\t//从左出\n2. rpop\tkey\t//从右出\n\n~~~tex\nlpush\tlist\ta\tb\tc\n结果：c b a\n===============================\nrpush list a\tb\tc\n结果：a b c\n===============================\nlist=[a\tb\tc]\nlpop list\n结果：b c\n===============================\nlist=[a b c]\nrpop list\n结果：a b\n===============================\n~~~\n\n>注：\n>1. list中保存的数据都是String，数据总量是有限的，最多$2^{32}-1$个元素\n>2. list具有索引的概念，但是操作数据时通常以队列的形式进行入队出队操作（或以栈的形式进行入栈出栈操作）\n>3. 当stop的值为-1时，获取的是全部数据\n>4. list对数据进行分页操作，通常第一页的数据使的信息来自list，其他页面的数据通过数据形式进行加载\n\n### Set\n#### 命令\n1. 增：sadd\tkey\tmember\t[member1]\n2. 查：smembers\tkey\n3. 删：srem\tkey\tmember\t[member1]\n4. 获取总量：scard\tkey\n5. 判定是否存在：sismember\tkey\tmember\n6. 随机获取（原集合保留）：srandmember\tkey\t[count]\n7. 随机获取（原集合不保留）：spop\tkey\n8. 集合交集：sinter\tkey\tkey1\tkey2\n9. 集合并集：sunion\tkey\tkey1\tkey2\n10. 集合差集：sdiff\tkey\tkey1\tkey2\n11. 存储集合交集：sinterstore\tdestination\tkey\tkey1\tkey2\n12. 存储集合并集：sunionstore\tdestination\tkey\tkey1\tkey2\n13. 存储集合差集：sdiffstore\tdestination\tkey\tkey1\tkey2\n14. 集合元素移动：smove\tsource\tdestination\tmember\n\n### Sorted_Set\n#### 命令\n1. 增：zadd\tkey\tscore\tmember\t[score1\tmember1]\n2. 删：zrem\tkey\tmember\t[member1]\n3. 获取全部（正序）：zrange\tkey\tstart\tstop\t[withscores]\n4. 获取全部（倒序）：zrevrange\tkey\tstart\tstop\t[withscores]\n5. 按条件查（正序）：zrangebyscore\tkey\tmin\tmax\t[withscore\tlimit]\n6. 按条件查（倒序）：zrevrangebyscore\tkey\tmax\tmin\t[withscore\tlimit]\n7. 按条件删除（索引）：zremrangebyrank\tkey\tstart\tstop\n8. 按条件删除（积分）：zremrangebyscore\tkey\tmin\tmax\n9. 获取集合总量：zcard\tkey\t|\tzcount\tkey\tmin\tmax\n10. 存储集合交集：\tzinterstore\tdestination\tnumkeys\tkey\tkey1\n11. 存储集合并集：zunionstore\tdestination\tnumkeys\tkey\tkey1\n12. 获取索引（正序）：zrank\tkey\tmember\n13. 获取索引（倒序）：zrevrank\tkey\tmember\n14. score值获取：zscore\tkey\tmember\n15. score值修改：zincrby\tkey\tnum\tmember\n\n## Key的操作\n### 命令\n1. 删除：del\tkey\n2. 判断是否存在：exists\tkey\n3. 获取key类型：type\tkey\n4. 指定有效期：\n\t1.\texpire\tkey\tseconds\n\t2.\tpexpire\tkey\tmilliseconds\n\t3.\texpireat\tkey\ttimestamp\n\t4.\tpexpireat\tkey\tmilliseconds-timestamp\n5. 获取有效期：\n\t1.\tttl\tkey\n\t2.\tpttl\tkey\n6.\t设置永久：persist\tkey\n7.\t查询key：key\tpattern\t//*,?,[]\n8.\t重命名：rename\tkey\tnewkey\t|\trenamenx\tkey\tnewkey\n9.\t对key排序：sort\n\n## 数据库操作\n### 命令\n1. 选择数据库：select\tindex\n2. 数据移动：move\tkey\tdb\n3. 数据库大小：dbsize\n4. 数据清除：\n\t1. 单库删除：flushdb\n\t2. 多库删除：flushall\n\n## 持久化\n### RDB（Relational Database）\n#### 保存数据\n1. 指令（前台）：save\t//阻塞 立即保存\n2. 指令（后台）：bgsave\t//不立即执行\n3. 配置：save\tsecond\tchanges\t//用bgsave执行操作\n\n#### 常用配置项\n1. 数据文件名称：dbfilename\tdump.rdb\t//默认\n2. 数据保存路径：dir\n3. 是否开启压缩：rdbcompression\tyes\t//默认\n4. 是否开启格式检查：rdbchecksum\tyes|no //默认no\n\n#### RDB 持久化优点\n1. RDB是一个紧凑压缩的二进制文件，存储效率高\n2. RDB存储的是Redis在某个时间点的数据快照，非常适用于数据备份全量复制等场景\n3. RDB恢复数据速度比AOF快\n\n#### RDB应用\n服务器中每X小时执行bgsave备份，并将RDB文件拷贝到远程机器中，用于灾难恢复\n\n#### RDB持久化缺点\n1. RDB方式无论是执行命令还是进行配置，无法做到实时持久化，具有较大可能丢失数据\n2. bgsave每次运行要执行fork操作创建子进程，要牺牲一些性能\n3. Redis的众多版本中未进行RDB文件格式的版本统一，有可能出现各个版本服务器之间数据格式无法兼容\n4. 存储数量较大时，效率较低\n5. 大数据量下的I／O性能较低\n6. 基于fork创建子进程，内存产生额外消耗\n7. 宕机带来的数据丢失风险\n\n### AOF（Append Only File）\n#### 保存数据策略\n每次：always\n每秒：everysec\n系统控制：no\n\n#### 配置\n1. 是否开启：appendonly\tyes|no\t//默认no\n2. 保存策略：appendfsync\talways|everysec|no\n3. 重写：\n\t手动：bgrewriteaof\n\t自动：\n\t\tauto-aof-rewrite-min-size\tsize\n\t\tauto-aof-rewrite-percentage\tpercentage\n\n### RDB 与 AOF 如何选择\n对数据非常敏感，建议使用默认的AOF持久化方案\n\tAOF策略使用everysec，每秒fsync一次，该策略仍可保持很好性能，出现问题最多丢失一秒内的数据\n数据呈现阶段有效性，建议使用RDB持久化方案\n\t数据可以做到阶段内无丢失，且恢复较快，阶段点数据恢复通常使用RDB方案\n>注意：\n>AOF文件存储体积较大，恢复速度较慢\n>利用RDB使用线紧凑的数据持久化会使Redis性能降低\n\n综合：\n1. RDB与AOF选择实际上是在一种权衡，每种都有利有弊\n2. 如果不能承受分钟内的数据丢失，对业务数据非常敏感，选用AOF\n3. 如果能承受分钟内的数据丢失，且追求大数据集的恢复速度选用RDB\n4. 灾难恢复选用RDB\n5. 双保险策略，同时开启RDB和AOF，重启后Redis优先使用AOF来恢复数据，降低丢失数据量\n\n## 事务\nRedis事务不具有回滚机制\n\n### 命令\n1. 开启：multi\n2. 结束：exec\n3. 中断：discard\n\n### 事务中的错误\n#### 命令错误\n执行事务过程中输入的命令出现错误，Redis会结束事务不再执行，并报出错误的命令\n\n#### 操作错误\n执行事务过程中输入的命令正确，其他操作错误，事务中其他命令正常执行，错误操作报错\n\n## 锁\nRedis中锁和事务是相搭配使用的，可解决对key的监控\n\n### 命令\n加锁：watch\tkey\t[key1]\n解锁：unwatch  //取消掉所有key的监控\n\n## 数据删除策略\n当key过期后执行数据删除的策略\n### 定时删除（即可删除）\n创建一个具有时效性的key时，同时会创建一个定时器来监控该key是否过期，当key过期后立即进行key的删除\n\n优点：节约内存，到时就会进行删除，快速释放占用空间\n缺点：CPU压力大影响Redis响应时间和吞吐量\n总结：用处理器性能换取存储空间\n\n### 惰性删除\n当一个具有实效性的key过期后不会有删除操作，直到下一次调用时会先检查该key是否过期，如果过期则进行删除操作，并返回nil（该key不存在）\n\n优点：节约CPU性能，发现必须删除的时候才会删除\n缺点：内存压力大，出现长期占用内存的数据\n总结：用存储空间换取处理器性能\n\n### 定期删除\nRedis会根据设置的参数，定期对具有时效性的key进行清理工作，它是定时删除和惰性删除的结合者，既不像定时删除会立即进行删除给予CPU压力，也不会像惰性删除给予内存压力\n\n#### 步骤\n1. Redis启动服务器初始化时，读取配置server.hz的值（默认为10）\n2. 每秒钟执行server.hz次serverCron（）服务（serverCron【服务器级别】->databasesCron【数据库级别】->activeExpireCyle【活跃数据级别】）\n3. acitveExpireCyle（）对每个expires[*]（数据库）逐一进行检测，每次执行250ms／server.hz\n4. 对某个expires[*]检测时，随机挑选w个key进行检测\n\t1. 如果key超时，删除key\n\t2. 如果一轮中删除的kye的数量>w*25%，循环该过程\n\t3. 如果一轮中删除的可以的数量<=w*25%检查下一个expires[*],0-15（所有的数据库）循环\n>W=ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP\n>参数current_db用于记录activeExpireCyle进入哪个expires[*]执行\n\n## 数据淘汰策略\n当内存到达最大内存限制时进行的数据淘汰策略\n\n数据驱逐淘汰策略配置依据，使用info命令输出相关监控信息，查新缓存hit 命中次数和miss的次数，根据业务调优\n### 配置\n1. 最大可用内存：maxmemory\t//默认为0，一般设置全部内存50%以上\n2. 每次选取带删除数据个数：maxmemory-samples\t//采用随机获取方式\n3. 删除策略：maxmemory-policy\t//达到最大内存后，对被选取带数据进行的删除策略\n\n### 检测易失数据集（可能会过期数据server.db[i].expires）\n1. volatile-lru：挑选最近最少使用的数据淘汰（最近数据中使用时间离当前最远的数据）。**常用**\n2. volatile-lfu：挑选最近使用次数最少的数据淘汰（最近数据中使用次数最少的数据）\n3. volatile-ttl：挑选将要过期数据淘汰\n4. volatile-random：任意挑选数据淘汰\n\n>ttl：time to live\n>lru：least recently\tused\n>lfu：least frequently\tused\n\n### 检测全库数据（所有数据集server.db[i].dict）\n1. allkeys-lru：挑选最近最少使用的数据淘汰\n2. allkeys-lfu：挑选最近使用次数最少的数据淘汰\n3. allkeys-random：任意挑选数据淘汰\n\n### 放弃数据驱逐\nno-enviction\t//禁止驱逐数据\n4.0中默认策略，会引发OOM\n\n## 服务器基本配置\n1. 设置服务器守护进程方式：daemonize\tyes|no\n2. 绑定地址：bing\t127.0.0.1\n3. 设置服务器端口：port\t6379\n4. 设置数据库数量：databases\t16\n5. 设置服务器日志级别：loglevel\tdebug|verbose|notice|warning\n6. 日志文件名称：logfile\t端口号.log\n7. 设置客户端最大连接数：maxclients\t0\n8. 客户端闲置最大等待时长：timeout\t0\n\n## 高级数据类型\n### Bitmaps\n标记统计\n\n#### 命令\n1. 获取：getbit\tkey\toffset\n2. 设置：setbit\tkey\toffset\tvalue\t// 0 或 1\n3. 交、并、或异\n\tbitop\top\tdestkey\tkey1\tkey2\n\top：\n\t\t交：and\n\t\t并：or\n\t\t非：not\n\t\t异或：xor\n4. 统计指定key中1的数量：bitcount\tkey\t[start\tend]\n\n### HyperLoglog\n基数统计\n#### 命令\n1. 添加：pfadd\tkey\telement\t[element1]\n2. 统计：pfcount\tkey\t[key1]\n3. 合并：pfmerge\tdestkey\tsourcekey\t[sourcekey1]\n\n### GEO\n距离计算（只计算水平距离）\n\n#### 命令\n1. 添加：geoadd\tkey\tlongitude\tlatitude\tmember\t[longitude1\tlatitude1\tmember1]\n2. 获取：geopos\tkey\tmember\t[member1]\n3. 计算距离：geodist\tkey\tmember1\tmember2\t[count]\n4. 根据坐标求范围内数据：georadius\tkey\tlongitude\tlatitude\tradius\tm|km|ft|mi\n5. 根据点求范围内的数据：georadusbymember\tkey\tmember\tradius\tm|km|ft|mi\n6. 获取指定点对应的hash值：geohash\tkey\tmember\t[member1]\t\n\n## 主从复制\n### 创建链接\n+ 方式一：客户端发指令：slaveof\tmasterip masterport\n+\t方式二：参数启动：redis-server\t--slaveof\tmasterip masterport\n+\t方式三：服务器配置：slaveof\tmasterip masterport\n\n### 数据同步 \n#### 全量复制\n从：发送指令（psync2）\n主：执行bgsave\n主：第一个slave链接时，创建命令缓冲区\n主：生成RDB文件，通过socket发送给slave\n从：接收RDB文件，清空自己数据，执行RDB文件恢复过程\n\n#### 部分复制\n从：发送命令告知RDB恢复完成\n主：发送复制缓冲区信息\n从：接收信息，执行bgsavewriteaof，恢复数据\n\n## 哨兵模式\n### 配置\nsentinel.conf\n\n启动：redis-sentinel\n\n## 集群（cluster）\n### 配置\n1. 开启：cluster-enabled\tyes|no\n2. 配置文件名称：cluster-config-file\tfilename\n3. 节点超时时间：cluster-node-timeout\tmilliseconds\n4. master链接slave最小数：cluster-migration-barrier\tcount\n\n### 命令\n1. 查看节点信息：cluster nodes\n2. 从一个节点Redis，切换其主节点：cluster\treplicate\tmasterip\n3. 新增主节点：cluster meet\tip:port\n4. 忽略一个节点：cluster\tfoeget\tid\n5. 手动故障转移：cluster\tfailover\n\n","source":"notes/Redis.md","raw":"---\ntitle: Redis\ndate: 2019-12-26 23:50:21\n---\n\n# Redis（Remote Dictionary Server）\n\n\n\n## Redis 运行快速的原因\n\n1. 完全基于内存操作\n\n2. 数据结构简单，数据操作也简单\n\n3. 使用多路I/O复用模型\n\n## 数据类型\n### String\n#### 单条操作\n1. 增：set\tkey\tvalue\n2. 查：get\tkey\n3. 删：del\tkey\n\n#### 多条操作\n1. 增：mset\tkey\tvalue\t[key1\tvalue1]\n2. 查：mget\tkey\t[key1]\n\n#### 其他命令\n1. strlen\tkey\t//获取字符串长度\n2. append\tkey\tvalue\t//有则追加，无则新建\n3. setnx\tkey\tvalue\t//不存在就设置，存在就不设置\n4. incr\tkey\t//自增 1\n5. incrby\tkey\tnum  //给key的值增加num（int 类型），num 正数则为加，num 为负数 则为减\n6. incrbyfloat\tkey\tnum\t//给key的值增加num（float 类型）\n7. decr\tkey  //自减 1\n8. decrby\tkey\tnum\t//给key的值减num\n9. setex\tkey\tsecond\tvalue\t//设置key的值为value存活时间为second秒\n10. psetex\tkey\tmillisecond\tvalue\t//设置key的值为value存活时间为millisecond毫秒\n\n>注：\n>字符串值最大值为512m\n\n### Hash\n和字符串相似，可理解为字符串厘米套字符串\n\n#### 单条操作\n1. 增：hset\tkey\tfield\tvalue\t\n2. 查：hget\tkey\tfield\n3. 删：hdel\tkey\tfield\n\n#### 多条操作\n1. 增：hmset\tkey\tfield\tvalue\t[field1\tvalue2]\n2. 查：hmget\tkey\tfield\t[field1]\n\n#### 其他命令\n1. hgetall\tkey\t//获取key的全部的值\n2. hlen\tkey\t//获取key的值的数量\n3. hexists\tkey\tfield\t//是否存在field\n4. hkeys\tkey\t//所有key的字段（field）\n5. hvals\tkey\t//所有key的值 \n6. hincrby\tkey\tfield\tnum\t//给key的field的值增加num （num 为int值）\n7. hincrybyfloat\tkey\tfield\tnum\t//给key的field的值增加num （num 为float）\n8. hsetnx\tkey\tfield\tvalue\t//存在不设置，不存在设置\n\n>注：\n>1. Hash类型的value只能存字符串，不允许再嵌套其他类型，如果数据为空为Nil\n>2. 每个Hash可以存储$2^{32}-1$个键的值对\n>3. Hash类型十分贴近对象的数据存储，并且可以灵活添加、删除对象属性。但Hash类型设计并不是存在量而设计的，切记不可滥用，更不可将Hash作为对象列表使用\n>4. hgetall操作可以获取全部属性，如果内部field过多，遍历整体数据时效率会降低，很有可能成为数据访问的瓶颈\n\n### String存储对象（Json） VS Hash存储对象\n1. String存在对象讲究整体性，以读为主\n2. Hash存储对象讲究分散性，以写为主\n\n### List\n讲究顺序\n#### 添加\n1. lpush\tkey\tvalue\t[value1]\t//从左添加\n2. rpush\tkey\tvalue\t[value1]\t//从右添加\n\n#### 获取\n1. lrange\tkey\tstart\tstop\n2. lindex\tkey\tindex\n3. llen\tkey\n\n#### 获取并移除\n1. lpop\tkey\t//从左出\n2. rpop\tkey\t//从右出\n\n~~~tex\nlpush\tlist\ta\tb\tc\n结果：c b a\n===============================\nrpush list a\tb\tc\n结果：a b c\n===============================\nlist=[a\tb\tc]\nlpop list\n结果：b c\n===============================\nlist=[a b c]\nrpop list\n结果：a b\n===============================\n~~~\n\n>注：\n>1. list中保存的数据都是String，数据总量是有限的，最多$2^{32}-1$个元素\n>2. list具有索引的概念，但是操作数据时通常以队列的形式进行入队出队操作（或以栈的形式进行入栈出栈操作）\n>3. 当stop的值为-1时，获取的是全部数据\n>4. list对数据进行分页操作，通常第一页的数据使的信息来自list，其他页面的数据通过数据形式进行加载\n\n### Set\n#### 命令\n1. 增：sadd\tkey\tmember\t[member1]\n2. 查：smembers\tkey\n3. 删：srem\tkey\tmember\t[member1]\n4. 获取总量：scard\tkey\n5. 判定是否存在：sismember\tkey\tmember\n6. 随机获取（原集合保留）：srandmember\tkey\t[count]\n7. 随机获取（原集合不保留）：spop\tkey\n8. 集合交集：sinter\tkey\tkey1\tkey2\n9. 集合并集：sunion\tkey\tkey1\tkey2\n10. 集合差集：sdiff\tkey\tkey1\tkey2\n11. 存储集合交集：sinterstore\tdestination\tkey\tkey1\tkey2\n12. 存储集合并集：sunionstore\tdestination\tkey\tkey1\tkey2\n13. 存储集合差集：sdiffstore\tdestination\tkey\tkey1\tkey2\n14. 集合元素移动：smove\tsource\tdestination\tmember\n\n### Sorted_Set\n#### 命令\n1. 增：zadd\tkey\tscore\tmember\t[score1\tmember1]\n2. 删：zrem\tkey\tmember\t[member1]\n3. 获取全部（正序）：zrange\tkey\tstart\tstop\t[withscores]\n4. 获取全部（倒序）：zrevrange\tkey\tstart\tstop\t[withscores]\n5. 按条件查（正序）：zrangebyscore\tkey\tmin\tmax\t[withscore\tlimit]\n6. 按条件查（倒序）：zrevrangebyscore\tkey\tmax\tmin\t[withscore\tlimit]\n7. 按条件删除（索引）：zremrangebyrank\tkey\tstart\tstop\n8. 按条件删除（积分）：zremrangebyscore\tkey\tmin\tmax\n9. 获取集合总量：zcard\tkey\t|\tzcount\tkey\tmin\tmax\n10. 存储集合交集：\tzinterstore\tdestination\tnumkeys\tkey\tkey1\n11. 存储集合并集：zunionstore\tdestination\tnumkeys\tkey\tkey1\n12. 获取索引（正序）：zrank\tkey\tmember\n13. 获取索引（倒序）：zrevrank\tkey\tmember\n14. score值获取：zscore\tkey\tmember\n15. score值修改：zincrby\tkey\tnum\tmember\n\n## Key的操作\n### 命令\n1. 删除：del\tkey\n2. 判断是否存在：exists\tkey\n3. 获取key类型：type\tkey\n4. 指定有效期：\n\t1.\texpire\tkey\tseconds\n\t2.\tpexpire\tkey\tmilliseconds\n\t3.\texpireat\tkey\ttimestamp\n\t4.\tpexpireat\tkey\tmilliseconds-timestamp\n5. 获取有效期：\n\t1.\tttl\tkey\n\t2.\tpttl\tkey\n6.\t设置永久：persist\tkey\n7.\t查询key：key\tpattern\t//*,?,[]\n8.\t重命名：rename\tkey\tnewkey\t|\trenamenx\tkey\tnewkey\n9.\t对key排序：sort\n\n## 数据库操作\n### 命令\n1. 选择数据库：select\tindex\n2. 数据移动：move\tkey\tdb\n3. 数据库大小：dbsize\n4. 数据清除：\n\t1. 单库删除：flushdb\n\t2. 多库删除：flushall\n\n## 持久化\n### RDB（Relational Database）\n#### 保存数据\n1. 指令（前台）：save\t//阻塞 立即保存\n2. 指令（后台）：bgsave\t//不立即执行\n3. 配置：save\tsecond\tchanges\t//用bgsave执行操作\n\n#### 常用配置项\n1. 数据文件名称：dbfilename\tdump.rdb\t//默认\n2. 数据保存路径：dir\n3. 是否开启压缩：rdbcompression\tyes\t//默认\n4. 是否开启格式检查：rdbchecksum\tyes|no //默认no\n\n#### RDB 持久化优点\n1. RDB是一个紧凑压缩的二进制文件，存储效率高\n2. RDB存储的是Redis在某个时间点的数据快照，非常适用于数据备份全量复制等场景\n3. RDB恢复数据速度比AOF快\n\n#### RDB应用\n服务器中每X小时执行bgsave备份，并将RDB文件拷贝到远程机器中，用于灾难恢复\n\n#### RDB持久化缺点\n1. RDB方式无论是执行命令还是进行配置，无法做到实时持久化，具有较大可能丢失数据\n2. bgsave每次运行要执行fork操作创建子进程，要牺牲一些性能\n3. Redis的众多版本中未进行RDB文件格式的版本统一，有可能出现各个版本服务器之间数据格式无法兼容\n4. 存储数量较大时，效率较低\n5. 大数据量下的I／O性能较低\n6. 基于fork创建子进程，内存产生额外消耗\n7. 宕机带来的数据丢失风险\n\n### AOF（Append Only File）\n#### 保存数据策略\n每次：always\n每秒：everysec\n系统控制：no\n\n#### 配置\n1. 是否开启：appendonly\tyes|no\t//默认no\n2. 保存策略：appendfsync\talways|everysec|no\n3. 重写：\n\t手动：bgrewriteaof\n\t自动：\n\t\tauto-aof-rewrite-min-size\tsize\n\t\tauto-aof-rewrite-percentage\tpercentage\n\n### RDB 与 AOF 如何选择\n对数据非常敏感，建议使用默认的AOF持久化方案\n\tAOF策略使用everysec，每秒fsync一次，该策略仍可保持很好性能，出现问题最多丢失一秒内的数据\n数据呈现阶段有效性，建议使用RDB持久化方案\n\t数据可以做到阶段内无丢失，且恢复较快，阶段点数据恢复通常使用RDB方案\n>注意：\n>AOF文件存储体积较大，恢复速度较慢\n>利用RDB使用线紧凑的数据持久化会使Redis性能降低\n\n综合：\n1. RDB与AOF选择实际上是在一种权衡，每种都有利有弊\n2. 如果不能承受分钟内的数据丢失，对业务数据非常敏感，选用AOF\n3. 如果能承受分钟内的数据丢失，且追求大数据集的恢复速度选用RDB\n4. 灾难恢复选用RDB\n5. 双保险策略，同时开启RDB和AOF，重启后Redis优先使用AOF来恢复数据，降低丢失数据量\n\n## 事务\nRedis事务不具有回滚机制\n\n### 命令\n1. 开启：multi\n2. 结束：exec\n3. 中断：discard\n\n### 事务中的错误\n#### 命令错误\n执行事务过程中输入的命令出现错误，Redis会结束事务不再执行，并报出错误的命令\n\n#### 操作错误\n执行事务过程中输入的命令正确，其他操作错误，事务中其他命令正常执行，错误操作报错\n\n## 锁\nRedis中锁和事务是相搭配使用的，可解决对key的监控\n\n### 命令\n加锁：watch\tkey\t[key1]\n解锁：unwatch  //取消掉所有key的监控\n\n## 数据删除策略\n当key过期后执行数据删除的策略\n### 定时删除（即可删除）\n创建一个具有时效性的key时，同时会创建一个定时器来监控该key是否过期，当key过期后立即进行key的删除\n\n优点：节约内存，到时就会进行删除，快速释放占用空间\n缺点：CPU压力大影响Redis响应时间和吞吐量\n总结：用处理器性能换取存储空间\n\n### 惰性删除\n当一个具有实效性的key过期后不会有删除操作，直到下一次调用时会先检查该key是否过期，如果过期则进行删除操作，并返回nil（该key不存在）\n\n优点：节约CPU性能，发现必须删除的时候才会删除\n缺点：内存压力大，出现长期占用内存的数据\n总结：用存储空间换取处理器性能\n\n### 定期删除\nRedis会根据设置的参数，定期对具有时效性的key进行清理工作，它是定时删除和惰性删除的结合者，既不像定时删除会立即进行删除给予CPU压力，也不会像惰性删除给予内存压力\n\n#### 步骤\n1. Redis启动服务器初始化时，读取配置server.hz的值（默认为10）\n2. 每秒钟执行server.hz次serverCron（）服务（serverCron【服务器级别】->databasesCron【数据库级别】->activeExpireCyle【活跃数据级别】）\n3. acitveExpireCyle（）对每个expires[*]（数据库）逐一进行检测，每次执行250ms／server.hz\n4. 对某个expires[*]检测时，随机挑选w个key进行检测\n\t1. 如果key超时，删除key\n\t2. 如果一轮中删除的kye的数量>w*25%，循环该过程\n\t3. 如果一轮中删除的可以的数量<=w*25%检查下一个expires[*],0-15（所有的数据库）循环\n>W=ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP\n>参数current_db用于记录activeExpireCyle进入哪个expires[*]执行\n\n## 数据淘汰策略\n当内存到达最大内存限制时进行的数据淘汰策略\n\n数据驱逐淘汰策略配置依据，使用info命令输出相关监控信息，查新缓存hit 命中次数和miss的次数，根据业务调优\n### 配置\n1. 最大可用内存：maxmemory\t//默认为0，一般设置全部内存50%以上\n2. 每次选取带删除数据个数：maxmemory-samples\t//采用随机获取方式\n3. 删除策略：maxmemory-policy\t//达到最大内存后，对被选取带数据进行的删除策略\n\n### 检测易失数据集（可能会过期数据server.db[i].expires）\n1. volatile-lru：挑选最近最少使用的数据淘汰（最近数据中使用时间离当前最远的数据）。**常用**\n2. volatile-lfu：挑选最近使用次数最少的数据淘汰（最近数据中使用次数最少的数据）\n3. volatile-ttl：挑选将要过期数据淘汰\n4. volatile-random：任意挑选数据淘汰\n\n>ttl：time to live\n>lru：least recently\tused\n>lfu：least frequently\tused\n\n### 检测全库数据（所有数据集server.db[i].dict）\n1. allkeys-lru：挑选最近最少使用的数据淘汰\n2. allkeys-lfu：挑选最近使用次数最少的数据淘汰\n3. allkeys-random：任意挑选数据淘汰\n\n### 放弃数据驱逐\nno-enviction\t//禁止驱逐数据\n4.0中默认策略，会引发OOM\n\n## 服务器基本配置\n1. 设置服务器守护进程方式：daemonize\tyes|no\n2. 绑定地址：bing\t127.0.0.1\n3. 设置服务器端口：port\t6379\n4. 设置数据库数量：databases\t16\n5. 设置服务器日志级别：loglevel\tdebug|verbose|notice|warning\n6. 日志文件名称：logfile\t端口号.log\n7. 设置客户端最大连接数：maxclients\t0\n8. 客户端闲置最大等待时长：timeout\t0\n\n## 高级数据类型\n### Bitmaps\n标记统计\n\n#### 命令\n1. 获取：getbit\tkey\toffset\n2. 设置：setbit\tkey\toffset\tvalue\t// 0 或 1\n3. 交、并、或异\n\tbitop\top\tdestkey\tkey1\tkey2\n\top：\n\t\t交：and\n\t\t并：or\n\t\t非：not\n\t\t异或：xor\n4. 统计指定key中1的数量：bitcount\tkey\t[start\tend]\n\n### HyperLoglog\n基数统计\n#### 命令\n1. 添加：pfadd\tkey\telement\t[element1]\n2. 统计：pfcount\tkey\t[key1]\n3. 合并：pfmerge\tdestkey\tsourcekey\t[sourcekey1]\n\n### GEO\n距离计算（只计算水平距离）\n\n#### 命令\n1. 添加：geoadd\tkey\tlongitude\tlatitude\tmember\t[longitude1\tlatitude1\tmember1]\n2. 获取：geopos\tkey\tmember\t[member1]\n3. 计算距离：geodist\tkey\tmember1\tmember2\t[count]\n4. 根据坐标求范围内数据：georadius\tkey\tlongitude\tlatitude\tradius\tm|km|ft|mi\n5. 根据点求范围内的数据：georadusbymember\tkey\tmember\tradius\tm|km|ft|mi\n6. 获取指定点对应的hash值：geohash\tkey\tmember\t[member1]\t\n\n## 主从复制\n### 创建链接\n+ 方式一：客户端发指令：slaveof\tmasterip masterport\n+\t方式二：参数启动：redis-server\t--slaveof\tmasterip masterport\n+\t方式三：服务器配置：slaveof\tmasterip masterport\n\n### 数据同步 \n#### 全量复制\n从：发送指令（psync2）\n主：执行bgsave\n主：第一个slave链接时，创建命令缓冲区\n主：生成RDB文件，通过socket发送给slave\n从：接收RDB文件，清空自己数据，执行RDB文件恢复过程\n\n#### 部分复制\n从：发送命令告知RDB恢复完成\n主：发送复制缓冲区信息\n从：接收信息，执行bgsavewriteaof，恢复数据\n\n## 哨兵模式\n### 配置\nsentinel.conf\n\n启动：redis-sentinel\n\n## 集群（cluster）\n### 配置\n1. 开启：cluster-enabled\tyes|no\n2. 配置文件名称：cluster-config-file\tfilename\n3. 节点超时时间：cluster-node-timeout\tmilliseconds\n4. master链接slave最小数：cluster-migration-barrier\tcount\n\n### 命令\n1. 查看节点信息：cluster nodes\n2. 从一个节点Redis，切换其主节点：cluster\treplicate\tmasterip\n3. 新增主节点：cluster meet\tip:port\n4. 忽略一个节点：cluster\tfoeget\tid\n5. 手动故障转移：cluster\tfailover\n\n","updated":"2020-03-18T11:52:16.000Z","path":"notes/Redis.html","comments":1,"layout":"page","_id":"ck7ycejhf000pjr643h65uq3a","content":"<h1 id=\"Redis（Remote-Dictionary-Server）\"><a href=\"#Redis（Remote-Dictionary-Server）\" class=\"headerlink\" title=\"Redis（Remote Dictionary Server）\"></a>Redis（Remote Dictionary Server）</h1><h2 id=\"Redis-运行快速的原因\"><a href=\"#Redis-运行快速的原因\" class=\"headerlink\" title=\"Redis 运行快速的原因\"></a>Redis 运行快速的原因</h2><ol>\n<li><p>完全基于内存操作</p>\n</li>\n<li><p>数据结构简单，数据操作也简单</p>\n</li>\n<li><p>使用多路I/O复用模型</p>\n</li>\n</ol>\n<h2 id=\"数据类型\"><a href=\"#数据类型\" class=\"headerlink\" title=\"数据类型\"></a>数据类型</h2><h3 id=\"String\"><a href=\"#String\" class=\"headerlink\" title=\"String\"></a>String</h3><h4 id=\"单条操作\"><a href=\"#单条操作\" class=\"headerlink\" title=\"单条操作\"></a>单条操作</h4><ol>\n<li>增：set    key    value</li>\n<li>查：get    key</li>\n<li>删：del    key</li>\n</ol>\n<h4 id=\"多条操作\"><a href=\"#多条操作\" class=\"headerlink\" title=\"多条操作\"></a>多条操作</h4><ol>\n<li>增：mset    key    value    [key1    value1]</li>\n<li>查：mget    key    [key1]</li>\n</ol>\n<h4 id=\"其他命令\"><a href=\"#其他命令\" class=\"headerlink\" title=\"其他命令\"></a>其他命令</h4><ol>\n<li>strlen    key    //获取字符串长度</li>\n<li>append    key    value    //有则追加，无则新建</li>\n<li>setnx    key    value    //不存在就设置，存在就不设置</li>\n<li>incr    key    //自增 1</li>\n<li>incrby    key    num  //给key的值增加num（int 类型），num 正数则为加，num 为负数 则为减</li>\n<li>incrbyfloat    key    num    //给key的值增加num（float 类型）</li>\n<li>decr    key  //自减 1</li>\n<li>decrby    key    num    //给key的值减num</li>\n<li>setex    key    second    value    //设置key的值为value存活时间为second秒</li>\n<li>psetex    key    millisecond    value    //设置key的值为value存活时间为millisecond毫秒</li>\n</ol>\n<blockquote>\n<p>注：<br>字符串值最大值为512m</p>\n</blockquote>\n<h3 id=\"Hash\"><a href=\"#Hash\" class=\"headerlink\" title=\"Hash\"></a>Hash</h3><p>和字符串相似，可理解为字符串厘米套字符串</p>\n<h4 id=\"单条操作-1\"><a href=\"#单条操作-1\" class=\"headerlink\" title=\"单条操作\"></a>单条操作</h4><ol>\n<li>增：hset    key    field    value    </li>\n<li>查：hget    key    field</li>\n<li>删：hdel    key    field</li>\n</ol>\n<h4 id=\"多条操作-1\"><a href=\"#多条操作-1\" class=\"headerlink\" title=\"多条操作\"></a>多条操作</h4><ol>\n<li>增：hmset    key    field    value    [field1    value2]</li>\n<li>查：hmget    key    field    [field1]</li>\n</ol>\n<h4 id=\"其他命令-1\"><a href=\"#其他命令-1\" class=\"headerlink\" title=\"其他命令\"></a>其他命令</h4><ol>\n<li>hgetall    key    //获取key的全部的值</li>\n<li>hlen    key    //获取key的值的数量</li>\n<li>hexists    key    field    //是否存在field</li>\n<li>hkeys    key    //所有key的字段（field）</li>\n<li>hvals    key    //所有key的值 </li>\n<li>hincrby    key    field    num    //给key的field的值增加num （num 为int值）</li>\n<li>hincrybyfloat    key    field    num    //给key的field的值增加num （num 为float）</li>\n<li>hsetnx    key    field    value    //存在不设置，不存在设置</li>\n</ol>\n<blockquote>\n<p>注：</p>\n<ol>\n<li>Hash类型的value只能存字符串，不允许再嵌套其他类型，如果数据为空为Nil</li>\n<li>每个Hash可以存储$2^{32}-1$个键的值对</li>\n<li>Hash类型十分贴近对象的数据存储，并且可以灵活添加、删除对象属性。但Hash类型设计并不是存在量而设计的，切记不可滥用，更不可将Hash作为对象列表使用</li>\n<li>hgetall操作可以获取全部属性，如果内部field过多，遍历整体数据时效率会降低，很有可能成为数据访问的瓶颈</li>\n</ol>\n</blockquote>\n<h3 id=\"String存储对象（Json）-VS-Hash存储对象\"><a href=\"#String存储对象（Json）-VS-Hash存储对象\" class=\"headerlink\" title=\"String存储对象（Json） VS Hash存储对象\"></a>String存储对象（Json） VS Hash存储对象</h3><ol>\n<li>String存在对象讲究整体性，以读为主</li>\n<li>Hash存储对象讲究分散性，以写为主</li>\n</ol>\n<h3 id=\"List\"><a href=\"#List\" class=\"headerlink\" title=\"List\"></a>List</h3><p>讲究顺序</p>\n<h4 id=\"添加\"><a href=\"#添加\" class=\"headerlink\" title=\"添加\"></a>添加</h4><ol>\n<li>lpush    key    value    [value1]    //从左添加</li>\n<li>rpush    key    value    [value1]    //从右添加</li>\n</ol>\n<h4 id=\"获取\"><a href=\"#获取\" class=\"headerlink\" title=\"获取\"></a>获取</h4><ol>\n<li>lrange    key    start    stop</li>\n<li>lindex    key    index</li>\n<li>llen    key</li>\n</ol>\n<h4 id=\"获取并移除\"><a href=\"#获取并移除\" class=\"headerlink\" title=\"获取并移除\"></a>获取并移除</h4><ol>\n<li>lpop    key    //从左出</li>\n<li>rpop    key    //从右出</li>\n</ol>\n<pre class=\" language-tex\"><code class=\"language-tex\">lpush    list    a    b    c\n结果：c b a\n===============================\nrpush list a    b    c\n结果：a b c\n===============================\nlist=[a    b    c]\nlpop list\n结果：b c\n===============================\nlist=[a b c]\nrpop list\n结果：a b\n===============================</code></pre>\n<blockquote>\n<p>注：</p>\n<ol>\n<li>list中保存的数据都是String，数据总量是有限的，最多$2^{32}-1$个元素</li>\n<li>list具有索引的概念，但是操作数据时通常以队列的形式进行入队出队操作（或以栈的形式进行入栈出栈操作）</li>\n<li>当stop的值为-1时，获取的是全部数据</li>\n<li>list对数据进行分页操作，通常第一页的数据使的信息来自list，其他页面的数据通过数据形式进行加载</li>\n</ol>\n</blockquote>\n<h3 id=\"Set\"><a href=\"#Set\" class=\"headerlink\" title=\"Set\"></a>Set</h3><h4 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>增：sadd    key    member    [member1]</li>\n<li>查：smembers    key</li>\n<li>删：srem    key    member    [member1]</li>\n<li>获取总量：scard    key</li>\n<li>判定是否存在：sismember    key    member</li>\n<li>随机获取（原集合保留）：srandmember    key    [count]</li>\n<li>随机获取（原集合不保留）：spop    key</li>\n<li>集合交集：sinter    key    key1    key2</li>\n<li>集合并集：sunion    key    key1    key2</li>\n<li>集合差集：sdiff    key    key1    key2</li>\n<li>存储集合交集：sinterstore    destination    key    key1    key2</li>\n<li>存储集合并集：sunionstore    destination    key    key1    key2</li>\n<li>存储集合差集：sdiffstore    destination    key    key1    key2</li>\n<li>集合元素移动：smove    source    destination    member</li>\n</ol>\n<h3 id=\"Sorted-Set\"><a href=\"#Sorted-Set\" class=\"headerlink\" title=\"Sorted_Set\"></a>Sorted_Set</h3><h4 id=\"命令-1\"><a href=\"#命令-1\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>增：zadd    key    score    member    [score1    member1]</li>\n<li>删：zrem    key    member    [member1]</li>\n<li>获取全部（正序）：zrange    key    start    stop    [withscores]</li>\n<li>获取全部（倒序）：zrevrange    key    start    stop    [withscores]</li>\n<li>按条件查（正序）：zrangebyscore    key    min    max    [withscore    limit]</li>\n<li>按条件查（倒序）：zrevrangebyscore    key    max    min    [withscore    limit]</li>\n<li>按条件删除（索引）：zremrangebyrank    key    start    stop</li>\n<li>按条件删除（积分）：zremrangebyscore    key    min    max</li>\n<li>获取集合总量：zcard    key    |    zcount    key    min    max</li>\n<li>存储集合交集：    zinterstore    destination    numkeys    key    key1</li>\n<li>存储集合并集：zunionstore    destination    numkeys    key    key1</li>\n<li>获取索引（正序）：zrank    key    member</li>\n<li>获取索引（倒序）：zrevrank    key    member</li>\n<li>score值获取：zscore    key    member</li>\n<li>score值修改：zincrby    key    num    member</li>\n</ol>\n<h2 id=\"Key的操作\"><a href=\"#Key的操作\" class=\"headerlink\" title=\"Key的操作\"></a>Key的操作</h2><h3 id=\"命令-2\"><a href=\"#命令-2\" class=\"headerlink\" title=\"命令\"></a>命令</h3><ol>\n<li>删除：del    key</li>\n<li>判断是否存在：exists    key</li>\n<li>获取key类型：type    key</li>\n<li>指定有效期：<ol>\n<li>expire    key    seconds</li>\n<li>pexpire    key    milliseconds</li>\n<li>expireat    key    timestamp</li>\n<li>pexpireat    key    milliseconds-timestamp</li>\n</ol>\n</li>\n<li>获取有效期：<ol>\n<li>ttl    key</li>\n<li>pttl    key</li>\n</ol>\n</li>\n<li>设置永久：persist    key</li>\n<li>查询key：key    pattern    //*,?,[]</li>\n<li>重命名：rename    key    newkey    |    renamenx    key    newkey</li>\n<li>对key排序：sort</li>\n</ol>\n<h2 id=\"数据库操作\"><a href=\"#数据库操作\" class=\"headerlink\" title=\"数据库操作\"></a>数据库操作</h2><h3 id=\"命令-3\"><a href=\"#命令-3\" class=\"headerlink\" title=\"命令\"></a>命令</h3><ol>\n<li>选择数据库：select    index</li>\n<li>数据移动：move    key    db</li>\n<li>数据库大小：dbsize</li>\n<li>数据清除：<ol>\n<li>单库删除：flushdb</li>\n<li>多库删除：flushall</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"持久化\"><a href=\"#持久化\" class=\"headerlink\" title=\"持久化\"></a>持久化</h2><h3 id=\"RDB（Relational-Database）\"><a href=\"#RDB（Relational-Database）\" class=\"headerlink\" title=\"RDB（Relational Database）\"></a>RDB（Relational Database）</h3><h4 id=\"保存数据\"><a href=\"#保存数据\" class=\"headerlink\" title=\"保存数据\"></a>保存数据</h4><ol>\n<li>指令（前台）：save    //阻塞 立即保存</li>\n<li>指令（后台）：bgsave    //不立即执行</li>\n<li>配置：save    second    changes    //用bgsave执行操作</li>\n</ol>\n<h4 id=\"常用配置项\"><a href=\"#常用配置项\" class=\"headerlink\" title=\"常用配置项\"></a>常用配置项</h4><ol>\n<li>数据文件名称：dbfilename    dump.rdb    //默认</li>\n<li>数据保存路径：dir</li>\n<li>是否开启压缩：rdbcompression    yes    //默认</li>\n<li>是否开启格式检查：rdbchecksum    yes|no //默认no</li>\n</ol>\n<h4 id=\"RDB-持久化优点\"><a href=\"#RDB-持久化优点\" class=\"headerlink\" title=\"RDB 持久化优点\"></a>RDB 持久化优点</h4><ol>\n<li>RDB是一个紧凑压缩的二进制文件，存储效率高</li>\n<li>RDB存储的是Redis在某个时间点的数据快照，非常适用于数据备份全量复制等场景</li>\n<li>RDB恢复数据速度比AOF快</li>\n</ol>\n<h4 id=\"RDB应用\"><a href=\"#RDB应用\" class=\"headerlink\" title=\"RDB应用\"></a>RDB应用</h4><p>服务器中每X小时执行bgsave备份，并将RDB文件拷贝到远程机器中，用于灾难恢复</p>\n<h4 id=\"RDB持久化缺点\"><a href=\"#RDB持久化缺点\" class=\"headerlink\" title=\"RDB持久化缺点\"></a>RDB持久化缺点</h4><ol>\n<li>RDB方式无论是执行命令还是进行配置，无法做到实时持久化，具有较大可能丢失数据</li>\n<li>bgsave每次运行要执行fork操作创建子进程，要牺牲一些性能</li>\n<li>Redis的众多版本中未进行RDB文件格式的版本统一，有可能出现各个版本服务器之间数据格式无法兼容</li>\n<li>存储数量较大时，效率较低</li>\n<li>大数据量下的I／O性能较低</li>\n<li>基于fork创建子进程，内存产生额外消耗</li>\n<li>宕机带来的数据丢失风险</li>\n</ol>\n<h3 id=\"AOF（Append-Only-File）\"><a href=\"#AOF（Append-Only-File）\" class=\"headerlink\" title=\"AOF（Append Only File）\"></a>AOF（Append Only File）</h3><h4 id=\"保存数据策略\"><a href=\"#保存数据策略\" class=\"headerlink\" title=\"保存数据策略\"></a>保存数据策略</h4><p>每次：always<br>每秒：everysec<br>系统控制：no</p>\n<h4 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h4><ol>\n<li>是否开启：appendonly    yes|no    //默认no</li>\n<li>保存策略：appendfsync    always|everysec|no</li>\n<li>重写：<br> 手动：bgrewriteaof<br> 自动：<pre><code> auto-aof-rewrite-min-size    size\n auto-aof-rewrite-percentage    percentage</code></pre></li>\n</ol>\n<h3 id=\"RDB-与-AOF-如何选择\"><a href=\"#RDB-与-AOF-如何选择\" class=\"headerlink\" title=\"RDB 与 AOF 如何选择\"></a>RDB 与 AOF 如何选择</h3><p>对数据非常敏感，建议使用默认的AOF持久化方案<br>    AOF策略使用everysec，每秒fsync一次，该策略仍可保持很好性能，出现问题最多丢失一秒内的数据<br>数据呈现阶段有效性，建议使用RDB持久化方案<br>    数据可以做到阶段内无丢失，且恢复较快，阶段点数据恢复通常使用RDB方案</p>\n<blockquote>\n<p>注意：<br>AOF文件存储体积较大，恢复速度较慢<br>利用RDB使用线紧凑的数据持久化会使Redis性能降低</p>\n</blockquote>\n<p>综合：</p>\n<ol>\n<li>RDB与AOF选择实际上是在一种权衡，每种都有利有弊</li>\n<li>如果不能承受分钟内的数据丢失，对业务数据非常敏感，选用AOF</li>\n<li>如果能承受分钟内的数据丢失，且追求大数据集的恢复速度选用RDB</li>\n<li>灾难恢复选用RDB</li>\n<li>双保险策略，同时开启RDB和AOF，重启后Redis优先使用AOF来恢复数据，降低丢失数据量</li>\n</ol>\n<h2 id=\"事务\"><a href=\"#事务\" class=\"headerlink\" title=\"事务\"></a>事务</h2><p>Redis事务不具有回滚机制</p>\n<h3 id=\"命令-4\"><a href=\"#命令-4\" class=\"headerlink\" title=\"命令\"></a>命令</h3><ol>\n<li>开启：multi</li>\n<li>结束：exec</li>\n<li>中断：discard</li>\n</ol>\n<h3 id=\"事务中的错误\"><a href=\"#事务中的错误\" class=\"headerlink\" title=\"事务中的错误\"></a>事务中的错误</h3><h4 id=\"命令错误\"><a href=\"#命令错误\" class=\"headerlink\" title=\"命令错误\"></a>命令错误</h4><p>执行事务过程中输入的命令出现错误，Redis会结束事务不再执行，并报出错误的命令</p>\n<h4 id=\"操作错误\"><a href=\"#操作错误\" class=\"headerlink\" title=\"操作错误\"></a>操作错误</h4><p>执行事务过程中输入的命令正确，其他操作错误，事务中其他命令正常执行，错误操作报错</p>\n<h2 id=\"锁\"><a href=\"#锁\" class=\"headerlink\" title=\"锁\"></a>锁</h2><p>Redis中锁和事务是相搭配使用的，可解决对key的监控</p>\n<h3 id=\"命令-5\"><a href=\"#命令-5\" class=\"headerlink\" title=\"命令\"></a>命令</h3><p>加锁：watch    key    [key1]<br>解锁：unwatch  //取消掉所有key的监控</p>\n<h2 id=\"数据删除策略\"><a href=\"#数据删除策略\" class=\"headerlink\" title=\"数据删除策略\"></a>数据删除策略</h2><p>当key过期后执行数据删除的策略</p>\n<h3 id=\"定时删除（即可删除）\"><a href=\"#定时删除（即可删除）\" class=\"headerlink\" title=\"定时删除（即可删除）\"></a>定时删除（即可删除）</h3><p>创建一个具有时效性的key时，同时会创建一个定时器来监控该key是否过期，当key过期后立即进行key的删除</p>\n<p>优点：节约内存，到时就会进行删除，快速释放占用空间<br>缺点：CPU压力大影响Redis响应时间和吞吐量<br>总结：用处理器性能换取存储空间</p>\n<h3 id=\"惰性删除\"><a href=\"#惰性删除\" class=\"headerlink\" title=\"惰性删除\"></a>惰性删除</h3><p>当一个具有实效性的key过期后不会有删除操作，直到下一次调用时会先检查该key是否过期，如果过期则进行删除操作，并返回nil（该key不存在）</p>\n<p>优点：节约CPU性能，发现必须删除的时候才会删除<br>缺点：内存压力大，出现长期占用内存的数据<br>总结：用存储空间换取处理器性能</p>\n<h3 id=\"定期删除\"><a href=\"#定期删除\" class=\"headerlink\" title=\"定期删除\"></a>定期删除</h3><p>Redis会根据设置的参数，定期对具有时效性的key进行清理工作，它是定时删除和惰性删除的结合者，既不像定时删除会立即进行删除给予CPU压力，也不会像惰性删除给予内存压力</p>\n<h4 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h4><ol>\n<li>Redis启动服务器初始化时，读取配置server.hz的值（默认为10）</li>\n<li>每秒钟执行server.hz次serverCron（）服务（serverCron【服务器级别】-&gt;databasesCron【数据库级别】-&gt;activeExpireCyle【活跃数据级别】）</li>\n<li>acitveExpireCyle（）对每个expires[*]（数据库）逐一进行检测，每次执行250ms／server.hz</li>\n<li>对某个expires[*]检测时，随机挑选w个key进行检测<ol>\n<li>如果key超时，删除key</li>\n<li>如果一轮中删除的kye的数量&gt;w*25%，循环该过程</li>\n<li>如果一轮中删除的可以的数量&lt;=w<em>25%检查下一个expires[</em>],0-15（所有的数据库）循环<blockquote>\n<p>W=ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP<br>参数current_db用于记录activeExpireCyle进入哪个expires[*]执行</p>\n</blockquote>\n</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"数据淘汰策略\"><a href=\"#数据淘汰策略\" class=\"headerlink\" title=\"数据淘汰策略\"></a>数据淘汰策略</h2><p>当内存到达最大内存限制时进行的数据淘汰策略</p>\n<p>数据驱逐淘汰策略配置依据，使用info命令输出相关监控信息，查新缓存hit 命中次数和miss的次数，根据业务调优</p>\n<h3 id=\"配置-1\"><a href=\"#配置-1\" class=\"headerlink\" title=\"配置\"></a>配置</h3><ol>\n<li>最大可用内存：maxmemory    //默认为0，一般设置全部内存50%以上</li>\n<li>每次选取带删除数据个数：maxmemory-samples    //采用随机获取方式</li>\n<li>删除策略：maxmemory-policy    //达到最大内存后，对被选取带数据进行的删除策略</li>\n</ol>\n<h3 id=\"检测易失数据集（可能会过期数据server-db-i-expires）\"><a href=\"#检测易失数据集（可能会过期数据server-db-i-expires）\" class=\"headerlink\" title=\"检测易失数据集（可能会过期数据server.db[i].expires）\"></a>检测易失数据集（可能会过期数据server.db[i].expires）</h3><ol>\n<li>volatile-lru：挑选最近最少使用的数据淘汰（最近数据中使用时间离当前最远的数据）。<strong>常用</strong></li>\n<li>volatile-lfu：挑选最近使用次数最少的数据淘汰（最近数据中使用次数最少的数据）</li>\n<li>volatile-ttl：挑选将要过期数据淘汰</li>\n<li>volatile-random：任意挑选数据淘汰</li>\n</ol>\n<blockquote>\n<p>ttl：time to live<br>lru：least recently    used<br>lfu：least frequently    used</p>\n</blockquote>\n<h3 id=\"检测全库数据（所有数据集server-db-i-dict）\"><a href=\"#检测全库数据（所有数据集server-db-i-dict）\" class=\"headerlink\" title=\"检测全库数据（所有数据集server.db[i].dict）\"></a>检测全库数据（所有数据集server.db[i].dict）</h3><ol>\n<li>allkeys-lru：挑选最近最少使用的数据淘汰</li>\n<li>allkeys-lfu：挑选最近使用次数最少的数据淘汰</li>\n<li>allkeys-random：任意挑选数据淘汰</li>\n</ol>\n<h3 id=\"放弃数据驱逐\"><a href=\"#放弃数据驱逐\" class=\"headerlink\" title=\"放弃数据驱逐\"></a>放弃数据驱逐</h3><p>no-enviction    //禁止驱逐数据<br>4.0中默认策略，会引发OOM</p>\n<h2 id=\"服务器基本配置\"><a href=\"#服务器基本配置\" class=\"headerlink\" title=\"服务器基本配置\"></a>服务器基本配置</h2><ol>\n<li>设置服务器守护进程方式：daemonize    yes|no</li>\n<li>绑定地址：bing    127.0.0.1</li>\n<li>设置服务器端口：port    6379</li>\n<li>设置数据库数量：databases    16</li>\n<li>设置服务器日志级别：loglevel    debug|verbose|notice|warning</li>\n<li>日志文件名称：logfile    端口号.log</li>\n<li>设置客户端最大连接数：maxclients    0</li>\n<li>客户端闲置最大等待时长：timeout    0</li>\n</ol>\n<h2 id=\"高级数据类型\"><a href=\"#高级数据类型\" class=\"headerlink\" title=\"高级数据类型\"></a>高级数据类型</h2><h3 id=\"Bitmaps\"><a href=\"#Bitmaps\" class=\"headerlink\" title=\"Bitmaps\"></a>Bitmaps</h3><p>标记统计</p>\n<h4 id=\"命令-6\"><a href=\"#命令-6\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>获取：getbit    key    offset</li>\n<li>设置：setbit    key    offset    value    // 0 或 1</li>\n<li>交、并、或异<br> bitop    op    destkey    key1    key2<br> op：<pre><code> 交：and\n 并：or\n 非：not\n 异或：xor</code></pre></li>\n<li>统计指定key中1的数量：bitcount    key    [start    end]</li>\n</ol>\n<h3 id=\"HyperLoglog\"><a href=\"#HyperLoglog\" class=\"headerlink\" title=\"HyperLoglog\"></a>HyperLoglog</h3><p>基数统计</p>\n<h4 id=\"命令-7\"><a href=\"#命令-7\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>添加：pfadd    key    element    [element1]</li>\n<li>统计：pfcount    key    [key1]</li>\n<li>合并：pfmerge    destkey    sourcekey    [sourcekey1]</li>\n</ol>\n<h3 id=\"GEO\"><a href=\"#GEO\" class=\"headerlink\" title=\"GEO\"></a>GEO</h3><p>距离计算（只计算水平距离）</p>\n<h4 id=\"命令-8\"><a href=\"#命令-8\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>添加：geoadd    key    longitude    latitude    member    [longitude1    latitude1    member1]</li>\n<li>获取：geopos    key    member    [member1]</li>\n<li>计算距离：geodist    key    member1    member2    [count]</li>\n<li>根据坐标求范围内数据：georadius    key    longitude    latitude    radius    m|km|ft|mi</li>\n<li>根据点求范围内的数据：georadusbymember    key    member    radius    m|km|ft|mi</li>\n<li>获取指定点对应的hash值：geohash    key    member    [member1]    </li>\n</ol>\n<h2 id=\"主从复制\"><a href=\"#主从复制\" class=\"headerlink\" title=\"主从复制\"></a>主从复制</h2><h3 id=\"创建链接\"><a href=\"#创建链接\" class=\"headerlink\" title=\"创建链接\"></a>创建链接</h3><ul>\n<li>方式一：客户端发指令：slaveof    masterip masterport</li>\n<li>方式二：参数启动：redis-server    –slaveof    masterip masterport</li>\n<li>方式三：服务器配置：slaveof    masterip masterport</li>\n</ul>\n<h3 id=\"数据同步\"><a href=\"#数据同步\" class=\"headerlink\" title=\"数据同步\"></a>数据同步</h3><h4 id=\"全量复制\"><a href=\"#全量复制\" class=\"headerlink\" title=\"全量复制\"></a>全量复制</h4><p>从：发送指令（psync2）<br>主：执行bgsave<br>主：第一个slave链接时，创建命令缓冲区<br>主：生成RDB文件，通过socket发送给slave<br>从：接收RDB文件，清空自己数据，执行RDB文件恢复过程</p>\n<h4 id=\"部分复制\"><a href=\"#部分复制\" class=\"headerlink\" title=\"部分复制\"></a>部分复制</h4><p>从：发送命令告知RDB恢复完成<br>主：发送复制缓冲区信息<br>从：接收信息，执行bgsavewriteaof，恢复数据</p>\n<h2 id=\"哨兵模式\"><a href=\"#哨兵模式\" class=\"headerlink\" title=\"哨兵模式\"></a>哨兵模式</h2><h3 id=\"配置-2\"><a href=\"#配置-2\" class=\"headerlink\" title=\"配置\"></a>配置</h3><p>sentinel.conf</p>\n<p>启动：redis-sentinel</p>\n<h2 id=\"集群（cluster）\"><a href=\"#集群（cluster）\" class=\"headerlink\" title=\"集群（cluster）\"></a>集群（cluster）</h2><h3 id=\"配置-3\"><a href=\"#配置-3\" class=\"headerlink\" title=\"配置\"></a>配置</h3><ol>\n<li>开启：cluster-enabled    yes|no</li>\n<li>配置文件名称：cluster-config-file    filename</li>\n<li>节点超时时间：cluster-node-timeout    milliseconds</li>\n<li>master链接slave最小数：cluster-migration-barrier    count</li>\n</ol>\n<h3 id=\"命令-9\"><a href=\"#命令-9\" class=\"headerlink\" title=\"命令\"></a>命令</h3><ol>\n<li>查看节点信息：cluster nodes</li>\n<li>从一个节点Redis，切换其主节点：cluster    replicate    masterip</li>\n<li>新增主节点：cluster meet    ip:port</li>\n<li>忽略一个节点：cluster    foeget    id</li>\n<li>手动故障转移：cluster    failover</li>\n</ol>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"Redis（Remote-Dictionary-Server）\"><a href=\"#Redis（Remote-Dictionary-Server）\" class=\"headerlink\" title=\"Redis（Remote Dictionary Server）\"></a>Redis（Remote Dictionary Server）</h1><h2 id=\"Redis-运行快速的原因\"><a href=\"#Redis-运行快速的原因\" class=\"headerlink\" title=\"Redis 运行快速的原因\"></a>Redis 运行快速的原因</h2><ol>\n<li><p>完全基于内存操作</p>\n</li>\n<li><p>数据结构简单，数据操作也简单</p>\n</li>\n<li><p>使用多路I/O复用模型</p>\n</li>\n</ol>\n<h2 id=\"数据类型\"><a href=\"#数据类型\" class=\"headerlink\" title=\"数据类型\"></a>数据类型</h2><h3 id=\"String\"><a href=\"#String\" class=\"headerlink\" title=\"String\"></a>String</h3><h4 id=\"单条操作\"><a href=\"#单条操作\" class=\"headerlink\" title=\"单条操作\"></a>单条操作</h4><ol>\n<li>增：set    key    value</li>\n<li>查：get    key</li>\n<li>删：del    key</li>\n</ol>\n<h4 id=\"多条操作\"><a href=\"#多条操作\" class=\"headerlink\" title=\"多条操作\"></a>多条操作</h4><ol>\n<li>增：mset    key    value    [key1    value1]</li>\n<li>查：mget    key    [key1]</li>\n</ol>\n<h4 id=\"其他命令\"><a href=\"#其他命令\" class=\"headerlink\" title=\"其他命令\"></a>其他命令</h4><ol>\n<li>strlen    key    //获取字符串长度</li>\n<li>append    key    value    //有则追加，无则新建</li>\n<li>setnx    key    value    //不存在就设置，存在就不设置</li>\n<li>incr    key    //自增 1</li>\n<li>incrby    key    num  //给key的值增加num（int 类型），num 正数则为加，num 为负数 则为减</li>\n<li>incrbyfloat    key    num    //给key的值增加num（float 类型）</li>\n<li>decr    key  //自减 1</li>\n<li>decrby    key    num    //给key的值减num</li>\n<li>setex    key    second    value    //设置key的值为value存活时间为second秒</li>\n<li>psetex    key    millisecond    value    //设置key的值为value存活时间为millisecond毫秒</li>\n</ol>\n<blockquote>\n<p>注：<br>字符串值最大值为512m</p>\n</blockquote>\n<h3 id=\"Hash\"><a href=\"#Hash\" class=\"headerlink\" title=\"Hash\"></a>Hash</h3><p>和字符串相似，可理解为字符串厘米套字符串</p>\n<h4 id=\"单条操作-1\"><a href=\"#单条操作-1\" class=\"headerlink\" title=\"单条操作\"></a>单条操作</h4><ol>\n<li>增：hset    key    field    value    </li>\n<li>查：hget    key    field</li>\n<li>删：hdel    key    field</li>\n</ol>\n<h4 id=\"多条操作-1\"><a href=\"#多条操作-1\" class=\"headerlink\" title=\"多条操作\"></a>多条操作</h4><ol>\n<li>增：hmset    key    field    value    [field1    value2]</li>\n<li>查：hmget    key    field    [field1]</li>\n</ol>\n<h4 id=\"其他命令-1\"><a href=\"#其他命令-1\" class=\"headerlink\" title=\"其他命令\"></a>其他命令</h4><ol>\n<li>hgetall    key    //获取key的全部的值</li>\n<li>hlen    key    //获取key的值的数量</li>\n<li>hexists    key    field    //是否存在field</li>\n<li>hkeys    key    //所有key的字段（field）</li>\n<li>hvals    key    //所有key的值 </li>\n<li>hincrby    key    field    num    //给key的field的值增加num （num 为int值）</li>\n<li>hincrybyfloat    key    field    num    //给key的field的值增加num （num 为float）</li>\n<li>hsetnx    key    field    value    //存在不设置，不存在设置</li>\n</ol>\n<blockquote>\n<p>注：</p>\n<ol>\n<li>Hash类型的value只能存字符串，不允许再嵌套其他类型，如果数据为空为Nil</li>\n<li>每个Hash可以存储$2^{32}-1$个键的值对</li>\n<li>Hash类型十分贴近对象的数据存储，并且可以灵活添加、删除对象属性。但Hash类型设计并不是存在量而设计的，切记不可滥用，更不可将Hash作为对象列表使用</li>\n<li>hgetall操作可以获取全部属性，如果内部field过多，遍历整体数据时效率会降低，很有可能成为数据访问的瓶颈</li>\n</ol>\n</blockquote>\n<h3 id=\"String存储对象（Json）-VS-Hash存储对象\"><a href=\"#String存储对象（Json）-VS-Hash存储对象\" class=\"headerlink\" title=\"String存储对象（Json） VS Hash存储对象\"></a>String存储对象（Json） VS Hash存储对象</h3><ol>\n<li>String存在对象讲究整体性，以读为主</li>\n<li>Hash存储对象讲究分散性，以写为主</li>\n</ol>\n<h3 id=\"List\"><a href=\"#List\" class=\"headerlink\" title=\"List\"></a>List</h3><p>讲究顺序</p>\n<h4 id=\"添加\"><a href=\"#添加\" class=\"headerlink\" title=\"添加\"></a>添加</h4><ol>\n<li>lpush    key    value    [value1]    //从左添加</li>\n<li>rpush    key    value    [value1]    //从右添加</li>\n</ol>\n<h4 id=\"获取\"><a href=\"#获取\" class=\"headerlink\" title=\"获取\"></a>获取</h4><ol>\n<li>lrange    key    start    stop</li>\n<li>lindex    key    index</li>\n<li>llen    key</li>\n</ol>\n<h4 id=\"获取并移除\"><a href=\"#获取并移除\" class=\"headerlink\" title=\"获取并移除\"></a>获取并移除</h4><ol>\n<li>lpop    key    //从左出</li>\n<li>rpop    key    //从右出</li>\n</ol>\n<pre><code class=\"tex\">lpush    list    a    b    c\n结果：c b a\n===============================\nrpush list a    b    c\n结果：a b c\n===============================\nlist=[a    b    c]\nlpop list\n结果：b c\n===============================\nlist=[a b c]\nrpop list\n结果：a b\n===============================</code></pre>\n<blockquote>\n<p>注：</p>\n<ol>\n<li>list中保存的数据都是String，数据总量是有限的，最多$2^{32}-1$个元素</li>\n<li>list具有索引的概念，但是操作数据时通常以队列的形式进行入队出队操作（或以栈的形式进行入栈出栈操作）</li>\n<li>当stop的值为-1时，获取的是全部数据</li>\n<li>list对数据进行分页操作，通常第一页的数据使的信息来自list，其他页面的数据通过数据形式进行加载</li>\n</ol>\n</blockquote>\n<h3 id=\"Set\"><a href=\"#Set\" class=\"headerlink\" title=\"Set\"></a>Set</h3><h4 id=\"命令\"><a href=\"#命令\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>增：sadd    key    member    [member1]</li>\n<li>查：smembers    key</li>\n<li>删：srem    key    member    [member1]</li>\n<li>获取总量：scard    key</li>\n<li>判定是否存在：sismember    key    member</li>\n<li>随机获取（原集合保留）：srandmember    key    [count]</li>\n<li>随机获取（原集合不保留）：spop    key</li>\n<li>集合交集：sinter    key    key1    key2</li>\n<li>集合并集：sunion    key    key1    key2</li>\n<li>集合差集：sdiff    key    key1    key2</li>\n<li>存储集合交集：sinterstore    destination    key    key1    key2</li>\n<li>存储集合并集：sunionstore    destination    key    key1    key2</li>\n<li>存储集合差集：sdiffstore    destination    key    key1    key2</li>\n<li>集合元素移动：smove    source    destination    member</li>\n</ol>\n<h3 id=\"Sorted-Set\"><a href=\"#Sorted-Set\" class=\"headerlink\" title=\"Sorted_Set\"></a>Sorted_Set</h3><h4 id=\"命令-1\"><a href=\"#命令-1\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>增：zadd    key    score    member    [score1    member1]</li>\n<li>删：zrem    key    member    [member1]</li>\n<li>获取全部（正序）：zrange    key    start    stop    [withscores]</li>\n<li>获取全部（倒序）：zrevrange    key    start    stop    [withscores]</li>\n<li>按条件查（正序）：zrangebyscore    key    min    max    [withscore    limit]</li>\n<li>按条件查（倒序）：zrevrangebyscore    key    max    min    [withscore    limit]</li>\n<li>按条件删除（索引）：zremrangebyrank    key    start    stop</li>\n<li>按条件删除（积分）：zremrangebyscore    key    min    max</li>\n<li>获取集合总量：zcard    key    |    zcount    key    min    max</li>\n<li>存储集合交集：    zinterstore    destination    numkeys    key    key1</li>\n<li>存储集合并集：zunionstore    destination    numkeys    key    key1</li>\n<li>获取索引（正序）：zrank    key    member</li>\n<li>获取索引（倒序）：zrevrank    key    member</li>\n<li>score值获取：zscore    key    member</li>\n<li>score值修改：zincrby    key    num    member</li>\n</ol>\n<h2 id=\"Key的操作\"><a href=\"#Key的操作\" class=\"headerlink\" title=\"Key的操作\"></a>Key的操作</h2><h3 id=\"命令-2\"><a href=\"#命令-2\" class=\"headerlink\" title=\"命令\"></a>命令</h3><ol>\n<li>删除：del    key</li>\n<li>判断是否存在：exists    key</li>\n<li>获取key类型：type    key</li>\n<li>指定有效期：<ol>\n<li>expire    key    seconds</li>\n<li>pexpire    key    milliseconds</li>\n<li>expireat    key    timestamp</li>\n<li>pexpireat    key    milliseconds-timestamp</li>\n</ol>\n</li>\n<li>获取有效期：<ol>\n<li>ttl    key</li>\n<li>pttl    key</li>\n</ol>\n</li>\n<li>设置永久：persist    key</li>\n<li>查询key：key    pattern    //*,?,[]</li>\n<li>重命名：rename    key    newkey    |    renamenx    key    newkey</li>\n<li>对key排序：sort</li>\n</ol>\n<h2 id=\"数据库操作\"><a href=\"#数据库操作\" class=\"headerlink\" title=\"数据库操作\"></a>数据库操作</h2><h3 id=\"命令-3\"><a href=\"#命令-3\" class=\"headerlink\" title=\"命令\"></a>命令</h3><ol>\n<li>选择数据库：select    index</li>\n<li>数据移动：move    key    db</li>\n<li>数据库大小：dbsize</li>\n<li>数据清除：<ol>\n<li>单库删除：flushdb</li>\n<li>多库删除：flushall</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"持久化\"><a href=\"#持久化\" class=\"headerlink\" title=\"持久化\"></a>持久化</h2><h3 id=\"RDB（Relational-Database）\"><a href=\"#RDB（Relational-Database）\" class=\"headerlink\" title=\"RDB（Relational Database）\"></a>RDB（Relational Database）</h3><h4 id=\"保存数据\"><a href=\"#保存数据\" class=\"headerlink\" title=\"保存数据\"></a>保存数据</h4><ol>\n<li>指令（前台）：save    //阻塞 立即保存</li>\n<li>指令（后台）：bgsave    //不立即执行</li>\n<li>配置：save    second    changes    //用bgsave执行操作</li>\n</ol>\n<h4 id=\"常用配置项\"><a href=\"#常用配置项\" class=\"headerlink\" title=\"常用配置项\"></a>常用配置项</h4><ol>\n<li>数据文件名称：dbfilename    dump.rdb    //默认</li>\n<li>数据保存路径：dir</li>\n<li>是否开启压缩：rdbcompression    yes    //默认</li>\n<li>是否开启格式检查：rdbchecksum    yes|no //默认no</li>\n</ol>\n<h4 id=\"RDB-持久化优点\"><a href=\"#RDB-持久化优点\" class=\"headerlink\" title=\"RDB 持久化优点\"></a>RDB 持久化优点</h4><ol>\n<li>RDB是一个紧凑压缩的二进制文件，存储效率高</li>\n<li>RDB存储的是Redis在某个时间点的数据快照，非常适用于数据备份全量复制等场景</li>\n<li>RDB恢复数据速度比AOF快</li>\n</ol>\n<h4 id=\"RDB应用\"><a href=\"#RDB应用\" class=\"headerlink\" title=\"RDB应用\"></a>RDB应用</h4><p>服务器中每X小时执行bgsave备份，并将RDB文件拷贝到远程机器中，用于灾难恢复</p>\n<h4 id=\"RDB持久化缺点\"><a href=\"#RDB持久化缺点\" class=\"headerlink\" title=\"RDB持久化缺点\"></a>RDB持久化缺点</h4><ol>\n<li>RDB方式无论是执行命令还是进行配置，无法做到实时持久化，具有较大可能丢失数据</li>\n<li>bgsave每次运行要执行fork操作创建子进程，要牺牲一些性能</li>\n<li>Redis的众多版本中未进行RDB文件格式的版本统一，有可能出现各个版本服务器之间数据格式无法兼容</li>\n<li>存储数量较大时，效率较低</li>\n<li>大数据量下的I／O性能较低</li>\n<li>基于fork创建子进程，内存产生额外消耗</li>\n<li>宕机带来的数据丢失风险</li>\n</ol>\n<h3 id=\"AOF（Append-Only-File）\"><a href=\"#AOF（Append-Only-File）\" class=\"headerlink\" title=\"AOF（Append Only File）\"></a>AOF（Append Only File）</h3><h4 id=\"保存数据策略\"><a href=\"#保存数据策略\" class=\"headerlink\" title=\"保存数据策略\"></a>保存数据策略</h4><p>每次：always<br>每秒：everysec<br>系统控制：no</p>\n<h4 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h4><ol>\n<li>是否开启：appendonly    yes|no    //默认no</li>\n<li>保存策略：appendfsync    always|everysec|no</li>\n<li>重写：<br> 手动：bgrewriteaof<br> 自动：<pre><code> auto-aof-rewrite-min-size    size\n auto-aof-rewrite-percentage    percentage</code></pre></li>\n</ol>\n<h3 id=\"RDB-与-AOF-如何选择\"><a href=\"#RDB-与-AOF-如何选择\" class=\"headerlink\" title=\"RDB 与 AOF 如何选择\"></a>RDB 与 AOF 如何选择</h3><p>对数据非常敏感，建议使用默认的AOF持久化方案<br>    AOF策略使用everysec，每秒fsync一次，该策略仍可保持很好性能，出现问题最多丢失一秒内的数据<br>数据呈现阶段有效性，建议使用RDB持久化方案<br>    数据可以做到阶段内无丢失，且恢复较快，阶段点数据恢复通常使用RDB方案</p>\n<blockquote>\n<p>注意：<br>AOF文件存储体积较大，恢复速度较慢<br>利用RDB使用线紧凑的数据持久化会使Redis性能降低</p>\n</blockquote>\n<p>综合：</p>\n<ol>\n<li>RDB与AOF选择实际上是在一种权衡，每种都有利有弊</li>\n<li>如果不能承受分钟内的数据丢失，对业务数据非常敏感，选用AOF</li>\n<li>如果能承受分钟内的数据丢失，且追求大数据集的恢复速度选用RDB</li>\n<li>灾难恢复选用RDB</li>\n<li>双保险策略，同时开启RDB和AOF，重启后Redis优先使用AOF来恢复数据，降低丢失数据量</li>\n</ol>\n<h2 id=\"事务\"><a href=\"#事务\" class=\"headerlink\" title=\"事务\"></a>事务</h2><p>Redis事务不具有回滚机制</p>\n<h3 id=\"命令-4\"><a href=\"#命令-4\" class=\"headerlink\" title=\"命令\"></a>命令</h3><ol>\n<li>开启：multi</li>\n<li>结束：exec</li>\n<li>中断：discard</li>\n</ol>\n<h3 id=\"事务中的错误\"><a href=\"#事务中的错误\" class=\"headerlink\" title=\"事务中的错误\"></a>事务中的错误</h3><h4 id=\"命令错误\"><a href=\"#命令错误\" class=\"headerlink\" title=\"命令错误\"></a>命令错误</h4><p>执行事务过程中输入的命令出现错误，Redis会结束事务不再执行，并报出错误的命令</p>\n<h4 id=\"操作错误\"><a href=\"#操作错误\" class=\"headerlink\" title=\"操作错误\"></a>操作错误</h4><p>执行事务过程中输入的命令正确，其他操作错误，事务中其他命令正常执行，错误操作报错</p>\n<h2 id=\"锁\"><a href=\"#锁\" class=\"headerlink\" title=\"锁\"></a>锁</h2><p>Redis中锁和事务是相搭配使用的，可解决对key的监控</p>\n<h3 id=\"命令-5\"><a href=\"#命令-5\" class=\"headerlink\" title=\"命令\"></a>命令</h3><p>加锁：watch    key    [key1]<br>解锁：unwatch  //取消掉所有key的监控</p>\n<h2 id=\"数据删除策略\"><a href=\"#数据删除策略\" class=\"headerlink\" title=\"数据删除策略\"></a>数据删除策略</h2><p>当key过期后执行数据删除的策略</p>\n<h3 id=\"定时删除（即可删除）\"><a href=\"#定时删除（即可删除）\" class=\"headerlink\" title=\"定时删除（即可删除）\"></a>定时删除（即可删除）</h3><p>创建一个具有时效性的key时，同时会创建一个定时器来监控该key是否过期，当key过期后立即进行key的删除</p>\n<p>优点：节约内存，到时就会进行删除，快速释放占用空间<br>缺点：CPU压力大影响Redis响应时间和吞吐量<br>总结：用处理器性能换取存储空间</p>\n<h3 id=\"惰性删除\"><a href=\"#惰性删除\" class=\"headerlink\" title=\"惰性删除\"></a>惰性删除</h3><p>当一个具有实效性的key过期后不会有删除操作，直到下一次调用时会先检查该key是否过期，如果过期则进行删除操作，并返回nil（该key不存在）</p>\n<p>优点：节约CPU性能，发现必须删除的时候才会删除<br>缺点：内存压力大，出现长期占用内存的数据<br>总结：用存储空间换取处理器性能</p>\n<h3 id=\"定期删除\"><a href=\"#定期删除\" class=\"headerlink\" title=\"定期删除\"></a>定期删除</h3><p>Redis会根据设置的参数，定期对具有时效性的key进行清理工作，它是定时删除和惰性删除的结合者，既不像定时删除会立即进行删除给予CPU压力，也不会像惰性删除给予内存压力</p>\n<h4 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h4><ol>\n<li>Redis启动服务器初始化时，读取配置server.hz的值（默认为10）</li>\n<li>每秒钟执行server.hz次serverCron（）服务（serverCron【服务器级别】-&gt;databasesCron【数据库级别】-&gt;activeExpireCyle【活跃数据级别】）</li>\n<li>acitveExpireCyle（）对每个expires[*]（数据库）逐一进行检测，每次执行250ms／server.hz</li>\n<li>对某个expires[*]检测时，随机挑选w个key进行检测<ol>\n<li>如果key超时，删除key</li>\n<li>如果一轮中删除的kye的数量&gt;w*25%，循环该过程</li>\n<li>如果一轮中删除的可以的数量&lt;=w<em>25%检查下一个expires[</em>],0-15（所有的数据库）循环<blockquote>\n<p>W=ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP<br>参数current_db用于记录activeExpireCyle进入哪个expires[*]执行</p>\n</blockquote>\n</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"数据淘汰策略\"><a href=\"#数据淘汰策略\" class=\"headerlink\" title=\"数据淘汰策略\"></a>数据淘汰策略</h2><p>当内存到达最大内存限制时进行的数据淘汰策略</p>\n<p>数据驱逐淘汰策略配置依据，使用info命令输出相关监控信息，查新缓存hit 命中次数和miss的次数，根据业务调优</p>\n<h3 id=\"配置-1\"><a href=\"#配置-1\" class=\"headerlink\" title=\"配置\"></a>配置</h3><ol>\n<li>最大可用内存：maxmemory    //默认为0，一般设置全部内存50%以上</li>\n<li>每次选取带删除数据个数：maxmemory-samples    //采用随机获取方式</li>\n<li>删除策略：maxmemory-policy    //达到最大内存后，对被选取带数据进行的删除策略</li>\n</ol>\n<h3 id=\"检测易失数据集（可能会过期数据server-db-i-expires）\"><a href=\"#检测易失数据集（可能会过期数据server-db-i-expires）\" class=\"headerlink\" title=\"检测易失数据集（可能会过期数据server.db[i].expires）\"></a>检测易失数据集（可能会过期数据server.db[i].expires）</h3><ol>\n<li>volatile-lru：挑选最近最少使用的数据淘汰（最近数据中使用时间离当前最远的数据）。<strong>常用</strong></li>\n<li>volatile-lfu：挑选最近使用次数最少的数据淘汰（最近数据中使用次数最少的数据）</li>\n<li>volatile-ttl：挑选将要过期数据淘汰</li>\n<li>volatile-random：任意挑选数据淘汰</li>\n</ol>\n<blockquote>\n<p>ttl：time to live<br>lru：least recently    used<br>lfu：least frequently    used</p>\n</blockquote>\n<h3 id=\"检测全库数据（所有数据集server-db-i-dict）\"><a href=\"#检测全库数据（所有数据集server-db-i-dict）\" class=\"headerlink\" title=\"检测全库数据（所有数据集server.db[i].dict）\"></a>检测全库数据（所有数据集server.db[i].dict）</h3><ol>\n<li>allkeys-lru：挑选最近最少使用的数据淘汰</li>\n<li>allkeys-lfu：挑选最近使用次数最少的数据淘汰</li>\n<li>allkeys-random：任意挑选数据淘汰</li>\n</ol>\n<h3 id=\"放弃数据驱逐\"><a href=\"#放弃数据驱逐\" class=\"headerlink\" title=\"放弃数据驱逐\"></a>放弃数据驱逐</h3><p>no-enviction    //禁止驱逐数据<br>4.0中默认策略，会引发OOM</p>\n<h2 id=\"服务器基本配置\"><a href=\"#服务器基本配置\" class=\"headerlink\" title=\"服务器基本配置\"></a>服务器基本配置</h2><ol>\n<li>设置服务器守护进程方式：daemonize    yes|no</li>\n<li>绑定地址：bing    127.0.0.1</li>\n<li>设置服务器端口：port    6379</li>\n<li>设置数据库数量：databases    16</li>\n<li>设置服务器日志级别：loglevel    debug|verbose|notice|warning</li>\n<li>日志文件名称：logfile    端口号.log</li>\n<li>设置客户端最大连接数：maxclients    0</li>\n<li>客户端闲置最大等待时长：timeout    0</li>\n</ol>\n<h2 id=\"高级数据类型\"><a href=\"#高级数据类型\" class=\"headerlink\" title=\"高级数据类型\"></a>高级数据类型</h2><h3 id=\"Bitmaps\"><a href=\"#Bitmaps\" class=\"headerlink\" title=\"Bitmaps\"></a>Bitmaps</h3><p>标记统计</p>\n<h4 id=\"命令-6\"><a href=\"#命令-6\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>获取：getbit    key    offset</li>\n<li>设置：setbit    key    offset    value    // 0 或 1</li>\n<li>交、并、或异<br> bitop    op    destkey    key1    key2<br> op：<pre><code> 交：and\n 并：or\n 非：not\n 异或：xor</code></pre></li>\n<li>统计指定key中1的数量：bitcount    key    [start    end]</li>\n</ol>\n<h3 id=\"HyperLoglog\"><a href=\"#HyperLoglog\" class=\"headerlink\" title=\"HyperLoglog\"></a>HyperLoglog</h3><p>基数统计</p>\n<h4 id=\"命令-7\"><a href=\"#命令-7\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>添加：pfadd    key    element    [element1]</li>\n<li>统计：pfcount    key    [key1]</li>\n<li>合并：pfmerge    destkey    sourcekey    [sourcekey1]</li>\n</ol>\n<h3 id=\"GEO\"><a href=\"#GEO\" class=\"headerlink\" title=\"GEO\"></a>GEO</h3><p>距离计算（只计算水平距离）</p>\n<h4 id=\"命令-8\"><a href=\"#命令-8\" class=\"headerlink\" title=\"命令\"></a>命令</h4><ol>\n<li>添加：geoadd    key    longitude    latitude    member    [longitude1    latitude1    member1]</li>\n<li>获取：geopos    key    member    [member1]</li>\n<li>计算距离：geodist    key    member1    member2    [count]</li>\n<li>根据坐标求范围内数据：georadius    key    longitude    latitude    radius    m|km|ft|mi</li>\n<li>根据点求范围内的数据：georadusbymember    key    member    radius    m|km|ft|mi</li>\n<li>获取指定点对应的hash值：geohash    key    member    [member1]    </li>\n</ol>\n<h2 id=\"主从复制\"><a href=\"#主从复制\" class=\"headerlink\" title=\"主从复制\"></a>主从复制</h2><h3 id=\"创建链接\"><a href=\"#创建链接\" class=\"headerlink\" title=\"创建链接\"></a>创建链接</h3><ul>\n<li>方式一：客户端发指令：slaveof    masterip masterport</li>\n<li>方式二：参数启动：redis-server    –slaveof    masterip masterport</li>\n<li>方式三：服务器配置：slaveof    masterip masterport</li>\n</ul>\n<h3 id=\"数据同步\"><a href=\"#数据同步\" class=\"headerlink\" title=\"数据同步\"></a>数据同步</h3><h4 id=\"全量复制\"><a href=\"#全量复制\" class=\"headerlink\" title=\"全量复制\"></a>全量复制</h4><p>从：发送指令（psync2）<br>主：执行bgsave<br>主：第一个slave链接时，创建命令缓冲区<br>主：生成RDB文件，通过socket发送给slave<br>从：接收RDB文件，清空自己数据，执行RDB文件恢复过程</p>\n<h4 id=\"部分复制\"><a href=\"#部分复制\" class=\"headerlink\" title=\"部分复制\"></a>部分复制</h4><p>从：发送命令告知RDB恢复完成<br>主：发送复制缓冲区信息<br>从：接收信息，执行bgsavewriteaof，恢复数据</p>\n<h2 id=\"哨兵模式\"><a href=\"#哨兵模式\" class=\"headerlink\" title=\"哨兵模式\"></a>哨兵模式</h2><h3 id=\"配置-2\"><a href=\"#配置-2\" class=\"headerlink\" title=\"配置\"></a>配置</h3><p>sentinel.conf</p>\n<p>启动：redis-sentinel</p>\n<h2 id=\"集群（cluster）\"><a href=\"#集群（cluster）\" class=\"headerlink\" title=\"集群（cluster）\"></a>集群（cluster）</h2><h3 id=\"配置-3\"><a href=\"#配置-3\" class=\"headerlink\" title=\"配置\"></a>配置</h3><ol>\n<li>开启：cluster-enabled    yes|no</li>\n<li>配置文件名称：cluster-config-file    filename</li>\n<li>节点超时时间：cluster-node-timeout    milliseconds</li>\n<li>master链接slave最小数：cluster-migration-barrier    count</li>\n</ol>\n<h3 id=\"命令-9\"><a href=\"#命令-9\" class=\"headerlink\" title=\"命令\"></a>命令</h3><ol>\n<li>查看节点信息：cluster nodes</li>\n<li>从一个节点Redis，切换其主节点：cluster    replicate    masterip</li>\n<li>新增主节点：cluster meet    ip:port</li>\n<li>忽略一个节点：cluster    foeget    id</li>\n<li>手动故障转移：cluster    failover</li>\n</ol>\n"}],"Post":[{"title":"Hello World","date":"2018-09-12T14:25:00.000Z","author":"blinkfox","_content":"\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ndate: 2018-09-12 22:25:00\nauthor: blinkfox\ncategories: 前端\ntags:\n  - Hexo\n---\n\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"updated":"2020-03-19T04:51:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7ycejfq0001jr6418kgxh8b","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo new <span class=\"token string\">\"My New Post\"</span></code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo server</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo generate</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre class=\" language-bash\"><code class=\"language-bash\">$ hexo deploy</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre><code class=\"bash\">$ hexo new &quot;My New Post&quot;</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre><code class=\"bash\">$ hexo server</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre><code class=\"bash\">$ hexo generate</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre><code class=\"bash\">$ hexo deploy</code></pre>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n"},{"title":"大数据面试题","date":"2020-01-15T08:42:00.000Z","_content":"\n#  大数据面试题\n\n\n\n**1、你能简单描述一下Hbase吗？能画出它的架构图吗？**\n\n[hbase](http://lib.csdn.net/base/hbase)是一个面向列的 NoSQL 分布式[数据库](http://lib.csdn.net/base/mysql)，它利用HDFS作为底层存储系统。那么，HBase相对于传统的关系型数据库有什么不同呢？\n\n- HBase是schema-free的，它的列是可以动态增加的（仅仅定义列族），并且为空的列不占物理存储空间。\n- HBase是基于列存储的，每个列族都由几个文件保存，不同的列族的文件是分离的。\n- HBase自动切分数据，使得数据存储自动具有很好的横向扩展性。\n- HBase没有任何事务，提供了高并发读写操作的支持。\n\nHBase中的Table是一个稀疏的、多维度的、排序的映射表，这张表的索引是[RowKey, ColumnFamily, ColumnQualifier, Timestamp]，其中Timestamp表示版本，默认获取最新版本。HBase是通过RowKey来检索数据的，RowKey是Table设计的核心，它按照ASCII有序排序，因此应尽量避免顺序写入。RowKey设计应该注意三点：\n\n- 唯一原则：在HBase中rowkey可以看成是表的主键，必须保证其唯一性。\n- 散列原则：由于rowkey是按字典有序的，故应避免rowkey连续有序而导致在某一台RegionServer上堆积的现象。例如可以拼接随机数、将时间戳倒序等。\n- 长度原则：设计时RowKey要尽量短，这样可以提高有效数据的比例，节省存储空间，也可以提高查询的性能。\n\n下面是HBase的整体架构图：\n\n![img](http://img.blog.csdn.net/20160423184359154)\n\n**2、你说了解kafka，能简单描述一下Kafka吗？能画出它的架构图吗？**\n\nKafka是一个高吞吐、易扩展的分布式发布-订阅消息系统，它能够将消息持久化到磁盘，用于批量的消费。Kafka中有以下几个概念：\n\n- Topic：特指Kafka处理的消息源（feeds of messages）的不同分类。\n- Partition：Topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。\n- Broker：Kafa集群中包含一台或多台服务器，这种服务器被称为broker。\n- Producer：生产者，向Kafka的一个topic发布消息。\n- Consumers：消费者，从kafka的某个topic读取消息。\n\nKafka架构图如下：\n\n![img](http://img.blog.csdn.net/20160423204357095)\n\n详见：[Apache Kafka：下一代分布式消息系统](http://www.infoq.com/cn/articles/apache-kafka/)\n\n**3、请介绍你的一个亮点项目？你在其中做了什么？碰到了什么技术难点？**\n\n【解】介绍项目《九州卡牌》手游，我在项目中主要负责客户端逻辑与战斗效果的实现，以及网络通信模块的设计与开发。\n首先，对于网络通信我们选择使用**TCP长连接**，因为对于卡牌类手游可以容忍偶尔地延迟，并且有服务器主动给客户端推送消息的需求。\n优点：\n\n- 简单有效的长连接\n- 可靠的信息传输\n- 数据包的大小没有限制\n- 服务器可以主动向客户端推送消息（广播等）\n\n客户端每隔3s发送一次心跳包给服务器，通知服务器自己仍然在线，并获取服务器数据更新 —— 心跳包可以防止TCP的死连接问题，避免出现长时间不在线的死链接仍然出现在服务端的管理任务中。当客户端长时间切换到后台时，进程被挂起，连接会断开。\nTCP协议本身就有keep-alive机制，为什么还要在应用层实现自己的心跳检测机制呢？\n\n- TCP的keep-alive机制可能在短暂的网络异常中，将一个良好的连接给断开；\n- keep-alive设计初衷是清除和回收死亡时间长的连接，不适合实时性高的场合，而且它会先要求连接一定时间内没有活动，周期长，这样其实已经断开很长一段时间，没有及时性；\n- keep-alive不能主动通知应用层；\n- 另外，想要通过心跳包来获取服务器的数据更新，所以选择自己在应用层实现；\n\n还有一个问题就是一台机器的连接数有限制，可以通过**滚服**或者**分布式**来解决。\n\n- **滚服：**指老的服务器连接数达到上限了，就开新的服务区，不同服务区的用户不能交互。\n- **分布式：**长连接不分服的话，可以多个cluster节点连接同样的CACHE数据源，只是跨节点进行通信比较麻烦一点（如用户A连接到节点1，用户B连接到节点2，用户A向节点1发起TCP请求处理业务需要再通知到节点2的用户B）。一般来说有2种解决方案： \n  ①是建立场景服务器，即专门用一个socket server来保持所有玩家的连接，然后它只处理数据推送，不做业务，可以达到10-20W承载；②是采用发布订阅方式实现节点间的实时通信。\n\n我在[Linux](http://lib.csdn.net/base/linux)下写了一个**Socket心跳包示例程序**，见文《[TCP socket心跳包示例程序](http://blog.csdn.net/lisonglisonglisong/article/details/51327695)》。\n\n**4、请介绍一下MapReduce的工作原理。**\n\n【解】MapReduce是一个分布式计算框架，用于大规模数据集的并行运算。简单地说，MapReduce就是”任务的分解与结果的汇总”：将一个大的数据处理任务划分成许多个子任务，并将这些子任务分配给各个节点并行处理，然后通过整合各个节点的中间结果，得到最终结果。\n\nMapReduce是主从架构，在master上跑的是JobTracker/ResourceManager，负责资源分配与任务调度；而各个slave上跑的是TaskTracker/NodeManager，负责执行任务，并定期向master汇报最新状态与执行进度。\n\n对于一个MR任务，它的输入、输出以及中间结果都是`<key, value>`键值对：\n\n- Map：`<k1, v1>` ——> `list(<k2, v2>)`\n- Reduce：`<k2, list(v2)>` ——> `list(<k3, v3>)`\n\nMR程序的执行过程主要分为三步：Map阶段、Shuffle阶段、Reduce阶段，如下图：\n\n![img](http://img.blog.csdn.net/20160811132825039)\n\n1. **Map阶段**\n   - 分片（Split）：map阶段的输入通常是HDFS上文件，在运行Mapper前，FileInputFormat会将输入文件分割成多个split ——1个split至少包含1个HDFS的Block（默认为64M）；然后每一个分片运行一个map进行处理。\n   - 执行（Map）：对输入分片中的每个键值对调用`map()`函数进行运算，然后输出一个结果键值对。\n     - Partitioner：对 map 函数的输出进行partition，即根据key或value及reduce的数量来决定当前的这对键值对最终应该交由哪个reduce处理。默认是对key哈希后再以reduce task数量取模，默认的取模方式只是为了避免数据倾斜。然后该key/value对以及partitionIdx的结果都会被写入环形缓冲区。\n   - 溢写（Spill）：map输出写在内存中的环形缓冲区，默认当缓冲区满80%，启动溢写线程，将缓冲的数据写出到磁盘。\n     - Sort：在溢写到磁盘之前，使用快排对缓冲区数据按照partitionIdx, key排序。（每个partitionIdx表示一个分区，一个分区对应一个reduce）\n     - Combiner：如果设置了Combiner，那么在Sort之后，还会对具有相同key的键值对进行合并，减少溢写到磁盘的数据量。\n   - 合并（Merge）：溢写可能会生成多个文件，这时需要将多个文件合并成一个文件。合并的过程中会不断地进行 sort & combine 操作，最后合并成了一个已分区且已排序的文件。\n2. **Shuffle阶段**：广义上Shuffle阶段横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和merge/sort过程。通常认为Shuffle阶段就是将map的输出作为reduce的输入的过程\n   - Copy过程：Reduce端启动一些copy线程，通过HTTP方式将map端输出文件中属于自己的部分拉取到本地。Reduce会从多个map端拉取数据，并且每个map的数据都是有序的。\n   - Merge过程：Copy过来的数据会先放入内存缓冲区中，这里的缓冲区比较大；当缓冲区数据量达到一定阈值时，将数据溢写到磁盘（与map端类似，溢写过程会执行 sort & combine）。如果生成了多个溢写文件，它们会被merge成一个**有序的最终文件**。这个过程也会不停地执行 sort & combine 操作。\n3. **Reduce阶段**：Shuffle阶段最终生成了一个有序的文件作为Reduce的输入，对于该文件中的每一个键值对调用`reduce()`方法，并将结果写到HDFS。\n\n参考《[了解MapReduce核心Shuff](http://www.aboutyun.com/thread-7078-1-1.html)","source":"_posts/大数据面试题.md","raw":"---\ntitle: 大数据面试题\ndate: 2020-01-15 16:42:00\ncategories: 大数据面试题\ntags: 大数据面试题\n---\n\n#  大数据面试题\n\n\n\n**1、你能简单描述一下Hbase吗？能画出它的架构图吗？**\n\n[hbase](http://lib.csdn.net/base/hbase)是一个面向列的 NoSQL 分布式[数据库](http://lib.csdn.net/base/mysql)，它利用HDFS作为底层存储系统。那么，HBase相对于传统的关系型数据库有什么不同呢？\n\n- HBase是schema-free的，它的列是可以动态增加的（仅仅定义列族），并且为空的列不占物理存储空间。\n- HBase是基于列存储的，每个列族都由几个文件保存，不同的列族的文件是分离的。\n- HBase自动切分数据，使得数据存储自动具有很好的横向扩展性。\n- HBase没有任何事务，提供了高并发读写操作的支持。\n\nHBase中的Table是一个稀疏的、多维度的、排序的映射表，这张表的索引是[RowKey, ColumnFamily, ColumnQualifier, Timestamp]，其中Timestamp表示版本，默认获取最新版本。HBase是通过RowKey来检索数据的，RowKey是Table设计的核心，它按照ASCII有序排序，因此应尽量避免顺序写入。RowKey设计应该注意三点：\n\n- 唯一原则：在HBase中rowkey可以看成是表的主键，必须保证其唯一性。\n- 散列原则：由于rowkey是按字典有序的，故应避免rowkey连续有序而导致在某一台RegionServer上堆积的现象。例如可以拼接随机数、将时间戳倒序等。\n- 长度原则：设计时RowKey要尽量短，这样可以提高有效数据的比例，节省存储空间，也可以提高查询的性能。\n\n下面是HBase的整体架构图：\n\n![img](http://img.blog.csdn.net/20160423184359154)\n\n**2、你说了解kafka，能简单描述一下Kafka吗？能画出它的架构图吗？**\n\nKafka是一个高吞吐、易扩展的分布式发布-订阅消息系统，它能够将消息持久化到磁盘，用于批量的消费。Kafka中有以下几个概念：\n\n- Topic：特指Kafka处理的消息源（feeds of messages）的不同分类。\n- Partition：Topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。\n- Broker：Kafa集群中包含一台或多台服务器，这种服务器被称为broker。\n- Producer：生产者，向Kafka的一个topic发布消息。\n- Consumers：消费者，从kafka的某个topic读取消息。\n\nKafka架构图如下：\n\n![img](http://img.blog.csdn.net/20160423204357095)\n\n详见：[Apache Kafka：下一代分布式消息系统](http://www.infoq.com/cn/articles/apache-kafka/)\n\n**3、请介绍你的一个亮点项目？你在其中做了什么？碰到了什么技术难点？**\n\n【解】介绍项目《九州卡牌》手游，我在项目中主要负责客户端逻辑与战斗效果的实现，以及网络通信模块的设计与开发。\n首先，对于网络通信我们选择使用**TCP长连接**，因为对于卡牌类手游可以容忍偶尔地延迟，并且有服务器主动给客户端推送消息的需求。\n优点：\n\n- 简单有效的长连接\n- 可靠的信息传输\n- 数据包的大小没有限制\n- 服务器可以主动向客户端推送消息（广播等）\n\n客户端每隔3s发送一次心跳包给服务器，通知服务器自己仍然在线，并获取服务器数据更新 —— 心跳包可以防止TCP的死连接问题，避免出现长时间不在线的死链接仍然出现在服务端的管理任务中。当客户端长时间切换到后台时，进程被挂起，连接会断开。\nTCP协议本身就有keep-alive机制，为什么还要在应用层实现自己的心跳检测机制呢？\n\n- TCP的keep-alive机制可能在短暂的网络异常中，将一个良好的连接给断开；\n- keep-alive设计初衷是清除和回收死亡时间长的连接，不适合实时性高的场合，而且它会先要求连接一定时间内没有活动，周期长，这样其实已经断开很长一段时间，没有及时性；\n- keep-alive不能主动通知应用层；\n- 另外，想要通过心跳包来获取服务器的数据更新，所以选择自己在应用层实现；\n\n还有一个问题就是一台机器的连接数有限制，可以通过**滚服**或者**分布式**来解决。\n\n- **滚服：**指老的服务器连接数达到上限了，就开新的服务区，不同服务区的用户不能交互。\n- **分布式：**长连接不分服的话，可以多个cluster节点连接同样的CACHE数据源，只是跨节点进行通信比较麻烦一点（如用户A连接到节点1，用户B连接到节点2，用户A向节点1发起TCP请求处理业务需要再通知到节点2的用户B）。一般来说有2种解决方案： \n  ①是建立场景服务器，即专门用一个socket server来保持所有玩家的连接，然后它只处理数据推送，不做业务，可以达到10-20W承载；②是采用发布订阅方式实现节点间的实时通信。\n\n我在[Linux](http://lib.csdn.net/base/linux)下写了一个**Socket心跳包示例程序**，见文《[TCP socket心跳包示例程序](http://blog.csdn.net/lisonglisonglisong/article/details/51327695)》。\n\n**4、请介绍一下MapReduce的工作原理。**\n\n【解】MapReduce是一个分布式计算框架，用于大规模数据集的并行运算。简单地说，MapReduce就是”任务的分解与结果的汇总”：将一个大的数据处理任务划分成许多个子任务，并将这些子任务分配给各个节点并行处理，然后通过整合各个节点的中间结果，得到最终结果。\n\nMapReduce是主从架构，在master上跑的是JobTracker/ResourceManager，负责资源分配与任务调度；而各个slave上跑的是TaskTracker/NodeManager，负责执行任务，并定期向master汇报最新状态与执行进度。\n\n对于一个MR任务，它的输入、输出以及中间结果都是`<key, value>`键值对：\n\n- Map：`<k1, v1>` ——> `list(<k2, v2>)`\n- Reduce：`<k2, list(v2)>` ——> `list(<k3, v3>)`\n\nMR程序的执行过程主要分为三步：Map阶段、Shuffle阶段、Reduce阶段，如下图：\n\n![img](http://img.blog.csdn.net/20160811132825039)\n\n1. **Map阶段**\n   - 分片（Split）：map阶段的输入通常是HDFS上文件，在运行Mapper前，FileInputFormat会将输入文件分割成多个split ——1个split至少包含1个HDFS的Block（默认为64M）；然后每一个分片运行一个map进行处理。\n   - 执行（Map）：对输入分片中的每个键值对调用`map()`函数进行运算，然后输出一个结果键值对。\n     - Partitioner：对 map 函数的输出进行partition，即根据key或value及reduce的数量来决定当前的这对键值对最终应该交由哪个reduce处理。默认是对key哈希后再以reduce task数量取模，默认的取模方式只是为了避免数据倾斜。然后该key/value对以及partitionIdx的结果都会被写入环形缓冲区。\n   - 溢写（Spill）：map输出写在内存中的环形缓冲区，默认当缓冲区满80%，启动溢写线程，将缓冲的数据写出到磁盘。\n     - Sort：在溢写到磁盘之前，使用快排对缓冲区数据按照partitionIdx, key排序。（每个partitionIdx表示一个分区，一个分区对应一个reduce）\n     - Combiner：如果设置了Combiner，那么在Sort之后，还会对具有相同key的键值对进行合并，减少溢写到磁盘的数据量。\n   - 合并（Merge）：溢写可能会生成多个文件，这时需要将多个文件合并成一个文件。合并的过程中会不断地进行 sort & combine 操作，最后合并成了一个已分区且已排序的文件。\n2. **Shuffle阶段**：广义上Shuffle阶段横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和merge/sort过程。通常认为Shuffle阶段就是将map的输出作为reduce的输入的过程\n   - Copy过程：Reduce端启动一些copy线程，通过HTTP方式将map端输出文件中属于自己的部分拉取到本地。Reduce会从多个map端拉取数据，并且每个map的数据都是有序的。\n   - Merge过程：Copy过来的数据会先放入内存缓冲区中，这里的缓冲区比较大；当缓冲区数据量达到一定阈值时，将数据溢写到磁盘（与map端类似，溢写过程会执行 sort & combine）。如果生成了多个溢写文件，它们会被merge成一个**有序的最终文件**。这个过程也会不停地执行 sort & combine 操作。\n3. **Reduce阶段**：Shuffle阶段最终生成了一个有序的文件作为Reduce的输入，对于该文件中的每一个键值对调用`reduce()`方法，并将结果写到HDFS。\n\n参考《[了解MapReduce核心Shuff](http://www.aboutyun.com/thread-7078-1-1.html)","slug":"大数据面试题","published":1,"updated":"2020-03-18T11:52:16.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7ycejhb000mjr648t1ky96h","content":"<h1 id=\"大数据面试题\"><a href=\"#大数据面试题\" class=\"headerlink\" title=\"大数据面试题\"></a>大数据面试题</h1><p><strong>1、你能简单描述一下Hbase吗？能画出它的架构图吗？</strong></p>\n<p><a href=\"http://lib.csdn.net/base/hbase\" target=\"_blank\" rel=\"noopener\">hbase</a>是一个面向列的 NoSQL 分布式<a href=\"http://lib.csdn.net/base/mysql\" target=\"_blank\" rel=\"noopener\">数据库</a>，它利用HDFS作为底层存储系统。那么，HBase相对于传统的关系型数据库有什么不同呢？</p>\n<ul>\n<li>HBase是schema-free的，它的列是可以动态增加的（仅仅定义列族），并且为空的列不占物理存储空间。</li>\n<li>HBase是基于列存储的，每个列族都由几个文件保存，不同的列族的文件是分离的。</li>\n<li>HBase自动切分数据，使得数据存储自动具有很好的横向扩展性。</li>\n<li>HBase没有任何事务，提供了高并发读写操作的支持。</li>\n</ul>\n<p>HBase中的Table是一个稀疏的、多维度的、排序的映射表，这张表的索引是[RowKey, ColumnFamily, ColumnQualifier, Timestamp]，其中Timestamp表示版本，默认获取最新版本。HBase是通过RowKey来检索数据的，RowKey是Table设计的核心，它按照ASCII有序排序，因此应尽量避免顺序写入。RowKey设计应该注意三点：</p>\n<ul>\n<li>唯一原则：在HBase中rowkey可以看成是表的主键，必须保证其唯一性。</li>\n<li>散列原则：由于rowkey是按字典有序的，故应避免rowkey连续有序而导致在某一台RegionServer上堆积的现象。例如可以拼接随机数、将时间戳倒序等。</li>\n<li>长度原则：设计时RowKey要尽量短，这样可以提高有效数据的比例，节省存储空间，也可以提高查询的性能。</li>\n</ul>\n<p>下面是HBase的整体架构图：</p>\n<p><img src=\"http://img.blog.csdn.net/20160423184359154\" alt=\"img\"></p>\n<p><strong>2、你说了解kafka，能简单描述一下Kafka吗？能画出它的架构图吗？</strong></p>\n<p>Kafka是一个高吞吐、易扩展的分布式发布-订阅消息系统，它能够将消息持久化到磁盘，用于批量的消费。Kafka中有以下几个概念：</p>\n<ul>\n<li>Topic：特指Kafka处理的消息源（feeds of messages）的不同分类。</li>\n<li>Partition：Topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。</li>\n<li>Broker：Kafa集群中包含一台或多台服务器，这种服务器被称为broker。</li>\n<li>Producer：生产者，向Kafka的一个topic发布消息。</li>\n<li>Consumers：消费者，从kafka的某个topic读取消息。</li>\n</ul>\n<p>Kafka架构图如下：</p>\n<p><img src=\"http://img.blog.csdn.net/20160423204357095\" alt=\"img\"></p>\n<p>详见：<a href=\"http://www.infoq.com/cn/articles/apache-kafka/\" target=\"_blank\" rel=\"noopener\">Apache Kafka：下一代分布式消息系统</a></p>\n<p><strong>3、请介绍你的一个亮点项目？你在其中做了什么？碰到了什么技术难点？</strong></p>\n<p>【解】介绍项目《九州卡牌》手游，我在项目中主要负责客户端逻辑与战斗效果的实现，以及网络通信模块的设计与开发。<br>首先，对于网络通信我们选择使用<strong>TCP长连接</strong>，因为对于卡牌类手游可以容忍偶尔地延迟，并且有服务器主动给客户端推送消息的需求。<br>优点：</p>\n<ul>\n<li>简单有效的长连接</li>\n<li>可靠的信息传输</li>\n<li>数据包的大小没有限制</li>\n<li>服务器可以主动向客户端推送消息（广播等）</li>\n</ul>\n<p>客户端每隔3s发送一次心跳包给服务器，通知服务器自己仍然在线，并获取服务器数据更新 —— 心跳包可以防止TCP的死连接问题，避免出现长时间不在线的死链接仍然出现在服务端的管理任务中。当客户端长时间切换到后台时，进程被挂起，连接会断开。<br>TCP协议本身就有keep-alive机制，为什么还要在应用层实现自己的心跳检测机制呢？</p>\n<ul>\n<li>TCP的keep-alive机制可能在短暂的网络异常中，将一个良好的连接给断开；</li>\n<li>keep-alive设计初衷是清除和回收死亡时间长的连接，不适合实时性高的场合，而且它会先要求连接一定时间内没有活动，周期长，这样其实已经断开很长一段时间，没有及时性；</li>\n<li>keep-alive不能主动通知应用层；</li>\n<li>另外，想要通过心跳包来获取服务器的数据更新，所以选择自己在应用层实现；</li>\n</ul>\n<p>还有一个问题就是一台机器的连接数有限制，可以通过<strong>滚服</strong>或者<strong>分布式</strong>来解决。</p>\n<ul>\n<li><strong>滚服：</strong>指老的服务器连接数达到上限了，就开新的服务区，不同服务区的用户不能交互。</li>\n<li><strong>分布式：</strong>长连接不分服的话，可以多个cluster节点连接同样的CACHE数据源，只是跨节点进行通信比较麻烦一点（如用户A连接到节点1，用户B连接到节点2，用户A向节点1发起TCP请求处理业务需要再通知到节点2的用户B）。一般来说有2种解决方案：<br>①是建立场景服务器，即专门用一个socket server来保持所有玩家的连接，然后它只处理数据推送，不做业务，可以达到10-20W承载；②是采用发布订阅方式实现节点间的实时通信。</li>\n</ul>\n<p>我在<a href=\"http://lib.csdn.net/base/linux\" target=\"_blank\" rel=\"noopener\">Linux</a>下写了一个<strong>Socket心跳包示例程序</strong>，见文《<a href=\"http://blog.csdn.net/lisonglisonglisong/article/details/51327695\" target=\"_blank\" rel=\"noopener\">TCP socket心跳包示例程序</a>》。</p>\n<p><strong>4、请介绍一下MapReduce的工作原理。</strong></p>\n<p>【解】MapReduce是一个分布式计算框架，用于大规模数据集的并行运算。简单地说，MapReduce就是”任务的分解与结果的汇总”：将一个大的数据处理任务划分成许多个子任务，并将这些子任务分配给各个节点并行处理，然后通过整合各个节点的中间结果，得到最终结果。</p>\n<p>MapReduce是主从架构，在master上跑的是JobTracker/ResourceManager，负责资源分配与任务调度；而各个slave上跑的是TaskTracker/NodeManager，负责执行任务，并定期向master汇报最新状态与执行进度。</p>\n<p>对于一个MR任务，它的输入、输出以及中间结果都是<code>&lt;key, value&gt;</code>键值对：</p>\n<ul>\n<li>Map：<code>&lt;k1, v1&gt;</code> ——&gt; <code>list(&lt;k2, v2&gt;)</code></li>\n<li>Reduce：<code>&lt;k2, list(v2)&gt;</code> ——&gt; <code>list(&lt;k3, v3&gt;)</code></li>\n</ul>\n<p>MR程序的执行过程主要分为三步：Map阶段、Shuffle阶段、Reduce阶段，如下图：</p>\n<p><img src=\"http://img.blog.csdn.net/20160811132825039\" alt=\"img\"></p>\n<ol>\n<li><strong>Map阶段</strong><ul>\n<li>分片（Split）：map阶段的输入通常是HDFS上文件，在运行Mapper前，FileInputFormat会将输入文件分割成多个split ——1个split至少包含1个HDFS的Block（默认为64M）；然后每一个分片运行一个map进行处理。</li>\n<li>执行（Map）：对输入分片中的每个键值对调用<code>map()</code>函数进行运算，然后输出一个结果键值对。<ul>\n<li>Partitioner：对 map 函数的输出进行partition，即根据key或value及reduce的数量来决定当前的这对键值对最终应该交由哪个reduce处理。默认是对key哈希后再以reduce task数量取模，默认的取模方式只是为了避免数据倾斜。然后该key/value对以及partitionIdx的结果都会被写入环形缓冲区。</li>\n</ul>\n</li>\n<li>溢写（Spill）：map输出写在内存中的环形缓冲区，默认当缓冲区满80%，启动溢写线程，将缓冲的数据写出到磁盘。<ul>\n<li>Sort：在溢写到磁盘之前，使用快排对缓冲区数据按照partitionIdx, key排序。（每个partitionIdx表示一个分区，一个分区对应一个reduce）</li>\n<li>Combiner：如果设置了Combiner，那么在Sort之后，还会对具有相同key的键值对进行合并，减少溢写到磁盘的数据量。</li>\n</ul>\n</li>\n<li>合并（Merge）：溢写可能会生成多个文件，这时需要将多个文件合并成一个文件。合并的过程中会不断地进行 sort &amp; combine 操作，最后合并成了一个已分区且已排序的文件。</li>\n</ul>\n</li>\n<li><strong>Shuffle阶段</strong>：广义上Shuffle阶段横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和merge/sort过程。通常认为Shuffle阶段就是将map的输出作为reduce的输入的过程<ul>\n<li>Copy过程：Reduce端启动一些copy线程，通过HTTP方式将map端输出文件中属于自己的部分拉取到本地。Reduce会从多个map端拉取数据，并且每个map的数据都是有序的。</li>\n<li>Merge过程：Copy过来的数据会先放入内存缓冲区中，这里的缓冲区比较大；当缓冲区数据量达到一定阈值时，将数据溢写到磁盘（与map端类似，溢写过程会执行 sort &amp; combine）。如果生成了多个溢写文件，它们会被merge成一个<strong>有序的最终文件</strong>。这个过程也会不停地执行 sort &amp; combine 操作。</li>\n</ul>\n</li>\n<li><strong>Reduce阶段</strong>：Shuffle阶段最终生成了一个有序的文件作为Reduce的输入，对于该文件中的每一个键值对调用<code>reduce()</code>方法，并将结果写到HDFS。</li>\n</ol>\n<p>参考《<a href=\"http://www.aboutyun.com/thread-7078-1-1.html\" target=\"_blank\" rel=\"noopener\">了解MapReduce核心Shuff</a></p>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h1 id=\"大数据面试题\"><a href=\"#大数据面试题\" class=\"headerlink\" title=\"大数据面试题\"></a>大数据面试题</h1><p><strong>1、你能简单描述一下Hbase吗？能画出它的架构图吗？</strong></p>\n<p><a href=\"http://lib.csdn.net/base/hbase\" target=\"_blank\" rel=\"noopener\">hbase</a>是一个面向列的 NoSQL 分布式<a href=\"http://lib.csdn.net/base/mysql\" target=\"_blank\" rel=\"noopener\">数据库</a>，它利用HDFS作为底层存储系统。那么，HBase相对于传统的关系型数据库有什么不同呢？</p>\n<ul>\n<li>HBase是schema-free的，它的列是可以动态增加的（仅仅定义列族），并且为空的列不占物理存储空间。</li>\n<li>HBase是基于列存储的，每个列族都由几个文件保存，不同的列族的文件是分离的。</li>\n<li>HBase自动切分数据，使得数据存储自动具有很好的横向扩展性。</li>\n<li>HBase没有任何事务，提供了高并发读写操作的支持。</li>\n</ul>\n<p>HBase中的Table是一个稀疏的、多维度的、排序的映射表，这张表的索引是[RowKey, ColumnFamily, ColumnQualifier, Timestamp]，其中Timestamp表示版本，默认获取最新版本。HBase是通过RowKey来检索数据的，RowKey是Table设计的核心，它按照ASCII有序排序，因此应尽量避免顺序写入。RowKey设计应该注意三点：</p>\n<ul>\n<li>唯一原则：在HBase中rowkey可以看成是表的主键，必须保证其唯一性。</li>\n<li>散列原则：由于rowkey是按字典有序的，故应避免rowkey连续有序而导致在某一台RegionServer上堆积的现象。例如可以拼接随机数、将时间戳倒序等。</li>\n<li>长度原则：设计时RowKey要尽量短，这样可以提高有效数据的比例，节省存储空间，也可以提高查询的性能。</li>\n</ul>\n<p>下面是HBase的整体架构图：</p>\n<p><img src=\"http://img.blog.csdn.net/20160423184359154\" alt=\"img\"></p>\n<p><strong>2、你说了解kafka，能简单描述一下Kafka吗？能画出它的架构图吗？</strong></p>\n<p>Kafka是一个高吞吐、易扩展的分布式发布-订阅消息系统，它能够将消息持久化到磁盘，用于批量的消费。Kafka中有以下几个概念：</p>\n<ul>\n<li>Topic：特指Kafka处理的消息源（feeds of messages）的不同分类。</li>\n<li>Partition：Topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。</li>\n<li>Broker：Kafa集群中包含一台或多台服务器，这种服务器被称为broker。</li>\n<li>Producer：生产者，向Kafka的一个topic发布消息。</li>\n<li>Consumers：消费者，从kafka的某个topic读取消息。</li>\n</ul>\n<p>Kafka架构图如下：</p>\n<p><img src=\"http://img.blog.csdn.net/20160423204357095\" alt=\"img\"></p>\n<p>详见：<a href=\"http://www.infoq.com/cn/articles/apache-kafka/\" target=\"_blank\" rel=\"noopener\">Apache Kafka：下一代分布式消息系统</a></p>\n<p><strong>3、请介绍你的一个亮点项目？你在其中做了什么？碰到了什么技术难点？</strong></p>\n<p>【解】介绍项目《九州卡牌》手游，我在项目中主要负责客户端逻辑与战斗效果的实现，以及网络通信模块的设计与开发。<br>首先，对于网络通信我们选择使用<strong>TCP长连接</strong>，因为对于卡牌类手游可以容忍偶尔地延迟，并且有服务器主动给客户端推送消息的需求。<br>优点：</p>\n<ul>\n<li>简单有效的长连接</li>\n<li>可靠的信息传输</li>\n<li>数据包的大小没有限制</li>\n<li>服务器可以主动向客户端推送消息（广播等）</li>\n</ul>\n<p>客户端每隔3s发送一次心跳包给服务器，通知服务器自己仍然在线，并获取服务器数据更新 —— 心跳包可以防止TCP的死连接问题，避免出现长时间不在线的死链接仍然出现在服务端的管理任务中。当客户端长时间切换到后台时，进程被挂起，连接会断开。<br>TCP协议本身就有keep-alive机制，为什么还要在应用层实现自己的心跳检测机制呢？</p>\n<ul>\n<li>TCP的keep-alive机制可能在短暂的网络异常中，将一个良好的连接给断开；</li>\n<li>keep-alive设计初衷是清除和回收死亡时间长的连接，不适合实时性高的场合，而且它会先要求连接一定时间内没有活动，周期长，这样其实已经断开很长一段时间，没有及时性；</li>\n<li>keep-alive不能主动通知应用层；</li>\n<li>另外，想要通过心跳包来获取服务器的数据更新，所以选择自己在应用层实现；</li>\n</ul>\n<p>还有一个问题就是一台机器的连接数有限制，可以通过<strong>滚服</strong>或者<strong>分布式</strong>来解决。</p>\n<ul>\n<li><strong>滚服：</strong>指老的服务器连接数达到上限了，就开新的服务区，不同服务区的用户不能交互。</li>\n<li><strong>分布式：</strong>长连接不分服的话，可以多个cluster节点连接同样的CACHE数据源，只是跨节点进行通信比较麻烦一点（如用户A连接到节点1，用户B连接到节点2，用户A向节点1发起TCP请求处理业务需要再通知到节点2的用户B）。一般来说有2种解决方案：<br>①是建立场景服务器，即专门用一个socket server来保持所有玩家的连接，然后它只处理数据推送，不做业务，可以达到10-20W承载；②是采用发布订阅方式实现节点间的实时通信。</li>\n</ul>\n<p>我在<a href=\"http://lib.csdn.net/base/linux\" target=\"_blank\" rel=\"noopener\">Linux</a>下写了一个<strong>Socket心跳包示例程序</strong>，见文《<a href=\"http://blog.csdn.net/lisonglisonglisong/article/details/51327695\" target=\"_blank\" rel=\"noopener\">TCP socket心跳包示例程序</a>》。</p>\n<p><strong>4、请介绍一下MapReduce的工作原理。</strong></p>\n<p>【解】MapReduce是一个分布式计算框架，用于大规模数据集的并行运算。简单地说，MapReduce就是”任务的分解与结果的汇总”：将一个大的数据处理任务划分成许多个子任务，并将这些子任务分配给各个节点并行处理，然后通过整合各个节点的中间结果，得到最终结果。</p>\n<p>MapReduce是主从架构，在master上跑的是JobTracker/ResourceManager，负责资源分配与任务调度；而各个slave上跑的是TaskTracker/NodeManager，负责执行任务，并定期向master汇报最新状态与执行进度。</p>\n<p>对于一个MR任务，它的输入、输出以及中间结果都是<code>&lt;key, value&gt;</code>键值对：</p>\n<ul>\n<li>Map：<code>&lt;k1, v1&gt;</code> ——&gt; <code>list(&lt;k2, v2&gt;)</code></li>\n<li>Reduce：<code>&lt;k2, list(v2)&gt;</code> ——&gt; <code>list(&lt;k3, v3&gt;)</code></li>\n</ul>\n<p>MR程序的执行过程主要分为三步：Map阶段、Shuffle阶段、Reduce阶段，如下图：</p>\n<p><img src=\"http://img.blog.csdn.net/20160811132825039\" alt=\"img\"></p>\n<ol>\n<li><strong>Map阶段</strong><ul>\n<li>分片（Split）：map阶段的输入通常是HDFS上文件，在运行Mapper前，FileInputFormat会将输入文件分割成多个split ——1个split至少包含1个HDFS的Block（默认为64M）；然后每一个分片运行一个map进行处理。</li>\n<li>执行（Map）：对输入分片中的每个键值对调用<code>map()</code>函数进行运算，然后输出一个结果键值对。<ul>\n<li>Partitioner：对 map 函数的输出进行partition，即根据key或value及reduce的数量来决定当前的这对键值对最终应该交由哪个reduce处理。默认是对key哈希后再以reduce task数量取模，默认的取模方式只是为了避免数据倾斜。然后该key/value对以及partitionIdx的结果都会被写入环形缓冲区。</li>\n</ul>\n</li>\n<li>溢写（Spill）：map输出写在内存中的环形缓冲区，默认当缓冲区满80%，启动溢写线程，将缓冲的数据写出到磁盘。<ul>\n<li>Sort：在溢写到磁盘之前，使用快排对缓冲区数据按照partitionIdx, key排序。（每个partitionIdx表示一个分区，一个分区对应一个reduce）</li>\n<li>Combiner：如果设置了Combiner，那么在Sort之后，还会对具有相同key的键值对进行合并，减少溢写到磁盘的数据量。</li>\n</ul>\n</li>\n<li>合并（Merge）：溢写可能会生成多个文件，这时需要将多个文件合并成一个文件。合并的过程中会不断地进行 sort &amp; combine 操作，最后合并成了一个已分区且已排序的文件。</li>\n</ul>\n</li>\n<li><strong>Shuffle阶段</strong>：广义上Shuffle阶段横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和merge/sort过程。通常认为Shuffle阶段就是将map的输出作为reduce的输入的过程<ul>\n<li>Copy过程：Reduce端启动一些copy线程，通过HTTP方式将map端输出文件中属于自己的部分拉取到本地。Reduce会从多个map端拉取数据，并且每个map的数据都是有序的。</li>\n<li>Merge过程：Copy过来的数据会先放入内存缓冲区中，这里的缓冲区比较大；当缓冲区数据量达到一定阈值时，将数据溢写到磁盘（与map端类似，溢写过程会执行 sort &amp; combine）。如果生成了多个溢写文件，它们会被merge成一个<strong>有序的最终文件</strong>。这个过程也会不停地执行 sort &amp; combine 操作。</li>\n</ul>\n</li>\n<li><strong>Reduce阶段</strong>：Shuffle阶段最终生成了一个有序的文件作为Reduce的输入，对于该文件中的每一个键值对调用<code>reduce()</code>方法，并将结果写到HDFS。</li>\n</ol>\n<p>参考《<a href=\"http://www.aboutyun.com/thread-7078-1-1.html\" target=\"_blank\" rel=\"noopener\">了解MapReduce核心Shuff</a></p>\n"},{"title":"手撕Spark之WordCount RDD执行流程","date":"2019-12-18T02:03:14.000Z","_content":"\n##  手撕Spark之WordCount RDD执行流程\n\n[TOC]\n\n\n\n### 写在前面\n\n一个Spark程序在初始化的时候会构造DAGScheduler、TaskSchedulerImpl、MapOutTrackerMaster等对象，DAGScheduler主要负责生成DAG、启动Job、提交Stage等操作，TaskSchedulerImpl主要负责Task Set的添加调度等，MapOutTrackerMaster主要负责数据的Shuffle等，这里不再赘述。\n\n**注意几个概念：**\n\n- Application   //一个Spark程序会有一个Application，也就拥有了唯一的一个applicationId\n- Job    //调用Action 算子 触发runJob，触发一次runJob就会产生一个Job\n- Stage  //遇到一次宽依赖就会生成一个Stage\n- Task  //Spark程序运行的最小单元\n\n> 注：一个Spark程序会有1个Application，会有1～N 个Job，会有1～N 个Stage，会有1～N 个Task\n>\n> 1 Application = [1 ~ N  ] Job\n> 1 Job = [ 1 ~ N ] Stage\n> 1 Stage = [ 1 ~ N ] Task\n>\n> Stage数 = Shuffle数 +1\n\n\n\n### 软件环境\n\n+ Spark：2.3.0\n\n### 代码\n\n写一个简单的WordCount计算代码\n\ndata.txt\n\n~~~txt\nhello world\nhello java\nhello scala\nhello hadoop\nhello spark\n~~~\n\nWCAnalyzer.scala\n\n~~~scala\n\n    //设置日志输出级别，便于观察日志\n    Logger.getLogger(\"org.apache\").setLevel(Level.ALL)\n\n    //创建sc\n    val sc = new SparkContext(new SparkConf().setMaster(\"local[1]\")\n                              .setAppName(\"WCAnalyzer\"))\n\n    //从文件读取数据\n    sc.textFile(\"data/data.txt\", 1)\n      //将数据按照空格进行切分（切分出单个单词）\n      .flatMap(_.split(\" \"))\n      //将每个单词和1组成一个Tuple\n      .map((_, 1))\n      //按照相同的单词进行聚合\n      .reduceByKey(_ + _)\n      //将聚合后的结果将（key，value）数据进行倒置 转换成（value，key）便于排序\n      .map(v => (v._2, v._1))\n      //按照聚合后的单词数量进行降序排序\n      .sortByKey(false)\n      //将排序后的数据进行倒置\n      .map(v => (v._2, v._1))\n      //将数据收集到driver\n      .collect()\n      //输出数据\n      .foreach(println)\n\n    //关闭sc\n    sc.stop()\n  }\n~~~\n\n\n\n### 过程分析\n\n本代码只会生成一个Job，3个Stage，8个RDD。\n\n+ 划分Stage\n\n  Stage的划分要从后向前，每遇到一次宽依赖就划分一个Stage，因此这个简单的WC代码可以分为3个Stage，分别是由textFile、flatMap、map算子组成的第一个Stage 0；由reduceByKey、map算子组成的Stage 1；由sortByKey、map算子组成的Stage 2。\n\n+ RDD的生成 \n\n  textFile（HadoopRDD [0] ，MapPartitionsRDD [1] ）  //[ ] 内为该rdd的序号\n\n  flatMap（MapPartitionsRDD [2] ）\n\n  map（MapPartitionsRDD [3] ）\n\n  reduceByKey（ShuffledRDD [4] ）\n\n  map（MapPartitionsRDD [5] ）\n\n  sortByKey（ShuffledRDD [6] ）\n\n  map （MapPartitionsRDD [7] ）\n\n+ 日志分析\n\n    ~~~txt\n    org.apache.spark.SparkContext                     - Starting job: collect at WCAnalyzer.scala:34\n    ~~~\n    由collect算子触发runJob 启动一个Job，代码中的`foreach(println)`其中`foreach`并不是RDD中的算子，因此不会触发runJob，也就不会生成一个Job\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler           - Got job 0 (collect at WCAnalyzer.scala:34) with 1 output partitions\n    ~~~\n\n    \t生成一个Job 0 ，这个Job是由collect算子生成，在代码第34行，有一个分区\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler          - Final stage: ResultStage 2 (collect at WCAnalyzer.scala:34)\n    org.apache.spark.scheduler.DAGScheduler          - Parents of final stage: List(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - Missing parents: List(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 0)\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 0)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List()\n    ~~~\n\n    Job 的Final Stage 为ResultStage 0，ResultStage 的父依赖为ShuffleMapStage 1，遗留的父依赖为ShuffleMapStage 1。\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - submitStage(ResultStage 2)\n    ~~~\n\n    尝试提交ResultStage 2\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 1)\n    ~~~\n\n    遗留一个ShuffleMapStage 1\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 1)\n    ~~~\n\n    尝试提交ShuffleMapStage 1\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 0)\n    ~~~\n\n    遗留一个ShuffleMapStage 0\n\n     ~~~txt\n      org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 0)\n     ~~~\n    \n     尝试提交ShuffleMapStage 0\n    \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - missing: List()\n    ~~~\n  \n    没有遗留的Stage\n  \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WCAnalyzer.scala:24), which has no missing parents\n    ~~~\n  \n    提交ShuffleMapStage 0，该Stage的最后一个RDD是MapPartitionsRDD[3]，是由map算子生成，在代码第24行\n  \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - submitMissingTasks(ShuffleMapStage 0)\n    ~~~\n  \n    提交Tasks，一个Stage就是一个Task Set集合\n  \n    ~~~txt\n    org.apache.spark.scheduler.TaskSchedulerImpl    - Adding task set 0.0 with 1 tasks\n    ~~~\n    TaskSchedulerImpl 调度器添加一个Task Set集合\n  \n    ~~~txt\n    org.apache.spark.scheduler.TaskSetManager       - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes)\n    ~~~\n  \n    TaskSetManager 启动stage 0.0 中的task 0.0（taskid=0.0，host=localhost，executor=driver，partition=0，taskLocality=PROCESS_LOCAL，serializedTask=7909 bytes\n  \n    ~~~txt\n    org.apache.spark.executor.Executor              - Running task 0.0 in stage 0.0 (TID 0)\n    ~~~\n  \n    Executor 端运行task\n  \n    ~~~txt\n    org.apache.spark.executor.Executor              - Finished task 0.0 in stage 0.0 (TID 0). 1159 bytes result sent to driver\n    ~~~\n  \n    Executor 端 运行完成task，将序列化后大小为1159 bytes结果数据发送回driver端\n  \n    ~~~txt\n    org.apache.spark.scheduler.TaskSetManager       - Finished task 0.0 in stage 0.0 (TID 0) in 194 ms on localhost (executor driver) (1/1)\n    ~~~\n  \n    TaskSetManager 运行完task  完成task数量／总攻task数量\n  \n    ~~~txt\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 0.0, whose tasks have all completed, from pool \n    ~~~\n  \n    TaskSchedulerImpl 移除TaskSet 集合\n  \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver\n    ~~~\n  \n    DAGScheduler 完成ShuffleMapTask 的计算\n  \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 0 (map at WCAnalyzer.scala:24) finished in 0.289 s\n    ~~~\n  \n    DAGScheduler 完成ShuffleMapStage 的计算，用时共 0.289 s\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages\n    org.apache.spark.scheduler.DAGScheduler          - running: Set()\n    org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n    org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 1\n    org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 0 are now runnable\n    org.apache.spark.scheduler.DAGScheduler          - running: Set()\n    org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n    ~~~\n    \n    Stage在计算完后，DAGScheduler会查询是否还有未完成的计算，直到有新的Stage提交\n    \n    ~~~txt\n    ============================   ShuffleMapStage 1 的提交计算过程  ==========================\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List()\n    org.apache.spark.scheduler.DAGScheduler          - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at map at WCAnalyzer.scala:28), which has no missing parents\n    org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ShuffleMapStage 1)\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 1.0 with 1 tasks\n    org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)\n    org.apache.spark.executor.Executor               - Running task 0.0 in stage 1.0 (TID 1)\n    org.apache.spark.executor.Executor               - Finished task 0.0 in stage 1.0 (TID 1). 1331 bytes result sent to driver\n    org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 1.0 (TID 1) in 102 ms on localhost (executor driver) (1/1)\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 1.0, whose tasks have all completed, from pool \n    org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver\n    org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 1 (map at WCAnalyzer.scala:28) finished in 0.117 s\n    ============================   ResultStage 2 的提交计算过程  =============================\n    org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages\n    org.apache.spark.scheduler.DAGScheduler          - running: Set()\n    org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n    org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 2\n    org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 1 are now runnable\n    org.apache.spark.scheduler.DAGScheduler          - running: Set()\n    org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List()\n    org.apache.spark.scheduler.DAGScheduler          - Submitting ResultStage 2 (MapPartitionsRDD[7] at map at WCAnalyzer.scala:32), which has no missing parents\n    org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ResultStage 2)\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 2.0 with 1 tasks\n    org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)\n    org.apache.spark.executor.Executor               - Running task 0.0 in stage 2.0 (TID 2)\n    org.apache.spark.executor.Executor               - Finished task 0.0 in stage 2.0 (TID 2). 1387 bytes result sent to driver\n    org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 2.0 (TID 2) in 44 ms on localhost (executor driver) (1/1)\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 2.0, whose tasks have all completed, from pool \n    org.apache.spark.scheduler.DAGScheduler          - ResultStage 2 (collect at WCAnalyzer.scala:34) finished in 0.057 s\n    ~~~\n    \n    以上是ShuffleMapStage 1和ResultStage 2的提交计算过程，与ShuffleMapStage 0一样，不再赘述\n    \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - Job 0 finished: collect at WCAnalyzer.scala:34, took 0.770898 s\n    ~~~\n    \n    DAGScheduler 当所有的Stage 提交计算完成 结束Job\n\n\n\n\n","source":"_posts/手撕Spark之WordCount RDD执行流程.md","raw":"---\ntitle: 手撕Spark之WordCount RDD执行流程\ndate: 2019-12-18 10:03:14\ncategories: Spark\ntags: Spark\n---\n\n##  手撕Spark之WordCount RDD执行流程\n\n[TOC]\n\n\n\n### 写在前面\n\n一个Spark程序在初始化的时候会构造DAGScheduler、TaskSchedulerImpl、MapOutTrackerMaster等对象，DAGScheduler主要负责生成DAG、启动Job、提交Stage等操作，TaskSchedulerImpl主要负责Task Set的添加调度等，MapOutTrackerMaster主要负责数据的Shuffle等，这里不再赘述。\n\n**注意几个概念：**\n\n- Application   //一个Spark程序会有一个Application，也就拥有了唯一的一个applicationId\n- Job    //调用Action 算子 触发runJob，触发一次runJob就会产生一个Job\n- Stage  //遇到一次宽依赖就会生成一个Stage\n- Task  //Spark程序运行的最小单元\n\n> 注：一个Spark程序会有1个Application，会有1～N 个Job，会有1～N 个Stage，会有1～N 个Task\n>\n> 1 Application = [1 ~ N  ] Job\n> 1 Job = [ 1 ~ N ] Stage\n> 1 Stage = [ 1 ~ N ] Task\n>\n> Stage数 = Shuffle数 +1\n\n\n\n### 软件环境\n\n+ Spark：2.3.0\n\n### 代码\n\n写一个简单的WordCount计算代码\n\ndata.txt\n\n~~~txt\nhello world\nhello java\nhello scala\nhello hadoop\nhello spark\n~~~\n\nWCAnalyzer.scala\n\n~~~scala\n\n    //设置日志输出级别，便于观察日志\n    Logger.getLogger(\"org.apache\").setLevel(Level.ALL)\n\n    //创建sc\n    val sc = new SparkContext(new SparkConf().setMaster(\"local[1]\")\n                              .setAppName(\"WCAnalyzer\"))\n\n    //从文件读取数据\n    sc.textFile(\"data/data.txt\", 1)\n      //将数据按照空格进行切分（切分出单个单词）\n      .flatMap(_.split(\" \"))\n      //将每个单词和1组成一个Tuple\n      .map((_, 1))\n      //按照相同的单词进行聚合\n      .reduceByKey(_ + _)\n      //将聚合后的结果将（key，value）数据进行倒置 转换成（value，key）便于排序\n      .map(v => (v._2, v._1))\n      //按照聚合后的单词数量进行降序排序\n      .sortByKey(false)\n      //将排序后的数据进行倒置\n      .map(v => (v._2, v._1))\n      //将数据收集到driver\n      .collect()\n      //输出数据\n      .foreach(println)\n\n    //关闭sc\n    sc.stop()\n  }\n~~~\n\n\n\n### 过程分析\n\n本代码只会生成一个Job，3个Stage，8个RDD。\n\n+ 划分Stage\n\n  Stage的划分要从后向前，每遇到一次宽依赖就划分一个Stage，因此这个简单的WC代码可以分为3个Stage，分别是由textFile、flatMap、map算子组成的第一个Stage 0；由reduceByKey、map算子组成的Stage 1；由sortByKey、map算子组成的Stage 2。\n\n+ RDD的生成 \n\n  textFile（HadoopRDD [0] ，MapPartitionsRDD [1] ）  //[ ] 内为该rdd的序号\n\n  flatMap（MapPartitionsRDD [2] ）\n\n  map（MapPartitionsRDD [3] ）\n\n  reduceByKey（ShuffledRDD [4] ）\n\n  map（MapPartitionsRDD [5] ）\n\n  sortByKey（ShuffledRDD [6] ）\n\n  map （MapPartitionsRDD [7] ）\n\n+ 日志分析\n\n    ~~~txt\n    org.apache.spark.SparkContext                     - Starting job: collect at WCAnalyzer.scala:34\n    ~~~\n    由collect算子触发runJob 启动一个Job，代码中的`foreach(println)`其中`foreach`并不是RDD中的算子，因此不会触发runJob，也就不会生成一个Job\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler           - Got job 0 (collect at WCAnalyzer.scala:34) with 1 output partitions\n    ~~~\n\n    \t生成一个Job 0 ，这个Job是由collect算子生成，在代码第34行，有一个分区\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler          - Final stage: ResultStage 2 (collect at WCAnalyzer.scala:34)\n    org.apache.spark.scheduler.DAGScheduler          - Parents of final stage: List(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - Missing parents: List(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 0)\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 0)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List()\n    ~~~\n\n    Job 的Final Stage 为ResultStage 0，ResultStage 的父依赖为ShuffleMapStage 1，遗留的父依赖为ShuffleMapStage 1。\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - submitStage(ResultStage 2)\n    ~~~\n\n    尝试提交ResultStage 2\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 1)\n    ~~~\n\n    遗留一个ShuffleMapStage 1\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 1)\n    ~~~\n\n    尝试提交ShuffleMapStage 1\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 0)\n    ~~~\n\n    遗留一个ShuffleMapStage 0\n\n     ~~~txt\n      org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 0)\n     ~~~\n    \n     尝试提交ShuffleMapStage 0\n    \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - missing: List()\n    ~~~\n  \n    没有遗留的Stage\n  \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WCAnalyzer.scala:24), which has no missing parents\n    ~~~\n  \n    提交ShuffleMapStage 0，该Stage的最后一个RDD是MapPartitionsRDD[3]，是由map算子生成，在代码第24行\n  \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - submitMissingTasks(ShuffleMapStage 0)\n    ~~~\n  \n    提交Tasks，一个Stage就是一个Task Set集合\n  \n    ~~~txt\n    org.apache.spark.scheduler.TaskSchedulerImpl    - Adding task set 0.0 with 1 tasks\n    ~~~\n    TaskSchedulerImpl 调度器添加一个Task Set集合\n  \n    ~~~txt\n    org.apache.spark.scheduler.TaskSetManager       - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes)\n    ~~~\n  \n    TaskSetManager 启动stage 0.0 中的task 0.0（taskid=0.0，host=localhost，executor=driver，partition=0，taskLocality=PROCESS_LOCAL，serializedTask=7909 bytes\n  \n    ~~~txt\n    org.apache.spark.executor.Executor              - Running task 0.0 in stage 0.0 (TID 0)\n    ~~~\n  \n    Executor 端运行task\n  \n    ~~~txt\n    org.apache.spark.executor.Executor              - Finished task 0.0 in stage 0.0 (TID 0). 1159 bytes result sent to driver\n    ~~~\n  \n    Executor 端 运行完成task，将序列化后大小为1159 bytes结果数据发送回driver端\n  \n    ~~~txt\n    org.apache.spark.scheduler.TaskSetManager       - Finished task 0.0 in stage 0.0 (TID 0) in 194 ms on localhost (executor driver) (1/1)\n    ~~~\n  \n    TaskSetManager 运行完task  完成task数量／总攻task数量\n  \n    ~~~txt\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 0.0, whose tasks have all completed, from pool \n    ~~~\n  \n    TaskSchedulerImpl 移除TaskSet 集合\n  \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver\n    ~~~\n  \n    DAGScheduler 完成ShuffleMapTask 的计算\n  \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 0 (map at WCAnalyzer.scala:24) finished in 0.289 s\n    ~~~\n  \n    DAGScheduler 完成ShuffleMapStage 的计算，用时共 0.289 s\n\n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages\n    org.apache.spark.scheduler.DAGScheduler          - running: Set()\n    org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n    org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 1\n    org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 0 are now runnable\n    org.apache.spark.scheduler.DAGScheduler          - running: Set()\n    org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n    ~~~\n    \n    Stage在计算完后，DAGScheduler会查询是否还有未完成的计算，直到有新的Stage提交\n    \n    ~~~txt\n    ============================   ShuffleMapStage 1 的提交计算过程  ==========================\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List()\n    org.apache.spark.scheduler.DAGScheduler          - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at map at WCAnalyzer.scala:28), which has no missing parents\n    org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ShuffleMapStage 1)\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 1.0 with 1 tasks\n    org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)\n    org.apache.spark.executor.Executor               - Running task 0.0 in stage 1.0 (TID 1)\n    org.apache.spark.executor.Executor               - Finished task 0.0 in stage 1.0 (TID 1). 1331 bytes result sent to driver\n    org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 1.0 (TID 1) in 102 ms on localhost (executor driver) (1/1)\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 1.0, whose tasks have all completed, from pool \n    org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver\n    org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 1 (map at WCAnalyzer.scala:28) finished in 0.117 s\n    ============================   ResultStage 2 的提交计算过程  =============================\n    org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages\n    org.apache.spark.scheduler.DAGScheduler          - running: Set()\n    org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n    org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 2\n    org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 1 are now runnable\n    org.apache.spark.scheduler.DAGScheduler          - running: Set()\n    org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n    org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)\n    org.apache.spark.scheduler.DAGScheduler          - missing: List()\n    org.apache.spark.scheduler.DAGScheduler          - Submitting ResultStage 2 (MapPartitionsRDD[7] at map at WCAnalyzer.scala:32), which has no missing parents\n    org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ResultStage 2)\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 2.0 with 1 tasks\n    org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)\n    org.apache.spark.executor.Executor               - Running task 0.0 in stage 2.0 (TID 2)\n    org.apache.spark.executor.Executor               - Finished task 0.0 in stage 2.0 (TID 2). 1387 bytes result sent to driver\n    org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 2.0 (TID 2) in 44 ms on localhost (executor driver) (1/1)\n    org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 2.0, whose tasks have all completed, from pool \n    org.apache.spark.scheduler.DAGScheduler          - ResultStage 2 (collect at WCAnalyzer.scala:34) finished in 0.057 s\n    ~~~\n    \n    以上是ShuffleMapStage 1和ResultStage 2的提交计算过程，与ShuffleMapStage 0一样，不再赘述\n    \n    ~~~txt\n    org.apache.spark.scheduler.DAGScheduler         - Job 0 finished: collect at WCAnalyzer.scala:34, took 0.770898 s\n    ~~~\n    \n    DAGScheduler 当所有的Stage 提交计算完成 结束Job\n\n\n\n\n","slug":"手撕Spark之WordCount RDD执行流程","published":1,"updated":"2020-03-18T11:52:16.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ck7ycejhd000ojr64rw99bnbw","content":"<h2 id=\"手撕Spark之WordCount-RDD执行流程\"><a href=\"#手撕Spark之WordCount-RDD执行流程\" class=\"headerlink\" title=\"手撕Spark之WordCount RDD执行流程\"></a>手撕Spark之WordCount RDD执行流程</h2><p>[TOC]</p>\n<h3 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h3><p>一个Spark程序在初始化的时候会构造DAGScheduler、TaskSchedulerImpl、MapOutTrackerMaster等对象，DAGScheduler主要负责生成DAG、启动Job、提交Stage等操作，TaskSchedulerImpl主要负责Task Set的添加调度等，MapOutTrackerMaster主要负责数据的Shuffle等，这里不再赘述。</p>\n<p><strong>注意几个概念：</strong></p>\n<ul>\n<li>Application   //一个Spark程序会有一个Application，也就拥有了唯一的一个applicationId</li>\n<li>Job    //调用Action 算子 触发runJob，触发一次runJob就会产生一个Job</li>\n<li>Stage  //遇到一次宽依赖就会生成一个Stage</li>\n<li>Task  //Spark程序运行的最小单元</li>\n</ul>\n<blockquote>\n<p>注：一个Spark程序会有1个Application，会有1～N 个Job，会有1～N 个Stage，会有1～N 个Task</p>\n<p>1 Application = [1 ~ N  ] Job<br>1 Job = [ 1 ~ N ] Stage<br>1 Stage = [ 1 ~ N ] Task</p>\n<p>Stage数 = Shuffle数 +1</p>\n</blockquote>\n<h3 id=\"软件环境\"><a href=\"#软件环境\" class=\"headerlink\" title=\"软件环境\"></a>软件环境</h3><ul>\n<li>Spark：2.3.0</li>\n</ul>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><p>写一个简单的WordCount计算代码</p>\n<p>data.txt</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">hello world\nhello java\nhello scala\nhello hadoop\nhello spark</code></pre>\n<p>WCAnalyzer.scala</p>\n<pre class=\" language-scala\"><code class=\"language-scala\">\n    <span class=\"token comment\" spellcheck=\"true\">//设置日志输出级别，便于观察日志</span>\n    Logger<span class=\"token punctuation\">.</span>getLogger<span class=\"token punctuation\">(</span><span class=\"token string\">\"org.apache\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>setLevel<span class=\"token punctuation\">(</span>Level<span class=\"token punctuation\">.</span>ALL<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">//创建sc</span>\n    <span class=\"token keyword\">val</span> sc <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> SparkContext<span class=\"token punctuation\">(</span><span class=\"token keyword\">new</span> SparkConf<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>setMaster<span class=\"token punctuation\">(</span><span class=\"token string\">\"local[1]\"</span><span class=\"token punctuation\">)</span>\n                              <span class=\"token punctuation\">.</span>setAppName<span class=\"token punctuation\">(</span><span class=\"token string\">\"WCAnalyzer\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">//从文件读取数据</span>\n    sc<span class=\"token punctuation\">.</span>textFile<span class=\"token punctuation\">(</span><span class=\"token string\">\"data/data.txt\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n      <span class=\"token comment\" spellcheck=\"true\">//将数据按照空格进行切分（切分出单个单词）</span>\n      <span class=\"token punctuation\">.</span>flatMap<span class=\"token punctuation\">(</span>_<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">\" \"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n      <span class=\"token comment\" spellcheck=\"true\">//将每个单词和1组成一个Tuple</span>\n      <span class=\"token punctuation\">.</span>map<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>_<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n      <span class=\"token comment\" spellcheck=\"true\">//按照相同的单词进行聚合</span>\n      <span class=\"token punctuation\">.</span>reduceByKey<span class=\"token punctuation\">(</span>_ <span class=\"token operator\">+</span> _<span class=\"token punctuation\">)</span>\n      <span class=\"token comment\" spellcheck=\"true\">//将聚合后的结果将（key，value）数据进行倒置 转换成（value，key）便于排序</span>\n      <span class=\"token punctuation\">.</span>map<span class=\"token punctuation\">(</span>v <span class=\"token keyword\">=></span> <span class=\"token punctuation\">(</span>v<span class=\"token punctuation\">.</span>_2<span class=\"token punctuation\">,</span> v<span class=\"token punctuation\">.</span>_1<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n      <span class=\"token comment\" spellcheck=\"true\">//按照聚合后的单词数量进行降序排序</span>\n      <span class=\"token punctuation\">.</span>sortByKey<span class=\"token punctuation\">(</span><span class=\"token boolean\">false</span><span class=\"token punctuation\">)</span>\n      <span class=\"token comment\" spellcheck=\"true\">//将排序后的数据进行倒置</span>\n      <span class=\"token punctuation\">.</span>map<span class=\"token punctuation\">(</span>v <span class=\"token keyword\">=></span> <span class=\"token punctuation\">(</span>v<span class=\"token punctuation\">.</span>_2<span class=\"token punctuation\">,</span> v<span class=\"token punctuation\">.</span>_1<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n      <span class=\"token comment\" spellcheck=\"true\">//将数据收集到driver</span>\n      <span class=\"token punctuation\">.</span>collect<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n      <span class=\"token comment\" spellcheck=\"true\">//输出数据</span>\n      <span class=\"token punctuation\">.</span>foreach<span class=\"token punctuation\">(</span>println<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\" spellcheck=\"true\">//关闭sc</span>\n    sc<span class=\"token punctuation\">.</span>stop<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">}</span></code></pre>\n<h3 id=\"过程分析\"><a href=\"#过程分析\" class=\"headerlink\" title=\"过程分析\"></a>过程分析</h3><p>本代码只会生成一个Job，3个Stage，8个RDD。</p>\n<ul>\n<li><p>划分Stage</p>\n<p>Stage的划分要从后向前，每遇到一次宽依赖就划分一个Stage，因此这个简单的WC代码可以分为3个Stage，分别是由textFile、flatMap、map算子组成的第一个Stage 0；由reduceByKey、map算子组成的Stage 1；由sortByKey、map算子组成的Stage 2。</p>\n</li>\n<li><p>RDD的生成 </p>\n<p>textFile（HadoopRDD [0] ，MapPartitionsRDD [1] ）  //[ ] 内为该rdd的序号</p>\n<p>flatMap（MapPartitionsRDD [2] ）</p>\n<p>map（MapPartitionsRDD [3] ）</p>\n<p>reduceByKey（ShuffledRDD [4] ）</p>\n<p>map（MapPartitionsRDD [5] ）</p>\n<p>sortByKey（ShuffledRDD [6] ）</p>\n<p>map （MapPartitionsRDD [7] ）</p>\n</li>\n<li><p>日志分析</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.SparkContext                     - Starting job: collect at WCAnalyzer.scala:34</code></pre>\n<p>  由collect算子触发runJob 启动一个Job，代码中的<code>foreach(println)</code>其中<code>foreach</code>并不是RDD中的算子，因此不会触发runJob，也就不会生成一个Job</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler           - Got job 0 (collect at WCAnalyzer.scala:34) with 1 output partitions</code></pre>\n<pre><code>  生成一个Job 0 ，这个Job是由collect算子生成，在代码第34行，有一个分区</code></pre><pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler          - Final stage: ResultStage 2 (collect at WCAnalyzer.scala:34)\n  org.apache.spark.scheduler.DAGScheduler          - Parents of final stage: List(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - Missing parents: List(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 0)\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 0)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List()</code></pre>\n<p>  Job 的Final Stage 为ResultStage 0，ResultStage 的父依赖为ShuffleMapStage 1，遗留的父依赖为ShuffleMapStage 1。</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler         - submitStage(ResultStage 2)</code></pre>\n<p>  尝试提交ResultStage 2</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 1)</code></pre>\n<p>  遗留一个ShuffleMapStage 1</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 1)</code></pre>\n<p>  尝试提交ShuffleMapStage 1</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 0)</code></pre>\n<p>  遗留一个ShuffleMapStage 0</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">    org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 0)</code></pre>\n<p>   尝试提交ShuffleMapStage 0</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler         - missing: List()</code></pre>\n<p>  没有遗留的Stage</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler         - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WCAnalyzer.scala:24), which has no missing parents</code></pre>\n<p>  提交ShuffleMapStage 0，该Stage的最后一个RDD是MapPartitionsRDD[3]，是由map算子生成，在代码第24行</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler         - submitMissingTasks(ShuffleMapStage 0)</code></pre>\n<p>  提交Tasks，一个Stage就是一个Task Set集合</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.TaskSchedulerImpl    - Adding task set 0.0 with 1 tasks</code></pre>\n<p>  TaskSchedulerImpl 调度器添加一个Task Set集合</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.TaskSetManager       - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes)</code></pre>\n<p>  TaskSetManager 启动stage 0.0 中的task 0.0（taskid=0.0，host=localhost，executor=driver，partition=0，taskLocality=PROCESS_LOCAL，serializedTask=7909 bytes</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.executor.Executor              - Running task 0.0 in stage 0.0 (TID 0)</code></pre>\n<p>  Executor 端运行task</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.executor.Executor              - Finished task 0.0 in stage 0.0 (TID 0). 1159 bytes result sent to driver</code></pre>\n<p>  Executor 端 运行完成task，将序列化后大小为1159 bytes结果数据发送回driver端</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.TaskSetManager       - Finished task 0.0 in stage 0.0 (TID 0) in 194 ms on localhost (executor driver) (1/1)</code></pre>\n<p>  TaskSetManager 运行完task  完成task数量／总攻task数量</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 0.0, whose tasks have all completed, from pool </code></pre>\n<p>  TaskSchedulerImpl 移除TaskSet 集合</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver</code></pre>\n<p>  DAGScheduler 完成ShuffleMapTask 的计算</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 0 (map at WCAnalyzer.scala:24) finished in 0.289 s</code></pre>\n<p>  DAGScheduler 完成ShuffleMapStage 的计算，用时共 0.289 s</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages\n  org.apache.spark.scheduler.DAGScheduler          - running: Set()\n  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n  org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 1\n  org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 0 are now runnable\n  org.apache.spark.scheduler.DAGScheduler          - running: Set()\n  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - failed: Set()</code></pre>\n<p>  Stage在计算完后，DAGScheduler会查询是否还有未完成的计算，直到有新的Stage提交</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  ============================   ShuffleMapStage 1 的提交计算过程  ==========================\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List()\n  org.apache.spark.scheduler.DAGScheduler          - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at map at WCAnalyzer.scala:28), which has no missing parents\n  org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ShuffleMapStage 1)\n  org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 1.0 with 1 tasks\n  org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)\n  org.apache.spark.executor.Executor               - Running task 0.0 in stage 1.0 (TID 1)\n  org.apache.spark.executor.Executor               - Finished task 0.0 in stage 1.0 (TID 1). 1331 bytes result sent to driver\n  org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 1.0 (TID 1) in 102 ms on localhost (executor driver) (1/1)\n  org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 1.0, whose tasks have all completed, from pool \n  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver\n  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 1 (map at WCAnalyzer.scala:28) finished in 0.117 s\n  ============================   ResultStage 2 的提交计算过程  =============================\n  org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages\n  org.apache.spark.scheduler.DAGScheduler          - running: Set()\n  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n  org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 2\n  org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 1 are now runnable\n  org.apache.spark.scheduler.DAGScheduler          - running: Set()\n  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List()\n  org.apache.spark.scheduler.DAGScheduler          - Submitting ResultStage 2 (MapPartitionsRDD[7] at map at WCAnalyzer.scala:32), which has no missing parents\n  org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ResultStage 2)\n  org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 2.0 with 1 tasks\n  org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)\n  org.apache.spark.executor.Executor               - Running task 0.0 in stage 2.0 (TID 2)\n  org.apache.spark.executor.Executor               - Finished task 0.0 in stage 2.0 (TID 2). 1387 bytes result sent to driver\n  org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 2.0 (TID 2) in 44 ms on localhost (executor driver) (1/1)\n  org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 2.0, whose tasks have all completed, from pool \n  org.apache.spark.scheduler.DAGScheduler          - ResultStage 2 (collect at WCAnalyzer.scala:34) finished in 0.057 s</code></pre>\n<p>  以上是ShuffleMapStage 1和ResultStage 2的提交计算过程，与ShuffleMapStage 0一样，不再赘述</p>\n<pre class=\" language-txt\"><code class=\"language-txt\">  org.apache.spark.scheduler.DAGScheduler         - Job 0 finished: collect at WCAnalyzer.scala:34, took 0.770898 s</code></pre>\n<p>  DAGScheduler 当所有的Stage 提交计算完成 结束Job</p>\n</li>\n</ul>\n","site":{"data":{"friends":[{"avatar":"http://image.luokangyuan.com/1_qq_27922023.jpg","name":"码酱","introduction":"我不是大佬，只是在追寻大佬的脚步","url":"http://luokangyuan.com/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/20190601.png","name":"码农","introduction":"这里不隐含扭曲的价值观，而是整合并充盈正能量","url":"https://www.90c.vip/","title":"前去学习"},{"avatar":"https://jiangliuhong.gitee.io/images/avatar.jpg","name":"编程常青树","introduction":"平凡的脚步也可以走出伟大的行程","url":"https://jiangliuhong.gitee.io/","title":"前去学习"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar2.png","name":"洪卫の博客","introduction":"凭寄狂夫书一纸，信在成都万里桥。","url":"https://sunhwee.com","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar.jpg","name":"过客~励む","introduction":"你现在的努力，是为了以后有更多的选择。","url":"https://yafine-blog.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar5.png","name":"Sitoi","introduction":"妄想通过成为 Spider-Man 来实现财富自由的程序猿","url":"https://sitoi.cn","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/11/23/avatar3.jpeg","name":"Five-great","introduction":"有事多研究，没事瞎琢磨。","url":"http://www.fivecc.cn/","title":"前去探索"},{"avatar":"http://static.blinkfox.com/2019/12/24logo.png","name":"A2Data","introduction":"武术跨行大数据，用技术推动梦想的落地！","url":"https://www.a2data.cn","title":"开启跨行之旅"}],"musics":[{"name":"五月雨变奏电音","artist":"AnimeVibe","url":"http://static.blinkfox.com/music1.mp3","cover":"http://static.blinkfox.com/music-cover1.png"},{"name":"Take me hand","artist":"DAISHI DANCE,Cecile Corbel","url":"http://static.blinkfox.com/music2.mp3","cover":"http://static.blinkfox.com/music-cover2.png"},{"name":"Shape of You","artist":"J.Fla","url":"http://static.blinkfox.com/music3.mp3","cover":"http://static.blinkfox.com/music-cover3.png"}]}},"excerpt":"","more":"<h2 id=\"手撕Spark之WordCount-RDD执行流程\"><a href=\"#手撕Spark之WordCount-RDD执行流程\" class=\"headerlink\" title=\"手撕Spark之WordCount RDD执行流程\"></a>手撕Spark之WordCount RDD执行流程</h2><p>[TOC]</p>\n<h3 id=\"写在前面\"><a href=\"#写在前面\" class=\"headerlink\" title=\"写在前面\"></a>写在前面</h3><p>一个Spark程序在初始化的时候会构造DAGScheduler、TaskSchedulerImpl、MapOutTrackerMaster等对象，DAGScheduler主要负责生成DAG、启动Job、提交Stage等操作，TaskSchedulerImpl主要负责Task Set的添加调度等，MapOutTrackerMaster主要负责数据的Shuffle等，这里不再赘述。</p>\n<p><strong>注意几个概念：</strong></p>\n<ul>\n<li>Application   //一个Spark程序会有一个Application，也就拥有了唯一的一个applicationId</li>\n<li>Job    //调用Action 算子 触发runJob，触发一次runJob就会产生一个Job</li>\n<li>Stage  //遇到一次宽依赖就会生成一个Stage</li>\n<li>Task  //Spark程序运行的最小单元</li>\n</ul>\n<blockquote>\n<p>注：一个Spark程序会有1个Application，会有1～N 个Job，会有1～N 个Stage，会有1～N 个Task</p>\n<p>1 Application = [1 ~ N  ] Job<br>1 Job = [ 1 ~ N ] Stage<br>1 Stage = [ 1 ~ N ] Task</p>\n<p>Stage数 = Shuffle数 +1</p>\n</blockquote>\n<h3 id=\"软件环境\"><a href=\"#软件环境\" class=\"headerlink\" title=\"软件环境\"></a>软件环境</h3><ul>\n<li>Spark：2.3.0</li>\n</ul>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><p>写一个简单的WordCount计算代码</p>\n<p>data.txt</p>\n<pre><code class=\"txt\">hello world\nhello java\nhello scala\nhello hadoop\nhello spark</code></pre>\n<p>WCAnalyzer.scala</p>\n<pre><code class=\"scala\">\n    //设置日志输出级别，便于观察日志\n    Logger.getLogger(&quot;org.apache&quot;).setLevel(Level.ALL)\n\n    //创建sc\n    val sc = new SparkContext(new SparkConf().setMaster(&quot;local[1]&quot;)\n                              .setAppName(&quot;WCAnalyzer&quot;))\n\n    //从文件读取数据\n    sc.textFile(&quot;data/data.txt&quot;, 1)\n      //将数据按照空格进行切分（切分出单个单词）\n      .flatMap(_.split(&quot; &quot;))\n      //将每个单词和1组成一个Tuple\n      .map((_, 1))\n      //按照相同的单词进行聚合\n      .reduceByKey(_ + _)\n      //将聚合后的结果将（key，value）数据进行倒置 转换成（value，key）便于排序\n      .map(v =&gt; (v._2, v._1))\n      //按照聚合后的单词数量进行降序排序\n      .sortByKey(false)\n      //将排序后的数据进行倒置\n      .map(v =&gt; (v._2, v._1))\n      //将数据收集到driver\n      .collect()\n      //输出数据\n      .foreach(println)\n\n    //关闭sc\n    sc.stop()\n  }</code></pre>\n<h3 id=\"过程分析\"><a href=\"#过程分析\" class=\"headerlink\" title=\"过程分析\"></a>过程分析</h3><p>本代码只会生成一个Job，3个Stage，8个RDD。</p>\n<ul>\n<li><p>划分Stage</p>\n<p>Stage的划分要从后向前，每遇到一次宽依赖就划分一个Stage，因此这个简单的WC代码可以分为3个Stage，分别是由textFile、flatMap、map算子组成的第一个Stage 0；由reduceByKey、map算子组成的Stage 1；由sortByKey、map算子组成的Stage 2。</p>\n</li>\n<li><p>RDD的生成 </p>\n<p>textFile（HadoopRDD [0] ，MapPartitionsRDD [1] ）  //[ ] 内为该rdd的序号</p>\n<p>flatMap（MapPartitionsRDD [2] ）</p>\n<p>map（MapPartitionsRDD [3] ）</p>\n<p>reduceByKey（ShuffledRDD [4] ）</p>\n<p>map（MapPartitionsRDD [5] ）</p>\n<p>sortByKey（ShuffledRDD [6] ）</p>\n<p>map （MapPartitionsRDD [7] ）</p>\n</li>\n<li><p>日志分析</p>\n<pre><code class=\"txt\">  org.apache.spark.SparkContext                     - Starting job: collect at WCAnalyzer.scala:34</code></pre>\n<p>  由collect算子触发runJob 启动一个Job，代码中的<code>foreach(println)</code>其中<code>foreach</code>并不是RDD中的算子，因此不会触发runJob，也就不会生成一个Job</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler           - Got job 0 (collect at WCAnalyzer.scala:34) with 1 output partitions</code></pre>\n<pre><code>  生成一个Job 0 ，这个Job是由collect算子生成，在代码第34行，有一个分区</code></pre><pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler          - Final stage: ResultStage 2 (collect at WCAnalyzer.scala:34)\n  org.apache.spark.scheduler.DAGScheduler          - Parents of final stage: List(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - Missing parents: List(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 0)\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 0)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List()</code></pre>\n<p>  Job 的Final Stage 为ResultStage 0，ResultStage 的父依赖为ShuffleMapStage 1，遗留的父依赖为ShuffleMapStage 1。</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler         - submitStage(ResultStage 2)</code></pre>\n<p>  尝试提交ResultStage 2</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 1)</code></pre>\n<p>  遗留一个ShuffleMapStage 1</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 1)</code></pre>\n<p>  尝试提交ShuffleMapStage 1</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 0)</code></pre>\n<p>  遗留一个ShuffleMapStage 0</p>\n<pre><code class=\"txt\">    org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 0)</code></pre>\n<p>   尝试提交ShuffleMapStage 0</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler         - missing: List()</code></pre>\n<p>  没有遗留的Stage</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler         - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WCAnalyzer.scala:24), which has no missing parents</code></pre>\n<p>  提交ShuffleMapStage 0，该Stage的最后一个RDD是MapPartitionsRDD[3]，是由map算子生成，在代码第24行</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler         - submitMissingTasks(ShuffleMapStage 0)</code></pre>\n<p>  提交Tasks，一个Stage就是一个Task Set集合</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.TaskSchedulerImpl    - Adding task set 0.0 with 1 tasks</code></pre>\n<p>  TaskSchedulerImpl 调度器添加一个Task Set集合</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.TaskSetManager       - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes)</code></pre>\n<p>  TaskSetManager 启动stage 0.0 中的task 0.0（taskid=0.0，host=localhost，executor=driver，partition=0，taskLocality=PROCESS_LOCAL，serializedTask=7909 bytes</p>\n<pre><code class=\"txt\">  org.apache.spark.executor.Executor              - Running task 0.0 in stage 0.0 (TID 0)</code></pre>\n<p>  Executor 端运行task</p>\n<pre><code class=\"txt\">  org.apache.spark.executor.Executor              - Finished task 0.0 in stage 0.0 (TID 0). 1159 bytes result sent to driver</code></pre>\n<p>  Executor 端 运行完成task，将序列化后大小为1159 bytes结果数据发送回driver端</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.TaskSetManager       - Finished task 0.0 in stage 0.0 (TID 0) in 194 ms on localhost (executor driver) (1/1)</code></pre>\n<p>  TaskSetManager 运行完task  完成task数量／总攻task数量</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 0.0, whose tasks have all completed, from pool </code></pre>\n<p>  TaskSchedulerImpl 移除TaskSet 集合</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver</code></pre>\n<p>  DAGScheduler 完成ShuffleMapTask 的计算</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 0 (map at WCAnalyzer.scala:24) finished in 0.289 s</code></pre>\n<p>  DAGScheduler 完成ShuffleMapStage 的计算，用时共 0.289 s</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages\n  org.apache.spark.scheduler.DAGScheduler          - running: Set()\n  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n  org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 1\n  org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 0 are now runnable\n  org.apache.spark.scheduler.DAGScheduler          - running: Set()\n  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - failed: Set()</code></pre>\n<p>  Stage在计算完后，DAGScheduler会查询是否还有未完成的计算，直到有新的Stage提交</p>\n<pre><code class=\"txt\">  ============================   ShuffleMapStage 1 的提交计算过程  ==========================\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List()\n  org.apache.spark.scheduler.DAGScheduler          - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at map at WCAnalyzer.scala:28), which has no missing parents\n  org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ShuffleMapStage 1)\n  org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 1.0 with 1 tasks\n  org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)\n  org.apache.spark.executor.Executor               - Running task 0.0 in stage 1.0 (TID 1)\n  org.apache.spark.executor.Executor               - Finished task 0.0 in stage 1.0 (TID 1). 1331 bytes result sent to driver\n  org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 1.0 (TID 1) in 102 ms on localhost (executor driver) (1/1)\n  org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 1.0, whose tasks have all completed, from pool \n  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver\n  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 1 (map at WCAnalyzer.scala:28) finished in 0.117 s\n  ============================   ResultStage 2 的提交计算过程  =============================\n  org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages\n  org.apache.spark.scheduler.DAGScheduler          - running: Set()\n  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n  org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 2\n  org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 1 are now runnable\n  org.apache.spark.scheduler.DAGScheduler          - running: Set()\n  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - failed: Set()\n  org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)\n  org.apache.spark.scheduler.DAGScheduler          - missing: List()\n  org.apache.spark.scheduler.DAGScheduler          - Submitting ResultStage 2 (MapPartitionsRDD[7] at map at WCAnalyzer.scala:32), which has no missing parents\n  org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ResultStage 2)\n  org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 2.0 with 1 tasks\n  org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)\n  org.apache.spark.executor.Executor               - Running task 0.0 in stage 2.0 (TID 2)\n  org.apache.spark.executor.Executor               - Finished task 0.0 in stage 2.0 (TID 2). 1387 bytes result sent to driver\n  org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 2.0 (TID 2) in 44 ms on localhost (executor driver) (1/1)\n  org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 2.0, whose tasks have all completed, from pool \n  org.apache.spark.scheduler.DAGScheduler          - ResultStage 2 (collect at WCAnalyzer.scala:34) finished in 0.057 s</code></pre>\n<p>  以上是ShuffleMapStage 1和ResultStage 2的提交计算过程，与ShuffleMapStage 0一样，不再赘述</p>\n<pre><code class=\"txt\">  org.apache.spark.scheduler.DAGScheduler         - Job 0 finished: collect at WCAnalyzer.scala:34, took 0.770898 s</code></pre>\n<p>  DAGScheduler 当所有的Stage 提交计算完成 结束Job</p>\n</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ck7ycejfq0001jr6418kgxh8b","category_id":"ck7ycejg40004jr64xlx3uf76","_id":"ck7ycejgd000bjr64h3ru5nba"},{"post_id":"ck7ycejhb000mjr648t1ky96h","category_id":"ck7ycejhg000qjr64u0sekcjf","_id":"ck7ycejhi000vjr64f877bjmy"},{"post_id":"ck7ycejhd000ojr64rw99bnbw","category_id":"ck7ycejhh000sjr64zovfsnnp","_id":"ck7ycejhj000xjr64ubu1k9wb"}],"PostTag":[{"post_id":"ck7ycejfq0001jr6418kgxh8b","tag_id":"ck7ycejg60005jr64dky6lo15","_id":"ck7ycejgb0009jr64p0qd7av1"},{"post_id":"ck7ycejhb000mjr648t1ky96h","tag_id":"ck7ycejhg000rjr64z9kon39k","_id":"ck7ycejhh000ujr64tmmicjci"},{"post_id":"ck7ycejhd000ojr64rw99bnbw","tag_id":"ck7ycejhh000tjr64aqd7jvob","_id":"ck7ycejhi000wjr647silyenu"}],"Tag":[{"name":"Hexo","_id":"ck7ycejg60005jr64dky6lo15"},{"name":"大数据面试题","_id":"ck7ycejhg000rjr64z9kon39k"},{"name":"Spark","_id":"ck7ycejhh000tjr64aqd7jvob"}]}}