<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大数据面试题</title>
      <link href="/2020/01/15/da-shu-ju-mian-shi-ti/"/>
      <url>/2020/01/15/da-shu-ju-mian-shi-ti/</url>
      
        <content type="html"><![CDATA[<h1 id="大数据面试题"><a href="#大数据面试题" class="headerlink" title="大数据面试题"></a>大数据面试题</h1><p><strong>1、你能简单描述一下Hbase吗？能画出它的架构图吗？</strong></p><p><a href="http://lib.csdn.net/base/hbase" target="_blank" rel="noopener">hbase</a>是一个面向列的 NoSQL 分布式<a href="http://lib.csdn.net/base/mysql" target="_blank" rel="noopener">数据库</a>，它利用HDFS作为底层存储系统。那么，HBase相对于传统的关系型数据库有什么不同呢？</p><ul><li>HBase是schema-free的，它的列是可以动态增加的（仅仅定义列族），并且为空的列不占物理存储空间。</li><li>HBase是基于列存储的，每个列族都由几个文件保存，不同的列族的文件是分离的。</li><li>HBase自动切分数据，使得数据存储自动具有很好的横向扩展性。</li><li>HBase没有任何事务，提供了高并发读写操作的支持。</li></ul><p>HBase中的Table是一个稀疏的、多维度的、排序的映射表，这张表的索引是[RowKey, ColumnFamily, ColumnQualifier, Timestamp]，其中Timestamp表示版本，默认获取最新版本。HBase是通过RowKey来检索数据的，RowKey是Table设计的核心，它按照ASCII有序排序，因此应尽量避免顺序写入。RowKey设计应该注意三点：</p><ul><li>唯一原则：在HBase中rowkey可以看成是表的主键，必须保证其唯一性。</li><li>散列原则：由于rowkey是按字典有序的，故应避免rowkey连续有序而导致在某一台RegionServer上堆积的现象。例如可以拼接随机数、将时间戳倒序等。</li><li>长度原则：设计时RowKey要尽量短，这样可以提高有效数据的比例，节省存储空间，也可以提高查询的性能。</li></ul><p>下面是HBase的整体架构图：</p><p><img src="http://img.blog.csdn.net/20160423184359154" alt="img"></p><p><strong>2、你说了解kafka，能简单描述一下Kafka吗？能画出它的架构图吗？</strong></p><p>Kafka是一个高吞吐、易扩展的分布式发布-订阅消息系统，它能够将消息持久化到磁盘，用于批量的消费。Kafka中有以下几个概念：</p><ul><li>Topic：特指Kafka处理的消息源（feeds of messages）的不同分类。</li><li>Partition：Topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。</li><li>Broker：Kafa集群中包含一台或多台服务器，这种服务器被称为broker。</li><li>Producer：生产者，向Kafka的一个topic发布消息。</li><li>Consumers：消费者，从kafka的某个topic读取消息。</li></ul><p>Kafka架构图如下：</p><p><img src="http://img.blog.csdn.net/20160423204357095" alt="img"></p><p>详见：<a href="http://www.infoq.com/cn/articles/apache-kafka/" target="_blank" rel="noopener">Apache Kafka：下一代分布式消息系统</a></p><p><strong>3、请介绍你的一个亮点项目？你在其中做了什么？碰到了什么技术难点？</strong></p><p>【解】介绍项目《九州卡牌》手游，我在项目中主要负责客户端逻辑与战斗效果的实现，以及网络通信模块的设计与开发。<br>首先，对于网络通信我们选择使用<strong>TCP长连接</strong>，因为对于卡牌类手游可以容忍偶尔地延迟，并且有服务器主动给客户端推送消息的需求。<br>优点：</p><ul><li>简单有效的长连接</li><li>可靠的信息传输</li><li>数据包的大小没有限制</li><li>服务器可以主动向客户端推送消息（广播等）</li></ul><p>客户端每隔3s发送一次心跳包给服务器，通知服务器自己仍然在线，并获取服务器数据更新 —— 心跳包可以防止TCP的死连接问题，避免出现长时间不在线的死链接仍然出现在服务端的管理任务中。当客户端长时间切换到后台时，进程被挂起，连接会断开。<br>TCP协议本身就有keep-alive机制，为什么还要在应用层实现自己的心跳检测机制呢？</p><ul><li>TCP的keep-alive机制可能在短暂的网络异常中，将一个良好的连接给断开；</li><li>keep-alive设计初衷是清除和回收死亡时间长的连接，不适合实时性高的场合，而且它会先要求连接一定时间内没有活动，周期长，这样其实已经断开很长一段时间，没有及时性；</li><li>keep-alive不能主动通知应用层；</li><li>另外，想要通过心跳包来获取服务器的数据更新，所以选择自己在应用层实现；</li></ul><p>还有一个问题就是一台机器的连接数有限制，可以通过<strong>滚服</strong>或者<strong>分布式</strong>来解决。</p><ul><li><strong>滚服：</strong>指老的服务器连接数达到上限了，就开新的服务区，不同服务区的用户不能交互。</li><li><strong>分布式：</strong>长连接不分服的话，可以多个cluster节点连接同样的CACHE数据源，只是跨节点进行通信比较麻烦一点（如用户A连接到节点1，用户B连接到节点2，用户A向节点1发起TCP请求处理业务需要再通知到节点2的用户B）。一般来说有2种解决方案：<br>①是建立场景服务器，即专门用一个socket server来保持所有玩家的连接，然后它只处理数据推送，不做业务，可以达到10-20W承载；②是采用发布订阅方式实现节点间的实时通信。</li></ul><p>我在<a href="http://lib.csdn.net/base/linux" target="_blank" rel="noopener">Linux</a>下写了一个<strong>Socket心跳包示例程序</strong>，见文《<a href="http://blog.csdn.net/lisonglisonglisong/article/details/51327695" target="_blank" rel="noopener">TCP socket心跳包示例程序</a>》。</p><p><strong>4、请介绍一下MapReduce的工作原理。</strong></p><p>【解】MapReduce是一个分布式计算框架，用于大规模数据集的并行运算。简单地说，MapReduce就是”任务的分解与结果的汇总”：将一个大的数据处理任务划分成许多个子任务，并将这些子任务分配给各个节点并行处理，然后通过整合各个节点的中间结果，得到最终结果。</p><p>MapReduce是主从架构，在master上跑的是JobTracker/ResourceManager，负责资源分配与任务调度；而各个slave上跑的是TaskTracker/NodeManager，负责执行任务，并定期向master汇报最新状态与执行进度。</p><p>对于一个MR任务，它的输入、输出以及中间结果都是<code>&lt;key, value&gt;</code>键值对：</p><ul><li>Map：<code>&lt;k1, v1&gt;</code> ——&gt; <code>list(&lt;k2, v2&gt;)</code></li><li>Reduce：<code>&lt;k2, list(v2)&gt;</code> ——&gt; <code>list(&lt;k3, v3&gt;)</code></li></ul><p>MR程序的执行过程主要分为三步：Map阶段、Shuffle阶段、Reduce阶段，如下图：</p><p><img src="http://img.blog.csdn.net/20160811132825039" alt="img"></p><ol><li><strong>Map阶段</strong><ul><li>分片（Split）：map阶段的输入通常是HDFS上文件，在运行Mapper前，FileInputFormat会将输入文件分割成多个split ——1个split至少包含1个HDFS的Block（默认为64M）；然后每一个分片运行一个map进行处理。</li><li>执行（Map）：对输入分片中的每个键值对调用<code>map()</code>函数进行运算，然后输出一个结果键值对。<ul><li>Partitioner：对 map 函数的输出进行partition，即根据key或value及reduce的数量来决定当前的这对键值对最终应该交由哪个reduce处理。默认是对key哈希后再以reduce task数量取模，默认的取模方式只是为了避免数据倾斜。然后该key/value对以及partitionIdx的结果都会被写入环形缓冲区。</li></ul></li><li>溢写（Spill）：map输出写在内存中的环形缓冲区，默认当缓冲区满80%，启动溢写线程，将缓冲的数据写出到磁盘。<ul><li>Sort：在溢写到磁盘之前，使用快排对缓冲区数据按照partitionIdx, key排序。（每个partitionIdx表示一个分区，一个分区对应一个reduce）</li><li>Combiner：如果设置了Combiner，那么在Sort之后，还会对具有相同key的键值对进行合并，减少溢写到磁盘的数据量。</li></ul></li><li>合并（Merge）：溢写可能会生成多个文件，这时需要将多个文件合并成一个文件。合并的过程中会不断地进行 sort &amp; combine 操作，最后合并成了一个已分区且已排序的文件。</li></ul></li><li><strong>Shuffle阶段</strong>：广义上Shuffle阶段横跨Map端和Reduce端，在Map端包括Spill过程，在Reduce端包括copy和merge/sort过程。通常认为Shuffle阶段就是将map的输出作为reduce的输入的过程<ul><li>Copy过程：Reduce端启动一些copy线程，通过HTTP方式将map端输出文件中属于自己的部分拉取到本地。Reduce会从多个map端拉取数据，并且每个map的数据都是有序的。</li><li>Merge过程：Copy过来的数据会先放入内存缓冲区中，这里的缓冲区比较大；当缓冲区数据量达到一定阈值时，将数据溢写到磁盘（与map端类似，溢写过程会执行 sort &amp; combine）。如果生成了多个溢写文件，它们会被merge成一个<strong>有序的最终文件</strong>。这个过程也会不停地执行 sort &amp; combine 操作。</li></ul></li><li><strong>Reduce阶段</strong>：Shuffle阶段最终生成了一个有序的文件作为Reduce的输入，对于该文件中的每一个键值对调用<code>reduce()</code>方法，并将结果写到HDFS。</li></ol><p>参考《<a href="http://www.aboutyun.com/thread-7078-1-1.html" target="_blank" rel="noopener">了解MapReduce核心Shuff</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据面试题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手撕Spark之WordCount RDD执行流程</title>
      <link href="/2019/12/18/shou-si-spark-zhi-wordcount-rdd-zhi-xing-liu-cheng/"/>
      <url>/2019/12/18/shou-si-spark-zhi-wordcount-rdd-zhi-xing-liu-cheng/</url>
      
        <content type="html"><![CDATA[<h2 id="手撕Spark之WordCount-RDD执行流程"><a href="#手撕Spark之WordCount-RDD执行流程" class="headerlink" title="手撕Spark之WordCount RDD执行流程"></a>手撕Spark之WordCount RDD执行流程</h2><p>[TOC]</p><h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>一个Spark程序在初始化的时候会构造DAGScheduler、TaskSchedulerImpl、MapOutTrackerMaster等对象，DAGScheduler主要负责生成DAG、启动Job、提交Stage等操作，TaskSchedulerImpl主要负责Task Set的添加调度等，MapOutTrackerMaster主要负责数据的Shuffle等，这里不再赘述。</p><p><strong>注意几个概念：</strong></p><ul><li>Application   //一个Spark程序会有一个Application，也就拥有了唯一的一个applicationId</li><li>Job    //调用Action 算子 触发runJob，触发一次runJob就会产生一个Job</li><li>Stage  //遇到一次宽依赖就会生成一个Stage</li><li>Task  //Spark程序运行的最小单元</li></ul><blockquote><p>注：一个Spark程序会有1个Application，会有1～N 个Job，会有1～N 个Stage，会有1～N 个Task</p><p>1 Application = [1 ~ N  ] Job<br>1 Job = [ 1 ~ N ] Stage<br>1 Stage = [ 1 ~ N ] Task</p><p>Stage数 = Shuffle数 +1</p></blockquote><h3 id="软件环境"><a href="#软件环境" class="headerlink" title="软件环境"></a>软件环境</h3><ul><li>Spark：2.3.0</li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>写一个简单的WordCount计算代码</p><p>data.txt</p><pre class=" language-txt"><code class="language-txt">hello worldhello javahello scalahello hadoophello spark</code></pre><p>WCAnalyzer.scala</p><pre class=" language-scala"><code class="language-scala">    <span class="token comment" spellcheck="true">//设置日志输出级别，便于观察日志</span>    Logger<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">"org.apache"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>Level<span class="token punctuation">.</span>ALL<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//创建sc</span>    <span class="token keyword">val</span> sc <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span><span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[1]"</span><span class="token punctuation">)</span>                              <span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"WCAnalyzer"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//从文件读取数据</span>    sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"data/data.txt"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//将数据按照空格进行切分（切分出单个单词）</span>      <span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//将每个单词和1组成一个Tuple</span>      <span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//按照相同的单词进行聚合</span>      <span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//将聚合后的结果将（key，value）数据进行倒置 转换成（value，key）便于排序</span>      <span class="token punctuation">.</span>map<span class="token punctuation">(</span>v <span class="token keyword">=></span> <span class="token punctuation">(</span>v<span class="token punctuation">.</span>_2<span class="token punctuation">,</span> v<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//按照聚合后的单词数量进行降序排序</span>      <span class="token punctuation">.</span>sortByKey<span class="token punctuation">(</span><span class="token boolean">false</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//将排序后的数据进行倒置</span>      <span class="token punctuation">.</span>map<span class="token punctuation">(</span>v <span class="token keyword">=></span> <span class="token punctuation">(</span>v<span class="token punctuation">.</span>_2<span class="token punctuation">,</span> v<span class="token punctuation">.</span>_1<span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//将数据收集到driver</span>      <span class="token punctuation">.</span>collect<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true">//输出数据</span>      <span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">//关闭sc</span>    sc<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">}</span></code></pre><h3 id="过程分析"><a href="#过程分析" class="headerlink" title="过程分析"></a>过程分析</h3><p>本代码只会生成一个Job，3个Stage，8个RDD。</p><ul><li><p>划分Stage</p><p>Stage的划分要从后向前，每遇到一次宽依赖就划分一个Stage，因此这个简单的WC代码可以分为3个Stage，分别是由textFile、flatMap、map算子组成的第一个Stage 0；由reduceByKey、map算子组成的Stage 1；由sortByKey、map算子组成的Stage 2。</p></li><li><p>RDD的生成 </p><p>textFile（HadoopRDD [0] ，MapPartitionsRDD [1] ）  //[ ] 内为该rdd的序号</p><p>flatMap（MapPartitionsRDD [2] ）</p><p>map（MapPartitionsRDD [3] ）</p><p>reduceByKey（ShuffledRDD [4] ）</p><p>map（MapPartitionsRDD [5] ）</p><p>sortByKey（ShuffledRDD [6] ）</p><p>map （MapPartitionsRDD [7] ）</p></li><li><p>日志分析</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.SparkContext                     - Starting job: collect at WCAnalyzer.scala:34</code></pre><p>  由collect算子触发runJob 启动一个Job，代码中的<code>foreach(println)</code>其中<code>foreach</code>并不是RDD中的算子，因此不会触发runJob，也就不会生成一个Job</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler           - Got job 0 (collect at WCAnalyzer.scala:34) with 1 output partitions</code></pre><pre><code>  生成一个Job 0 ，这个Job是由collect算子生成，在代码第34行，有一个分区</code></pre><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler          - Final stage: ResultStage 2 (collect at WCAnalyzer.scala:34)  org.apache.spark.scheduler.DAGScheduler          - Parents of final stage: List(ShuffleMapStage 1)  org.apache.spark.scheduler.DAGScheduler          - Missing parents: List(ShuffleMapStage 1)  org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)  org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 1)  org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)  org.apache.spark.scheduler.DAGScheduler          - missing: List(ShuffleMapStage 0)  org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 0)  org.apache.spark.scheduler.DAGScheduler          - missing: List()</code></pre><p>  Job 的Final Stage 为ResultStage 0，ResultStage 的父依赖为ShuffleMapStage 1，遗留的父依赖为ShuffleMapStage 1。</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler         - submitStage(ResultStage 2)</code></pre><p>  尝试提交ResultStage 2</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 1)</code></pre><p>  遗留一个ShuffleMapStage 1</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 1)</code></pre><p>  尝试提交ShuffleMapStage 1</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler         - missing: List(ShuffleMapStage 0)</code></pre><p>  遗留一个ShuffleMapStage 0</p><pre class=" language-txt"><code class="language-txt">    org.apache.spark.scheduler.DAGScheduler         - submitStage(ShuffleMapStage 0)</code></pre><p>   尝试提交ShuffleMapStage 0</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler         - missing: List()</code></pre><p>  没有遗留的Stage</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler         - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WCAnalyzer.scala:24), which has no missing parents</code></pre><p>  提交ShuffleMapStage 0，该Stage的最后一个RDD是MapPartitionsRDD[3]，是由map算子生成，在代码第24行</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler         - submitMissingTasks(ShuffleMapStage 0)</code></pre><p>  提交Tasks，一个Stage就是一个Task Set集合</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.TaskSchedulerImpl    - Adding task set 0.0 with 1 tasks</code></pre><p>  TaskSchedulerImpl 调度器添加一个Task Set集合</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.TaskSetManager       - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7909 bytes)</code></pre><p>  TaskSetManager 启动stage 0.0 中的task 0.0（taskid=0.0，host=localhost，executor=driver，partition=0，taskLocality=PROCESS_LOCAL，serializedTask=7909 bytes</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.executor.Executor              - Running task 0.0 in stage 0.0 (TID 0)</code></pre><p>  Executor 端运行task</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.executor.Executor              - Finished task 0.0 in stage 0.0 (TID 0). 1159 bytes result sent to driver</code></pre><p>  Executor 端 运行完成task，将序列化后大小为1159 bytes结果数据发送回driver端</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.TaskSetManager       - Finished task 0.0 in stage 0.0 (TID 0) in 194 ms on localhost (executor driver) (1/1)</code></pre><p>  TaskSetManager 运行完task  完成task数量／总攻task数量</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 0.0, whose tasks have all completed, from pool </code></pre><p>  TaskSchedulerImpl 移除TaskSet 集合</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver</code></pre><p>  DAGScheduler 完成ShuffleMapTask 的计算</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 0 (map at WCAnalyzer.scala:24) finished in 0.289 s</code></pre><p>  DAGScheduler 完成ShuffleMapStage 的计算，用时共 0.289 s</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages  org.apache.spark.scheduler.DAGScheduler          - running: Set()  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)  org.apache.spark.scheduler.DAGScheduler          - failed: Set()  org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 1  org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 0 are now runnable  org.apache.spark.scheduler.DAGScheduler          - running: Set()  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ShuffleMapStage 1, ResultStage 2)  org.apache.spark.scheduler.DAGScheduler          - failed: Set()</code></pre><p>  Stage在计算完后，DAGScheduler会查询是否还有未完成的计算，直到有新的Stage提交</p><pre class=" language-txt"><code class="language-txt">  ============================   ShuffleMapStage 1 的提交计算过程  ==========================  org.apache.spark.scheduler.DAGScheduler          - submitStage(ShuffleMapStage 1)  org.apache.spark.scheduler.DAGScheduler          - missing: List()  org.apache.spark.scheduler.DAGScheduler          - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at map at WCAnalyzer.scala:28), which has no missing parents  org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ShuffleMapStage 1)  org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 1.0 with 1 tasks  org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7638 bytes)  org.apache.spark.executor.Executor               - Running task 0.0 in stage 1.0 (TID 1)  org.apache.spark.executor.Executor               - Finished task 0.0 in stage 1.0 (TID 1). 1331 bytes result sent to driver  org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 1.0 (TID 1) in 102 ms on localhost (executor driver) (1/1)  org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 1.0, whose tasks have all completed, from pool   org.apache.spark.scheduler.DAGScheduler          - ShuffleMapTask finished on driver  org.apache.spark.scheduler.DAGScheduler          - ShuffleMapStage 1 (map at WCAnalyzer.scala:28) finished in 0.117 s  ============================   ResultStage 2 的提交计算过程  =============================  org.apache.spark.scheduler.DAGScheduler          - looking for newly runnable stages  org.apache.spark.scheduler.DAGScheduler          - running: Set()  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)  org.apache.spark.scheduler.DAGScheduler          - failed: Set()  org.apache.spark.MapOutputTrackerMaster          - Increasing epoch to 2  org.apache.spark.scheduler.DAGScheduler          - Checking if any dependencies of ShuffleMapStage 1 are now runnable  org.apache.spark.scheduler.DAGScheduler          - running: Set()  org.apache.spark.scheduler.DAGScheduler          - waiting: Set(ResultStage 2)  org.apache.spark.scheduler.DAGScheduler          - failed: Set()  org.apache.spark.scheduler.DAGScheduler          - submitStage(ResultStage 2)  org.apache.spark.scheduler.DAGScheduler          - missing: List()  org.apache.spark.scheduler.DAGScheduler          - Submitting ResultStage 2 (MapPartitionsRDD[7] at map at WCAnalyzer.scala:32), which has no missing parents  org.apache.spark.scheduler.DAGScheduler          - submitMissingTasks(ResultStage 2)  org.apache.spark.scheduler.TaskSchedulerImpl     - Adding task set 2.0 with 1 tasks  org.apache.spark.scheduler.TaskSetManager        - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 7649 bytes)  org.apache.spark.executor.Executor               - Running task 0.0 in stage 2.0 (TID 2)  org.apache.spark.executor.Executor               - Finished task 0.0 in stage 2.0 (TID 2). 1387 bytes result sent to driver  org.apache.spark.scheduler.TaskSetManager        - Finished task 0.0 in stage 2.0 (TID 2) in 44 ms on localhost (executor driver) (1/1)  org.apache.spark.scheduler.TaskSchedulerImpl     - Removed TaskSet 2.0, whose tasks have all completed, from pool   org.apache.spark.scheduler.DAGScheduler          - ResultStage 2 (collect at WCAnalyzer.scala:34) finished in 0.057 s</code></pre><p>  以上是ShuffleMapStage 1和ResultStage 2的提交计算过程，与ShuffleMapStage 0一样，不再赘述</p><pre class=" language-txt"><code class="language-txt">  org.apache.spark.scheduler.DAGScheduler         - Job 0 finished: collect at WCAnalyzer.scala:34, took 0.770898 s</code></pre><p>  DAGScheduler 当所有的Stage 提交计算完成 结束Job</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/09/12/hello-world/"/>
      <url>/2018/09/12/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
